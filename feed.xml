<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://starlight345.github.io/2025/feed.xml" rel="self" type="application/atom+xml"/><link href="https://starlight345.github.io/2025/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-04T12:12:09+08:00</updated><id>https://starlight345.github.io/2025/feed.xml</id><title type="html">ICLR Blogposts 2025</title><subtitle>Home to the 2025 ICLR Blogposts track </subtitle><entry><title type="html">Steering LLMs’ Behavior with Concept Activation Vectors</title><link href="https://starlight345.github.io/2025/blog/steering-llms-behavior/" rel="alternate" type="text/html" title="Steering LLMs’ Behavior with Concept Activation Vectors"/><published>2025-05-07T00:00:00+08:00</published><updated>2025-05-07T00:00:00+08:00</updated><id>https://starlight345.github.io/2025/blog/steering-llms-behavior</id><content type="html" xml:base="https://starlight345.github.io/2025/blog/steering-llms-behavior/"><![CDATA[<p>As a classic interpretation method, concept activation vectors (CAVs) describe the distribution features of activation in neural networks with simple linear classifiers. Some work has successfully transferred this to LLMs, using CAV to steer model behavior. Previous research on the ability of CAV to steer LLMs’ behavior has not been systematic enough, limited to safety concepts <d-cite key="Xu2024uncovering"></d-cite> or insufficient hyperparameter tuning <d-cite key="nejadgholi2022improving"></d-cite>. Other methods not using CAV to steer LLMs’ behavior, such as simply merging residual streams <d-cite key="turner2023activation"></d-cite>, is even less effective than the algorithm proposed in <d-cite key="Xu2024uncovering"></d-cite>, which can automatically merge multiple layers and determine hyperparameters. This blog aims to explore the boundaries of using CAV to steer LLMs’ behavior through extensive experiments and observations, fully showcasing its potential and limitations. <d-footnote>Since CAV originated from image neural networks interpretation, its "activation" is equal to the embeddings or representations of LLMs. For the sake of coherence, this blog uses "activation" when describing concepts related to CAV and uses "embedding" when describing concepts related to LLMs.</d-footnote></p> <h2 id="preliminaries">Preliminaries</h2> <p>Behavior steering is a technique of changing the behavior of LLMs when inferencing, such as changing the likes and dislikes of its output, or the degree of relevance to something. This method does not require pre-training or fine-tuning. It only needs to be adjusted according to the interpretability features of its embedding or attention during inferencing. Therefore, it is a promising method for the downstream application of interpretation techniques.</p> <p>In this blog, the behavior steering we discuss is by concept, which means that the behavior we want to change is aligned with human ideas, and then we try to extract the corresponding features from LLMs; instead of extracting relevant neural activities from LLM first, and then summarizing what their behaviors are in the eyes of human beings. Therefore, when human beings have a single idea, there is a single concept to steer.</p> <p>For single behavior steering, we use the pipeline outlined in <d-cite key="Xu2024uncovering"></d-cite>. The main difference between using CAV for steering and methods like ActAdd <d-cite key="turner2023activation"></d-cite> is that the linear classifiers relied upon by CAV can utilize the embedding distribution features of LLMs to the maximum extent, enabling behavior steering at minimal cost of performance damage.</p> <h3 id="overview-of-cav-perturbation-algorithm">Overview of CAV Perturbation Algorithm</h3> <ol> <li> <p><strong>Data Collection</strong>: Gather two sets of instructions aimed at carrying out two different tasks, labeling one as the positive and the other as negative. For example, a positive dataset might include instructions like <span style="color: red;">How to plant flowers in my garden?</span>, while the negative dataset might include instructions like <span style="color: green;">Comment planter des fleurs dans mon jardin?</span>. Thus, this two datasets can be used for “French” concept extraction and utilization. <d-footnote>For optimal results, each dataset contains more than 50 instructions, though <d-cite key="Xu2024uncovering"></d-cite> claims only 10 pairs of instructions are enough.</d-footnote></p> </li> <li> <p><strong>LLM Selection</strong>: Choose a target LLM, such as <em>LLaMA-3-8B</em>, known for its better capabilities. Collect the final token embeddings from each layer during the inference process for instructions in positive and negative datasets respectively.</p> </li> <li> <p><strong>Classifier Training</strong>: Train a linear classifier on these embeddings with their corresponding labels for each layer. That’s to say, we will get \(N\) classifiers for steering a single behavior, where \(N\) is the total number of transformer layers of the target LLM.</p> </li> <li> <p><strong>Text Generation with Perturbation</strong>: Use the trained classifier parameters to perturb a typical text generation process of the target LLM. Due to the transferability disclosed by <d-cite key="Xu2024uncovering"></d-cite>, it may be possible to apply the CAV trained on target LLM to other LLMs. However, since we have complete access to all open-source LLMs, it is assumed that the LLM used for training and generation is same by default.</p> </li> </ol> <h3 id="perturbation-process-details">Perturbation Process Details</h3> <p>Assuming classifiers have been trained for each layer, we perform perturbations sequentially during the generation process for each new token:</p> <ul> <li>For each layer \(n\) , we first evaluate the test accuracy of its corresponding classifier. If the accuracy is low, indicating the concept is not yet fully formed at this layer, we skip perturbation to minimize potential adverse impacts on the LLM’s overall capabilities. Conversely, if the accuracy exceeds a specified threshold \(P_1\), we proceed with the perturbation.</li> <li>Let \(\theta\) be the parameters of classifier \(f\), and \(e\) the original layer embedding. The perturbation process then depends on the intended direction of steering. If the input instruction originally targets a task described by the positive label and the goal of steering is to shift towards the negative, we reduce \(f_\theta(e)\) to achieve a targeted probability \(P_0\).</li> <li>The perturbation is represented by \(e\leftarrow e+g(\theta,e,P_0)\), where \(g(\cdot)\) is a closed-form function. This adjustment across layers modifies the LLM’s “recognition” of the input instruction, leading the output to exhibit characteristics represented by the negative label.</li> </ul> <h2 id="methodology">Methodology</h2> <p>We use the benign instructions provided in representation engineering <d-cite key="zou2023representation"></d-cite> as the training set base, as this dataset includes many normal instructions for LLMs. For each behavior steering case, we use 50 instructions as the training set and another 50 non-overlapping instructions as the test set to calculate the test accuracy of the classifiers. In other words, we extract 4 subsets of 50 instructions from the original dataset, with two assigned to each label for training and testing.</p> <p>In this blog, our positive instructions are the unmodified version, while the negative instructions have been modified to have steering goal characteristics. This setting is consistent with the example above <d-cite key="Xu2024uncovering"></d-cite>.</p> <p>To construct the negative dataset, that is, to modify negative instructions into the form with targeted behavior, we summarize three methods:</p> <ol> <li> <p><strong>Complete Replacement</strong>. For fundamentally different tasks, the original instruction can be directly replaced with a completely different instruction that has the target behavior. For example, in safety tasks, the instructions in the negative dataset are all recollected.</p> </li> <li> <p><strong>Prefix and Suffix Addition</strong>. For tasks like style transfer, a string describing requirements can be added in the form of a prefix or suffix to the original instruction. For instance, adding <code class="language-plaintext highlighter-rouge">"Answer in Python code please."</code> is a processing method suitable for “Code” concept.</p> <p>To avoid potential misguidance caused by a single format of additional requirement string, this modification has two random elements - randomly using a prefix or suffix, and randomly selecting one string from several to add to the original instruction.</p> </li> <li> <p><strong>Instruction Transfer</strong>. For tasks like language or grammar, the original instruction can be directly transferred to the target task. For example, for “French” concept, the original instruction can be translated into the corresponding language.</p> </li> </ol> <p>For example (CR - <em>Complete Replacement</em>; PSA - <em>Prefix and Suffix Addtion</em>; IT - <em>Instruction Transfer</em>):</p> <div>&emsp;&emsp;<span style="font-weight: bold;">x</span> = How to plant flowers in my garden?</div> <div>&emsp;&emsp;<span style="color: blue; font-weight: bold;">CR</span>(<span style="font-weight: bold;">x</span>) = How to make American coffee?</div> <div>&emsp;&emsp;<span style="color: orange; font-weight: bold;">PSA</span>(<span style="font-weight: bold;">x</span>) = How to plant flowers in my garden? Answer in Python please.</div> <div>&emsp;&emsp;<span style="color: pink; font-weight: bold;">IT</span>(<span style="font-weight: bold;">x</span>) = Comment planter des fleurs dans mon jardin?</div> <p><br/> The above three operations can be seen as operational primitives, and the results obtained by nesting them can serve as a method for constructing datasets for multi-behavior steering.</p> <div>&emsp;&emsp;<span style="color: orange; font-weight: bold;">PSA</span>(<span style="color: pink; font-weight: bold;">IT</span>(<span style="font-weight: bold;">x</span>)) = Comment planter des fleurs dans mon jardin? Answer in Python please.</div> <p><br/> After building the datasets, we perform the CAV perturbation in text generation process, achieving the effect of behavior steering.</p> <h2 id="experiments">Experiments</h2> <p>Before training classifiers, we apply PCA reduction to evaluate the separability of the embeddings of these instructions. We train the PCA on the dataset and observe good linear separability. We want to clarify that <strong>the intuitive results of PCA are not completely consistent with the actual test accuracy</strong>. In cases where positive and negative examples appear to overlap in the PCA results, the classifier’s test accuracy may still be very high, even as high as those layers that do not seem to overlap. In the next experiments, if there is such an inconsistency, we will provide data of both for reference.</p> <p>By default, we use <code class="language-plaintext highlighter-rouge">Llama-3-8B-Instruct</code> for experiments. Other LLMs may be involved for some concepts, and we will clearly indicate.</p> <h3 id="python-code-concept">Python (Code) Concept</h3> <p>First, we try the Python concept, which has a negative instruction dataset constructed by PSA (Perfix and Suffix Addition). The test accuracy for the CAV is quite high, above 99% except for layer 0. However, you will see in the PCA results shown below that the early layers seem to have better separability than the later layers. Therefore, the results of PCA can only be auxiliary, and the test accuracy is a better indicator for understanding the effectiveness of CAV.</p> <div class="l-screen"> <iframe src="/2025/assets/html/2025-05-07-steering-llms-behavior/pca_code.html" frameborder="0" scrolling="no" height="820px" width="100%"></iframe> </div> <p>After training the Python CAV, we will attempt to steer behavior with it. We will apply Python CAV to three types of tasks:</p> <ol> <li>Tasks completely unrelated to code, e.g., <strong>how to make American coffee</strong>;</li> <li>Tasks with a certain programming mindset requirement, e.g., <strong>how to solve Hanoi Tower problem</strong>;</li> <li>Complete programming tasks, e.g., <strong>how to calculate the maximum depth of a binary tree</strong>. The example is a classic algorithm problem, and we can see whether the solution is better with steering.</li> </ol> <p>You can try the interactive panel below to compare the outputs of the three tasks before and after using Python CAV to steer behavior.</p> <div class="l-page"> <iframe src="/2025/assets/html/2025-05-07-steering-llms-behavior/1.html" frameborder="0" scrolling="no" height="520px" width="100%"></iframe> </div> <p>Our observation are:</p> <ol> <li>For some specific tasks, Python CAV can enable LLMs to provide answers containing Python code, even if the original response doesn’t include Python code, or the original instruction is not a coding task at all.</li> <li>For coding tasks, there is a lack of broader experiments to demonstrate whether it will be better steering with CAV. Referring to the results of Task 1.3, the response before steering are more comprehensive, while the response after steering seem to be more straightforward.</li> </ol> <h3 id="language-concept">Language Concept</h3> <p>Next, we explore language-related concepts, which are considered a more practical steering usage. The experiments of language concepts involves four specific languages—English, French, Chinese (including Simplified and Traditional), and Arabic. The construction of the dataset involves two methods—Prefix and Suffix Addition (PSA) and Instruction Transfer (IT). Due to space limitations, it is not possible to present the results for all combinations pairwise; only some of the most meaningful and interesting content will be discussed below.</p> <h4 id="french-concept">French Concept</h4> <p>When studying the French concept, we also examine the differences between PSA and IT. When using PSA to induce French CAV, the instructions in both the positive and negative datasets are written in English. When using IT, the instructions in the positive dataset are in English, while the instructions in the negative dataset are translated into French by a translation API.</p> <p>We select three different text generation tasks to test the effects of using PSA and IT to induce the French CAV for behavior steering. Try the interactive panel below to view the PCA results and outputs of the two CAVs.</p> <div class="l-screen"> <iframe src="/2025/assets/html/2025-05-07-steering-llms-behavior/2.1.html" frameborder="0" scrolling="no" height="800px" width="100%"></iframe> </div> <div class="l-page"> <iframe src="/2025/assets/html/2025-05-07-steering-llms-behavior/2.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p>Our observations are:</p> <ol> <li>Using IT-induced French CAV achieves better effect than PSA-induced one; despite this, their test accuracies are quite high;</li> <li>When steering with PSA-induced CAV, the output often retains more or less English content, while using IT-induced CAV results in much less;</li> <li>The optimal \(P_0\) for steering with the two types of CAV are different, with about 25% for PSA-CAV and about 10% for IT-CAV. This difference seems to show that even we look like to induce the same CAVs, the results are different. This will be discussed further in the discussion section.</li> </ol> <div class="l-body"> <iframe src="/2025/assets/html/2025-05-07-steering-llms-behavior/test_accuracy_french_psa_it.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <h4 id="simplfiedtraditional-chinese-concept">Simplfied/Traditional Chinese Concept</h4> <p>The differences between simplified and traditional Chinese are a very interesting phenomenon. We use IT and translation APIs to construct positive and negative datasets, with the positive dataset translated into simplified Chinese and the negative dataset into traditional Chinese. However, we struggle to train a good CAV on <code class="language-plaintext highlighter-rouge">Llama-3-8B-Instruct</code> with such datasets, possibly because this model doesn’t have good Chinese output capabilities. Therefore, we use <code class="language-plaintext highlighter-rouge">Llama3.1-8B-Chinese-Chat</code>, a fine-tuned version of <code class="language-plaintext highlighter-rouge">Llama3.1-8B-Instruct</code> and its original version for this concept.</p> <div class="l-page"> <iframe src="/2025/assets/html/2025-05-07-steering-llms-behavior/3.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <div class="l-screen"> <iframe src="/2025/assets/html/2025-05-07-steering-llms-behavior/pca_chinese_3.1c.html" frameborder="0" scrolling="no" height="820px" width="100%"></iframe> </div> <p>Our observations are:</p> <ol> <li><code class="language-plaintext highlighter-rouge">Llama-3-8B-Instruct</code> answers all three tasks tested with English as the input language in Simplified Chinese, so there was no noticeable change after applying CAV perturbations; <code class="language-plaintext highlighter-rouge">Llama-3.1-8B-Instruct</code> and <code class="language-plaintext highlighter-rouge">Llama-3.1-8B-Chinese-Chat</code> are able to respond in Chinese, making the CAV perturbations effective. The text in the interactive panel above is generated by <code class="language-plaintext highlighter-rouge">Llama-3.1-8B-Chinese-Chat</code>;</li> <li>The accuracy trends of the CAVs trained on the three mentioned LLMs show a similar pattern across layers, initially decreasing and then increasing. In terms of relative accuracy, <code class="language-plaintext highlighter-rouge">Llama-3-8B-Instruct</code> is the lowest, while <code class="language-plaintext highlighter-rouge">Llama-3.1-8B-Instruct</code> is the highest (with some late layers of <code class="language-plaintext highlighter-rouge">Llama-3.1-8B-Chinese-Chat</code> being even higher). In certain middle layers, the difference between <code class="language-plaintext highlighter-rouge">Llama-3.1-8B-Instruct</code> and <code class="language-plaintext highlighter-rouge">Llama-3.1-8B-Chinese-Chat</code> is even greater than the difference between <code class="language-plaintext highlighter-rouge">Llama-3.1-8B-Chinese-Chat</code> and <code class="language-plaintext highlighter-rouge">Llama-3-8B-Instruct</code>. The reason for this is currently unclear.</li> </ol> <div class="l-body"> <iframe src="/2025/assets/html/2025-05-07-steering-llms-behavior/test_accuracy_chinese.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <h4 id="arabic-concept">Arabic Concept</h4> <p>Compared to French and Chinese, Arabic should be a less common language in Llama-3.1 and is also a low-resource one. How effective is the CAV extraction and behavior steering for this low-resource language? We also used PSA and IT methods along with the Arabic translation API to build datasets. Try the interactive panel below to see the steering results.</p> <div class="l-page"> <iframe src="/2025/assets/html/2025-05-07-steering-llms-behavior/5.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p>Our observations are:</p> <ol> <li>Basically, it is possible to give instructions in English and let the LLM respond in Arabic without any additional prompts. However, this steering is very unstable, which is why we present the results of steering under different \(P_0\). Even in most of the successful responses that are answered in Arabic, there are still a small number of characters in English or other languages. We are not proficient in Arabic, so we use GPT-4o to translate these Arabic passages, revealing that the content appropriately completed the given instruction.</li> <li>Interestingly, under both the PSA and IT, an intriguing phenomenon occurs. In some cases, the LLM responds extensively in languages other than Arabic and English, including Indonesian, Spanish, Italian and Russian (All identified by GPT-4o). Moreover, it is not the case that a smaller \(P_0\) guarantees a higher likelihood of Arabic responses; we find that in some cases, responses in Arabic occur when \(P_0\) is larger, while responses in other non-English languages occur when \(P_0\) is smaller. This may be due to the concepts in low-resource languages being too close to each other or not presenting linearly, which causes the CAV based on linear models to not describe them well.</li> <li>Even on LLMs specifically fine-tuned for Arabic, such as <code class="language-plaintext highlighter-rouge">MohamedRashad/Arabic-Orpo-Llama-3-8B-Instruct</code>, the above phenomenon remains.</li> <li>The extracted Arabic CAV has good capability to let LLMs respond in English, even though we directly give Arabic instructions to them.</li> </ol> <h3 id="areas-where-cav-excels-and-does-not">Areas where CAV Excels and Does Not</h3> <p>From the results of the two concepts above, it seems that CAV is quite good at modifying the style of the entire generated content. We have attempted more style concepts based on PSA, such as telling jokes, being more creative, childish, fairy tale-like, etc. The results show that CAV performs well in steering these concepts. Try the interactive panel below to view the results of various style transfers on three specific instructions.</p> <div class="l-page"> <iframe src="/2025/assets/html/2025-05-07-steering-llms-behavior/4.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p>In addition to these concepts, we also try many other concepts, but fail to demonstrate such effects. These PSA-based CAVs all had quite high test accuracy, but after multiple attempts with different values of \(P_0\), we could never produce significantly steered responses. These concepts include:</p> <ol> <li>“You are a character from <em>Genshin Impact</em> <d-footnote>A globally released RPG game.</d-footnote>”</li> <li>“Your name is Kimi, developed by Anthropic”</li> <li>“Remember 9.11 &lt; 9.8”</li> <li>“Every word in your response must be repeated twice”</li> <li>“Your answer should include a special symbol ^_^”</li> <li>“Act as if you forget anything about <em>the Bible</em>”</li> </ol> <p>Therefore, this technique seems to be more conducive to long-context text style transformation, and is powerless in short, intellectual modifications. This implies that CAV may not be well applied to knowledge editing tasks, perhaps because the concept here is too high-level to accurately locate the knowledge at the entity level. However, some interesting phenomena can also be found from these failures:</p> <ol> <li>Since Llama-3.1 itself cannot provide consistent responses to comparison instructions like x.11 and x.8 when x varies, it is impossible to determine whether the results of our steering are reasonable. We find that \(P_0=1\%\) and \(P_0=99\%\) let the model to give completely opposite results, which somewhat aligns with our expectations. Additionally, if \(P_0\) is not set to such extreme values, it appears to have no steering effect.</li> <li>The two requirements “Every word in your response must be repeated twice” and “Your answer should include a special symbol ^_^” failed when we set the CAVs in the desired direction without explicitly asking for them in the instructions. But this doesn’t mean that the extracted CAVs are useless. If we explicitly add these two requirements in the instructions and set the CAVs in the direction of not doing so, we can indeed disable the effect of the added prompts. To prove that this is not caused by other reasons, we conduct an ablation study using CAVs trained on other concepts, and the results confirm that the effect of disabling is indeed caused by the CAV trained on corresponding concept.</li> </ol> <p>In addition, we also find that the behavior steering effect of CAV performs better in longer responses, and the first few tokens of the response seem to retain the characteristics before steering, which indicates that the steering caused by CAV requires a few tokens to adapt.</p> <h2 id="discussion">Discussion</h2> <h3 id="is-psa-induced-cav-the-same-as-it-induced">Is PSA-induced CAV the same as IT-induced?</h3> <p>Through the experiments described above, we observe that both settings could effectively extract a French concept and achieve forward and reverse behavior steering. This raises an interesting question regarding the uniqueness of the French concept. The success in both PSA and IT induction suggests a relationship akin to a sufficient and necessary condition, implying that the French concept within LLMs may be unique. To explore this further, we extend the experiment settings as follows:</p> <ul> <li><strong>Dataset A</strong>: Normal instructions. <ul> <li>How to plant flowers in my garden?</li> </ul> </li> <li><strong>Dataset B</strong>: Directly use French. (using IT) <ul> <li>Comment planter des fleurs dans mon jardin?</li> </ul> </li> <li><strong>Dataset C</strong>: Request using French. (using PSA) <ul> <li>How to plant flowers in my garden? Answer the question in French please.</li> </ul> </li> <li><strong>Dataset D</strong>: Request using English. (using PSA+IT) <ul> <li>Comment planter des fleurs dans mon jardin? Répondre en anglais.</li> </ul> </li> <li><strong>Dataset E</strong>: Unrelated postive dataset.</li> <li><strong>Dataset F</strong>: Unrelated negative dataset.</li> </ul> <p>We train CAVs using the following pairs of datasets:</p> <ul> <li>Pair #1: A-B</li> <li>Pair #2: A-C</li> <li>Pair #3: B-D</li> <li>Pair #4: C-D</li> <li>Pair #5: E-F (baseline)</li> </ul> <p>The classifiers trained on these five pairs exhibited good test set accuracy. To further understand their behavior, we examine the cosine similarity between the parameters of these classifiers:</p> <div class="l-screen" style="display: flex; justify-content: center; align-items: center; height: 100%;"> <iframe src="/2025/assets/html/2025-05-07-steering-llms-behavior/heatmap_corr.html" frameborder="0" scrolling="no" height="800px" width="1500px" margin-top="-50px"></iframe> </div> <p>Our observations are:</p> <ul> <li>Despite similar steering results, the classifiers trained seem to be rather different (see the cosine similarity between Pair #1 and #2/#3), suggesting substantial differences in their internal representations.</li> <li>The cosine similarity between Pair #2 and #3 is relatively high, which aligns with our intuition since these pairs involve similar types of instructions (requests in English or French).</li> <li>The cosine similarity between baseline and others are close to 0, demonstrating the effectiveness of the comparison and validating that the steering methods were indeed capturing meaningful differences related to the targeted language concept.</li> </ul> <h3 id="can-expand-to-multi-behavior-steering">Can expand to multi-behavior steering?</h3> <p>Based on the methodology of single-behavior steering, there are two approaches to using CAV for multi-behavior manipulation. Assume there are three target behaviors A, B, and C.</p> <ol> <li><strong>Train the CAVs for A, B, and C separately</strong>, and apply these CAVs to perturb the embedding in a certain order or in a limited loop.</li> <li>Modify the negative dataset to have instructions that simultaneously have the characteristics of behaviors A, B, and C, and <strong>train only one CAV</strong>.</li> </ol> <p>There has already been some preliminary research on this <d-cite key="scalena2024multiproperty"></d-cite>. The first method needs consider the orthogonal nature between concepts; otherwise, the probabilities will oscillate at each perturbation step and fail to converge. The second method is straightforward, requiring only the use of PSA, IT, or their combinations, but whether CAV can represent such complex content and whether it will increase the damage to text quality needs to be tested.</p> <h2 id="conclusion">Conclusion</h2> <p>In this blog, we explore the breadth and boundaries of using CAV for LLM behavior steering. Using CAV for steering, PSA and IT are good ways to construct datasets, allowing for easy export of the corresponding CAV. CAV-based steering is more suitable for tasks that require the transfer of overall text style, including the language used in the text, and has the ability to disable some brief system prompt requirements, but cannot perform more complex cognitive demands. Research on CAV steering can more effectively promote the exploration of explainable AI and low-cost text style transfer generation.</p>]]></content><author><name>Ruixuan Huang</name></author><summary type="html"><![CDATA[Concept activation vectors have been shown to take effects in safety concepts, efficiently and effectively guiding a considerable number of open-source large language models (LLMs) to respond positively to malicious instructions. In this blog, we aim to explore the capability boundaries of concept activation vectors in guiding various behaviors of LLMs through more extensive experiments. Our experiments show that this technique can transfer the text style at a low cost, but it is powerless to deal with short factual knowledge.]]></summary></entry><entry><title type="html">Analysing The Spectral Biases in Generative Models</title><link href="https://starlight345.github.io/2025/blog/analysing-the-spectral-biases-in-generative-models/" rel="alternate" type="text/html" title="Analysing The Spectral Biases in Generative Models"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://starlight345.github.io/2025/blog/analysing-the-spectral-biases-in-generative-models</id><content type="html" xml:base="https://starlight345.github.io/2025/blog/analysing-the-spectral-biases-in-generative-models/"><![CDATA[<h1 id="viewing-images-in-frequency-domain">Viewing Images in Frequency Domain</h1> <p>While we typically view images in the spatial domain—where every pixel directly represents brightness or color—another compelling perspective is to examine them in the frequency domain. By applying the 2D Discrete Fourier Transform (DFT)<d-footnote>checkout <a href="https://github.com/Inspiaaa/2D-DFT-Visualisation" target="_blank">this</a> nice demo</d-footnote> <d-cite key="schwarz2021frequencybiasgenerativemodels"> </d-cite>, we can break down an image into its frequency components, revealing hidden structures and patterns that aren’t as apparent in the spatial view. A 2D discrete fourier transform maps a grayscale image $I \in \mathbb{R}^{H \times W}$ to the frequency domain as follows :</p> \[\hat{I}[k, l] = \frac{1}{HW} \sum_{x=0}^{H-1} \sum_{y=0}^{W-1} e^{-2\pi i \frac{x \cdot k}{H}} \cdot e^{-2\pi i \frac{y \cdot l}{W}} \cdot I[x, y]\] <p>Here, $k=0,1,2…H-1$ and $l=0,1,2,…W-1$. So it outputs an image in the frequency domain of size ${H \times W}$. Here $\hat{I}[k, l]$ is a complex value at the pixel $I[x, y]$. For example, given below is a grayscale image of a baboon<d-footnote>Image taken from <d-cite key="schwarz2021frequencybiasgenerativemodels"></d-cite>.</d-footnote> viewed in the frequency domain.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/baboon_spectrum-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/baboon_spectrum-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/baboon_spectrum-1400.webp"/> <img src="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/baboon_spectrum.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To enhance power spectrum visualization, we remove the DC component, apply a Hann window<d-footnote>refer to this <a href="https://en.wikipedia.org/wiki/Hann_function?utm_source=chatgpt.com" target="_blank">article</a></d-footnote> to reduce spectral leakage, normalize by the maximum power, and use a logarithmic scale. This ensures the most powerful frequency is set to 0.<d-cite key="khayatkhoei2020spatialfrequencybiasconvolutional"></d-cite></p> <p>Frequency basically refers to the rate of change of colour in an imgae. The smooth regions (where colour or pixel intensities don’t change much) in an image correspond to low frequency while the regions containing edges and granular features (where colour changes rapidly) like hair, wrinkles etc correspond to high frequency. We can understand this by relating it to fitting 1D step functions using fourier series. The region of the step has large coefficients for the higher frequency waves while the other region has large coefficients for the low frequency waves.</p> <p>To analyse the frequency content, we estimate the Power Spectral Density (PSD) by squaring the magnitudes of the Fourier components. To visualise the spectrum as a 1D plot, the reduced spectrum S i.e. the azimuthal average over the spectrum in normalized polar coordinates $r \in [0, 1]$, $\theta \in [0, 2\pi)$ is calculated as<d-cite key="schwarz2021frequencybiasgenerativemodels"></d-cite> :</p> \[\tilde{S}(r) = \frac{1}{2\pi} \int_{0}^{2\pi} S(r, \theta) \, d\theta \quad \text{with} \quad r = \sqrt{\frac{k^2 + l^2}{\frac{1}{4}(H^2 + W^2)}} \quad \text{and} \quad \theta = \text{atan2}(k, l)\] <p>For the above image of baboon in frequency domain, the reduced spectrum is shown:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/reduced_spectrum-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/reduced_spectrum-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/reduced_spectrum-1400.webp"/> <img src="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/reduced_spectrum.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The power spectrum of natural images follows a power law i.e. $\frac{1}{f^\alpha}$ with $\alpha$ ~ 2. A more complete model of the mean power spectra (using polar coordinates) can be written as</p> \[E[|I(f, \theta)|^2] = \frac{A_s(\theta)}{f^{\alpha_s(\theta)}}\] <p>in which the shape of the spectra is a function of orientation. The function $A_s(\theta)$ is an amplitude scaling factor for each orientation and $\alpha_s(\theta)$ is the frequency exponent as a function of orientation. Both factors contribute to the shape of the power spectra<d-footnote>refer to <a href="https://web.mit.edu/torralba/www/ne3302.pdf" target="_blank">this PDF</a></d-footnote>. From here we can see that in natural images, the power spectrum is high in the low frequency region and low in the high frequency region. This is intuitive as we expect any natural image to have more smoother regions than edges and complex patterns.</p> <p>In digital imaging, understanding <strong>aliasing</strong> and the <strong>Nyquist</strong> Frequency is crucial for accurately capturing and representing details. When we convert a continuous scene, like a real world view into a digital image, we sample it at discrete intervals i.e. each pixel acts as a tiny snapshot of the scene at that point. The Nyquist Frequency, defined as half of the sampling rate, is the highest spatial frequency that can be accurately captured by this pixel grid. Let us see why. Imagine a 1D wave. To represent its frequency accurately, we need at least two samples per cycle: one to capture the crest (peak) and one to capture the trough. Since images are discretized in space, the maximum frequency is determined by the Nyquist frequency. For a square image, $H = W$, it is given by $f_{\text{nyq}} = \sqrt{k^2 + l^2} = \frac{H}{\sqrt{2}}$, i.e. for $r = 1$.</p> <p>If the details in the scene change more rapidly than this (i.e., they have a higher spatial frequency than the Nyquist limit), we will be unable to capture the shape of the actual wave and this high frequency oscillating region will be considered as a smooth low frequency region. Thus, when the pixel grid cannot capture both the crest and trough, or the rapid change of light intensity, this leads to a phenonmenon called aliasing. Aliasing causes high-frequency details to appear as misleading lower-frequency patterns or distortions, like the moiré effect<d-footnote>refer to <a href="https://en.wikipedia.org/wiki/Moir%C3%A9_pattern" target="_blank">this article</a></d-footnote> that often appears on finely patterned textures. Due to this, we capture the wrong spectrum where high frequency content is less and low frequency content is more than in the actual scene captured. This can be seen for a sinusoidal wave where we predict a wave with less frequency than the ground truth wave due to aliasing.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/aliasing-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/aliasing-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/aliasing-1400.webp"/> <img src="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/aliasing.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Image Source: <a href="https://www.youtube.com/watch?v=IZJQXlbm2dU" target="_blank">https://www.youtube.com/watch?v=IZJQXlbm2dU</a> </div> <h2 id="analysis-of-bias-in-gans">Analysis of Bias in GANs</h2> <p>Now, let us analyze spectral biases<d-cite key="rahaman2019spectralbiasneuralnetworks"></d-cite> in GANs<d-cite key="goodfellow2014generativeadversarialnetworks"></d-cite>. GANs have been quite successful in producing photo-realistic images. But things are a bit different when we view the produced images in the frequency domain. In this section we show that the ability of GANs to learn a distribution is significantly biased against high spatial frequencies i.e. GANs produce less high frequency content than in the actual image<d-cite key="lee2024spectrumtranslationrefinementimage"></d-cite>.</p> <p>This was earlier attributed to a mere scarcity of high frequencies in natural images, but recent works<d-cite key="chen2020ssdganmeasuringrealnessspatial"></d-cite><d-cite key="khayatkhoei2020spatialfrequencybiasconvolutional"></d-cite><d-cite key="schwarz2021frequencybiasgenerativemodels"></d-cite> have shown that this is not the case. There are two main hypotheses that have been proposed for the spectral biases; one attributes it to the employment of upsampling operations<d-cite key="schwarz2021frequencybiasgenerativemodels"></d-cite>, and the other attributes it to linear dependencies in the convolution filter<d-cite key="khayatkhoei2020spatialfrequencybiasconvolutional"></d-cite>, i.e., the size of the kernel deployed in the generator network. We take up these hypotheses in the remainder of this section.</p> <h3 id="setting-up-the-generative-cnn-structure">Setting Up the Generative CNN Structure</h3> <p>We’ll start by setting up the structure of a generative CNN model, which typically consists of a series of convolutional layers with filters that learn different features. Our CNN is structured as a stack of convolutional layers, with each layer represented as:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/CNN_Image-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/CNN_Image-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/CNN_Image-1400.webp"/> <img src="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/CNN_Image.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> \[H_{l+1}^i = \text{Conv}_l^i(H_l) = \sum_c F_{l}^{i,c} * \text{Up}(\sigma(H_l^c))\] <p>where:</p> <ul> <li>$\text{H}_l$ : The feature map at layer <strong><em>l</em></strong>.</li> <li>$F_{l}^{i,c}$: A convolutional filter at layer <strong><em>l</em></strong>, of size ${k_l} \times \text{k}_l$ , that connects input channel <strong><em>c</em></strong> to output channel <strong><em>i</em></strong>.</li> <li>$\text{Up}(\cdot)$ : The upsampling operator, which increases the spatial dimensions, helping generate higher-resolution outputs.</li> <li>$\sigma(\cdot)$: A non-linearity function, typically a ReLU.</li> </ul> <p><strong>Inputs:</strong> The initial feature map is represented by $H_1$ with shape $d_0$ $\times$ $d_0$ .</p> <p><strong>Parameters:</strong> The model parameters are $W$ (weights for each layer).</p> <p><strong>Layers:</strong> The network is built from convolutional layers, each generating new feature maps based on its input. Each layer also performs upsampling and non-linear transformations to increase resolution and control spatial frequencies.</p> <p>Before starting with the analysis of a filter’s spectrum, we first need to introduce the idea of viewing ReLU as a fixed binary mask. Why do we need to do this? We’ll look at it in just a moment.</p> <h3 id="relu-as-a-fixed-binary-mask">ReLU as a Fixed Binary Mask</h3> <p>Considering ReLUs to be the activation $\sigma(\cdot)$, they can then be viewed as fixed binary masks<d-cite key="khayatkhoei2020spatialfrequencybiasconvolutional"></d-cite> in the neighbourhood of the parameter $W$. Here, this means that for small variations in the parameters $W$, the activation pattern of the ReLU units (which inputs are passed and which are zeroed) does not change and since the ReLU outputs are determined by the sign of the pre-activation values, these signs only change at specific boundaries in the parameter space, thus ensuring binary mask remains fixed within any given region. We will now attempt to prove this.</p> <p>This proof has been inspired from the paper “Spatial Frequency Bias in Convolutional Generative Adversarial Networks”<d-cite key="khayatkhoei2020spatialfrequencybiasconvolutional"></d-cite> and focuses on showing that in a finite ReLU-CNN, the set of parameter configurations where the scalar output of the network crosses zero (i.e., changes sign) has a measure of zero. What is measure zero? A set of measure zero essentially means that the set occupies “negligible space” in the parameter space. In high-dimensional spaces like $\mathbb{R}^n$, measure-zero sets can often be thought of as lower-dimensional “slices” (e.g., lines, points, or surfaces) within the larger space. While they may exist mathematically, they are effectively insignificant in the context of the full parameter space.</p> <p>Mathematically, We are working with a scalar ouptut $\mathcal{f}(W)$ of a convolutional layer in a finite ReLU-CNN. Therefore, the function depends on the weight parameter <em>W</em> and the latent input $H_{1}$. Now, we need to show that for any neighbourhood around $W$, the output of the function is entirely non-negative or entirely non-positive. This means proving that the set of parameters where 𝑓 changes sign within every neighbourhood of 𝑊 (i.e., it crosses zero somewhere in every neighbourhood) has measure zero.</p> \[\implies G = \{ W \in \mathcal{W} \mid \forall \mathcal{N}(W), \exists U, V \in \mathcal{N}(W) : f(U) &lt; 0 &lt; f(V) \}\] <p>where $\mathcal{N}(W)$ represents the neighbourhood of $W$. $G$ captures the parameter values $W$ where $f(W)$ crosses zero in every neighborhood. Therefore, our objective becomes to show that $G$ has measure zero.</p> <p>A finite ReLU-CNN has a finite number of neurons and, hence, a finite number of ReLU activations. Each ReLU activation behaves like a piecewise linear function that “splits” the parameter space into regions. <br/> $\implies$ For any fixed configuration of active/inactive neurons, $f(W)$ becomes a polynomial function of $W$. Thus, for each configuration of ReLU activations, $f(W)$ behaves as a polynomial, with each configuration yielding a different polynomial form.</p> <p>A polynomial function on $\mathbb{R}^n \text{ to } \mathbb{R}$ has a measure zero set of zero-crossings in the parameter space <d-footnote> refer to <a href="https://www.researchgate.net/publication/281285245_The_Zero_Set_of_a_Polynomial" target="_blank">this ResearchGate article</a></d-footnote>. Intuitively, this means that the solutions to $f(W)=0$ occupy “negligible” space in the parameter space. <br/> $\implies$ A finite set of such polynomials also has a measure zero set of zero-crossings. $\therefore$ $G$ is also a measure zero set.</p> <p>Finally, this reasoning holds for any scalar output $f$ of the network, at any spatial location or layer. Given that there are only a finite number of such outputs in a finite network, the measure of $G$ for all outputs is still zero, thereby completing the proof.</p> <p>To summarize, the proof hinges on the fact that with ReLU activations, each layer’s output depends on whether each neuron is active or inactive. For any fixed set of active/inactive states, the network’s output behaves as a polynomial with respect to the parameters. Since polynomials only have zero-crossings on a measure zero subset of the parameter space, the overall network exhibits non-negative or non-positive output behavior <em>almost</em> everywhere in the parameter space.</p> <p>This implies that <em>almost</em> all regions of the parameter space are “stable” in terms of sign, and this stability is a result of the ReLU non-linearity creating a finite set of polynomial behaviors for 𝑓.</p> <h4 id="why-this-matters">Why This Matters</h4> <p>The key consequences and takeaways of this result are:</p> <p><strong>Simplified Frequency Control:</strong> Since the ReLUs act like fixed binary masks, they don’t introduce additional variability. The network’s spectral characteristics become easier to analyze because the ReLUs don’t actively change the frequency content in these neighbourhoods.</p> <p><strong>Shifts Control to Filters:</strong> The network’s ability to adjust the output spectrum depends more on the convolutional filters ${F}_l^{i,c}$ than on the non-linear ReLUs.</p> <h3 id="onto-the-analysis-of-filters">Onto The Analysis of Filters</h3> <p>Now that we have set up the base, we can now move on analyzing the effect of convolutional filters on the spectrum.</p> <p>The filters ${F}_l^{i,c}$ in each convolutional layer are the primary tools for shaping the output spectrum. Thus, the filters try to carve out the desired spectrum out of the input spectrum which is complicated by:</p> <ol> <li>Binary masks (ReLU) which although don’t create new frequencies, but distort what frequencies are passed onto the next layer.</li> <li>Aliasing from Upsampling.</li> </ol> <p>Now, take any two spatial frequency components \(U =\mathcal{F}_{l}^{i,c}(u_0, v_0)\) and \(V = \mathcal{F}_{l}^{i,c}(u_1, v_1)\) on the kernel $F_l$ of the $l$’th convolution layer of spatial dimension $d_l$ and filter size $k_l$, at any point during training. Let $G_l$ be a filter of dimension \(\mathbb{R}^{d_l \times d_l}\). Because of it’s dimension, it is an unrestricted filter that can hypothetically model any spectrum in the output space of the layer. Hence, we can write $F_l$ as a restriction of $G_l$ using a pulse P of area $k_l^2$ :</p> \[F_l = P.G_l\] \[P(x,y) = \begin{cases} 1, &amp; \text{if } 0 \leq x, y &lt; k_l \\ 0, &amp; \text{if } k_l \leq x,y \leq d_l \end{cases}\] <p>Applying Convolution Theorem on $F_l$:</p> \[\mathcal{F}_l = \mathcal{F}_{P} \cdot \mathcal{G}_l = \mathcal{F}\{P\} * \mathcal{F}\{G_l\}\] <p>where $\mathcal{F}(\cdot)$ represents the $d_l$ point DFT.</p> <p>From (1), the Fourier Transform of $P(x,y)$ is given by:</p> \[\mathcal{F}\{P(x, y)\}(u, v) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} P(x, y) e^{-i 2 \pi (u x + v y)} \, dx \, dy\] \[\quad \implies \mathcal{F}\{P(x, y)\}(u, v) = \int_0^{k_l} \int_0^{k_l} e^{-i 2 \pi (u x + v y)} \, dx \, dy \quad\] <p>$\text{Evaluating wrt x:}$</p> \[\int_0^{k_l} e^{-i 2 \pi u x} d x=\frac{1-e^{-i 2 \pi u k_l}}{i 2 \pi u}=k_l \operatorname{sinc}\left(\frac{u k_l}{d_l}\right)\] <p>$\text{Evaluating wrt y:}$</p> \[\int_0^{k_l} e^{-i 2 \pi v y} d y=\frac{1-e^{-i 2 \pi v k_l}}{i 2 \pi v}=k_l \operatorname{sinc}\left(\frac{v k_l}{d_l}\right)\] <p>$\text{Combining these results, the Fourier transform of P(x,y) is:}$</p> \[\mathcal{F}\{P(x, y)\}(u, v)=k_l^2 \operatorname{sinc}\left(\frac{u \kappa_l}{d_l}\right) \operatorname{sinc}\left(\frac{v k_l}{d_l}\right)\] <p>When the function is sampled, aliasing causes the spectrum to repeat periodically in the frequency domain. Each repetition of the sinc function at integer multiples of the sampling frequency creates a periodic aliasing pattern. In case of P(x,y) the function transforms into:</p> \[\operatorname{Sinc}(u, v)=\frac{\sin \left(\frac{\pi u k_l}{d_l}\right) \sin \left(\frac{\pi v k_l}{d_l}\right)}{\sin \left(\frac{\pi u}{d_l}\right) \sin \left(\frac{\pi v}{d_l}\right)} e^{-j \pi(u+v)\left(\frac{k_{l}-1}{d_l}\right)}\] <p>Here’s a breakdown of the components: <br/> $\sin(\frac{\pi u k_l}{d_l})$ : This is the sinc function scaled by the ratio of $k_l$ and $d_l$, which determines how the spatial box function in the spatial domain transforms in the frequency domain.</p> <p>The phase term $e^{-j \pi(u+v)\left(\frac{k_{l}-1}{d_l}\right)}$ : This accounts for a shift in the frequency domain. This phase shift arises due to the position of the box function in the spatial domain. This ensures that the Fourier transform reflects the correct location of the box function.</p> <p>Calculating for the correlation between $U$ and $V$:</p> \[\operatorname{Cov}[U, V]=\operatorname{Cov}\left[\operatorname{Sinc} * \mathcal{F}\left\{G_l\right\}\left(u_0, v_0\right), \operatorname{Sinc} * \mathcal{F}\left\{G_l\right\}\left(u_1, v_1\right)\right]\] <p>To expand this covariance term, we express $U$ and $V$ in terms of the sinc function and the frequency components of $G_l$:</p> \[\begin{aligned} U &amp; =\sum_{u, v} \operatorname{Sinc}(u, v) \cdot \mathcal{F}\left\{G_l\right\}\left(u_0-u, v_0-v\right) \\ V &amp; =\sum_{\hat{u}, \hat{v}} \operatorname{Sinc}(\hat{u}, \hat{v}) \cdot \mathcal{F}\left\{G_l\right\}\left(u_1-\hat{u}, v_1-\hat{v}\right) \end{aligned}\] \[\implies \operatorname{Cov}[U, V]=\operatorname{Cov}\left(\sum_{u, v} \operatorname{Sinc}(u, v) \cdot \mathcal{F}\left\{G_l\right\}\left(u_0-u, v_0-v\right), \sum_{\hat{u}, \hat{v}} \operatorname{Sinc}(\hat{u}, \hat{v}) \cdot \mathcal{F}\left\{G_l\right\}\left(u_1-\hat{u}, v_1-\hat{v}\right)\right)\] <p>This expands to: \(\operatorname{Cov}[U, V]=\sum_{u, v} \sum_{\hat{u}, \hat{v}} \operatorname{Sinc}(u, v) \operatorname{Sinc}^*(\hat{u}, \hat{v}) \cdot \operatorname{Cov}\left(\mathcal{F}\left\{G_l\right\}\left(u_0-u, v_0-v\right), \mathcal{F}\left\{G_l\right\}\left(u_1-\hat{u}, v_1-\hat{v}\right)\right)\)</p> <p>Since $G_l$ is assumed to have independent frequency components (with variance $\sigma^2$ for each component), the covariance between any two distinct components is zero, while the variance of each component is $\sigma^2$. Therefore, the covariance term simplifies because we only need to consider the terms where $(u,v) = (\hat{u},\hat{v})$:</p> \[\operatorname{Cov}[U, V]=\sum_{u, v} \operatorname{Sinc}(u, v) \operatorname{Sinc}\left(u_0-u_1-u, v_0-v_1-v\right) \cdot \sigma^2\] <p>The final covariance expression simplifies further by factoring out $\sigma^2$ and recognizing the sum as a convolution:</p> \[\operatorname{Cov}[U, V]=\sigma^2 \sum_{u, v} \operatorname{Sinc}(u, v) \operatorname{Sinc}\left(u_0-u_1-u, v_0-v_1-v\right)\] <p>Using the definition of convolution, we get:</p> \[\operatorname{Cov}[U, V]=\sigma^2 \cdot \operatorname{Sinc} * \operatorname{Sinc}\left(u_0-u_1, v_0-v_1\right)\] <p>Since the sinc function is defined over the finite output space $d_l \times d_l$, the convolution integrates to $d_l^2$, giving us:</p> \[\operatorname{Cov}[U, V]=\sigma^2 d_l^2 \operatorname{Sinc}\left(u_0-u_1, v_0-v_1\right)\] <p>Next, we calculate the variance of $U$ (or similarly for $V$, due to symmetry) using the expression for $U$ from earlier. This is computed as:</p> \[\operatorname{Var}[U]=\operatorname{Var}\left(\sum_{u, v} \operatorname{Sinc}(u, v) \cdot \mathcal{F}\left\{G_l\right\}\left(u_0-u, v_0-v\right)\right)\] <p>Using independence again, the variance simplifies to:</p> \[\operatorname{Var}[U]=\sum_{u, v}|\operatorname{Sinc}(u, v)|^2 \cdot \operatorname{Var}\left(\mathcal{F}\left\{G_l\right\}\left(u_0-u, v_0-v\right)\right)\] <p>Substituting the variance $\sigma^2$ of each independent component:</p> \[\operatorname{Var}[U]=\sigma^2 \sum_{u, v}|\operatorname{Sinc}(u, v)|^2\] <p>The sum over \(|Sinc(u,v)|^2\) evaluates to \(d_l^2k_l^2\), so:</p> \[\operatorname{Var}[U]=\sigma^2 d_l^2 k_l^2\] <p>Finally, we calculate the complex correlation coefficient between $U$ and $V$, which is defined as:</p> \[\operatorname{corr}(U, V)=\frac{\operatorname{Cov}[U, V]}{\sqrt{\operatorname{Var}[U] \operatorname{Var}[V]}}\] <p>Substituting,</p> \[\operatorname{corr}(U, V)=\frac{\sigma^2 d_l^2 \operatorname{Sinc}\left(u_0-u_1, v_0-v_1\right)}{\sqrt{\sigma^2 d_l^2 k_l^2 \cdot \sigma^2 d_l^2 k_l^2}}\] \[\implies \operatorname{corr}(U, V)=\frac{\operatorname{Sinc}\left(u_0-u_1, v_0-v_1\right)}{k_l^2}\] <p>Now, if the $U$ and $V$ frequencies are diagonally adjacent, then the correlation coefficient becomes:</p> \[\begin{equation} |\operatorname{corr}(U, V)|=\frac{\sin ^2\left(\frac{\pi k_l}{d_l}\right)}{k_l^2 \sin ^2\left(\frac{\pi}{d_l}\right)} \end{equation}\] <p>This result indicates that the correlation between two frequency components in the spectrum of $F_l$ is inversely related to the filter size $k_l$. A larger filter (i.e., higher $k_l$) reduces the correlation between frequencies, enhancing the filter’s ability to represent diverse frequencies independently. Conversely, a smaller filter (lower $k_l$) increases correlation, meaning that adjustments to one part of the frequency spectrum impact neighboring frequencies, thereby limiting the filter’s effective capacity to separate and individually adjust each frequency component.</p> <p>In each convolutional layer, the maximum spatial frequency that can be achieved is bounded by the Nyquist frequency. This means that a convolutional layer can accurately control spatial frequencies within the range $[0, \frac{d_l}{2d}]$ without aliasing. As a result, the high-frequency components are predominantly generated by the earlier layers of the CNN, which have larger spatial dimensions $d_l$. With a fixed filter size $k_l$, an increase in $d_l$ leads to higher correlations across the filter’s spectrum, thereby reducing the filter’s effective capacity to fine-tune individual frequencies. Consequently, earlier layers, responsible for creating high frequencies, face more restrictions in their spectral capacity compared to later layers with smaller $d_l$, which have greater flexibility for spectral adjustments.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Filter_Response-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Filter_Response-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Filter_Response-1400.webp"/> <img src="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Filter_Response.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Image Source: <a href="https://arxiv.org/pdf/2403.05093" target="_blank">'Spectrum Translation for Refinement of Image Generation (STIG) Based on Contrastive Learning and Spectral Filter Profile'</a> </div> <p>Moreover, while only the earlier layers can produce high frequencies without aliasing, all layers can contribute to the low-frequency spectrum without this restriction. Thus, the spatial extent of the effective filter acting on low frequencies is consistently larger than that acting on high frequencies. Even if larger filter sizes $k_l$ are used in the earlier layers to counterbalance the larger $d_l$ , low frequencies continue to benefit from a larger effective filter size compared to high frequencies, which ultimately results in lower correlation at low frequencies.</p> <p>In addition to this, some works<d-cite key="chen2020ssdganmeasuringrealnessspatial"></d-cite> show that downsampling layers also cause missing frequencies in discriminator. This issue may make the generator lacking the gradient information to model high-frequency content, resulting in a significant spectrum discrepancy between generated images and real images. Frameworks like STIG<d-cite key="lee2024spectrumtranslationrefinementimage"></d-cite> have been used to eliminate this bias.</p> <h2 id="frequency-bias-in-diffusion-models">Frequency bias in Diffusion Models</h2> <p>It has been well known that like GANs<d-cite key="goodfellow2014generativeadversarialnetworks"></d-cite>, diffusion models<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> too show some frequency bias. <strong>Smaller models fail to fit the high frequency spectrum properly whereas larger models are succesful in doing so</strong><d-cite key="yang2022diffusionprobabilisticmodelslim"></d-cite>. In general, models have a hard time fitting the reduced spectrum graph especially where the magnitude of a particular frequency is low. This is shown in the graph below :</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectral_den_diff-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectral_den_diff-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectral_den_diff-1400.webp"/> <img src="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectral_den_diff.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Spectral density graphs of real and generated image of baboon. </div> <p>Diffusion models<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> first fit the high magnitude parts (which correspond to the low frequency region in natural images). After fitting the low frequency region , it then fits the graph in the high frequency region(or low magnitude regions). Large models have enough parameters and timesteps to fit the high frequency region spectrum as well but small models struggle to do so due to lack of enough timesteps<d-cite key="yang2022diffusionprobabilisticmodelslim"></d-cite>. We shall see a modified and quite detailed version of the math proof from the paper ‘Diffusion Probabilistic Model Made Slim’<d-cite key="yang2022diffusionprobabilisticmodelslim"></d-cite> paper. We show that by taking the assumption that the denoising network acts as a linear filter, the math works out such that the reduced spectrum is first fitted for the low frequency(or high magnitude) region in the initial timesteps and later fitted for the high frequency (or low magnitude region). Assuming the denoising network as a linear filter, we get it to work as an optimal linear filter or Weiner filter. The function of this Weiner filter<d-footnote>refer to <a href="https://en.wikipedia.org/wiki/Wiener_filter" target="_blank">this article</a></d-footnote> is to minimize the mean squared error between the actual noise and the predicted noise by the filter.</p> <p>Let the input image that we want to reconstruct be $x_0$ and $\epsilon$ be white noise of variance 1. $x_t$ is the noised sample at time step t. Hence we can write</p> \[\mathbf{x}_t = \sqrt{\bar{\alpha}} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}} \epsilon\] <p>In the denoising process , let $h_t$ be the filter that is learned . So $h_t^* $ is the optimal filter which minimizes the loss function of the diffusion model<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> $L_t = |\mathbf{h}_t * \mathbf{x}_t - \epsilon|^2$ . Here $* $ denotes the standard convolution operation. The optimal filter solution can be derived in the frequency domain i.e. \(\mathcal{H}_t^*(f) = \frac{1}{\bar{\alpha} |X_0(f)|^2 + 1 - \bar{\alpha}}\). Here $\mathcal{H}_t^*(f)$ represents the frequency response of the filter. \(|X_0(f)|^2\) is the power spectrum of the original signal i.e. $X_0$, representing the magnitude of a particular frequency f in the spectrum. During the denoising phase $\bar{\alpha}$ goes from 0 to 1.</p> <p>Now if the optimal filter $\mathbf{h}_t$ is learned, then we approximate $\epsilon$ by $\mathbf{h}_t * \mathbf{x}_t$ so our noising step equation becomes:</p> \[x_t = \sqrt{\bar{\alpha}} \, x_0 + \sqrt{1 - \bar{\alpha}} \, (h_t * x_t)\] <p>Taking the DFT on both sides, we get</p> \[X_t = \sqrt{\bar{\alpha}} \, X_0 + \sqrt{1 - \bar{\alpha}} \, (H_t\times X_t)\] <p>Rearranging the equation, we get</p> \[\left( \frac{1 - \sqrt{1 - \bar{\alpha}}}{\sqrt{\bar{\alpha}}} H_t \right) X_t = X_0\] <p>Let $\left( \frac{1 - \sqrt{1 - \bar{\alpha}}}{\sqrt{\bar{\alpha}}} H_t \right)$ = $G_t$. Here $G_t$ is the frequency response of a filter $g_t$ which is the optimal linear reconstruction filter. Now this optimal filter minimises the equation :</p> \[J_t = \left| G_t X_t - X_0 \right|^2\] \[J_t = \left| G_t(\sqrt{\bar{\alpha}} \mathbf{X}_0 + \sqrt{1 - \bar{\alpha}} \epsilon )- X_0 \right|^2\] <p>The equation is approximately equal to</p> \[J_t \approx \left| X_0 \right|^2 \left| 1 - \sqrt{\overline{\alpha}} \, G_t \right|^2 + ({1 - \overline{\alpha}}) \, \left| \epsilon \right|^2 \left| G_t \right|^2\] <p>as $\epsilon$ and $X_0$ are uncorrelated. Here $\left| X_0 \right|^2$ is the power spectrum of $X_0$ and $\left| \epsilon \right|^2$ is the power spectrum of white noise which is equal to 1. So to find this optimal reconstruction filter, we differentiate this equation wrt $G_t$ and equate it to 0. We get,</p> \[\frac{\partial J_t}{\partial G_t} = 0 \implies \left| X_0 \right|^2 \left[ \left( 1 - G_t^* \sqrt{\overline{\alpha}} \right) \left( -\sqrt{\overline{\alpha}} \right) \right] + G_t^* (1 - \sqrt{\overline{\alpha}}) = 0\] <p>This gives us</p> \[G_t^* = \frac{\sqrt{\overline{\alpha}}}{\overline{\alpha} + \frac{1 - \overline{\alpha}}{|X_0|^2}}\] <p>Here $ G_t^* $ is the conjugate reconstruction filter. As it is real, $G_t^* = G_t$ . Hence, \(G_t = \frac{\sqrt{\overline{\alpha}}}{\overline{\alpha} + \frac{1 - \overline{\alpha}}{|X_0|^2}}\)</p> <p>is the optimal linear reconstruction filter. The predicted $\hat{X}_0$ = $G_t \times X_t$. So predicted power spectrum \(|\hat{X}_0|^2 = |G_t|^2 |X_t|^2\)</p> \[|X_t|^2 \approx \, \overline{\alpha} |X_0|^2 + (1 - \overline{\alpha}) |\epsilon|^2 = \overline{\alpha} |X_0|^2 + 1 - \overline{\alpha}\] <p>We can approximate it like This as $X_0$ and $\epsilon$ are uncorrelated. Now, let’s analyse the expression</p> \[|\hat{X_0}|^2 = |G_t|^2 |X_t|^2 = \frac{\overline{\alpha} \, \left| X_0 \right|^4}{\left( \overline{\alpha} \, \left| X_0 \right|^2 + 1 - \overline{\alpha} \right)^2} \, \left( \overline{\alpha} \, \left| X_0 \right|^2 + 1 - \overline{\alpha} \right)\] <p>Now, during the initial denoising stages, \(\bar{\alpha} \approx 0\). So in the low frequency region, $|X_0|^2$ is very high($|X_0|^2 \gg 1$). We make the assumption that \({\overline{\alpha}} \, |X_0| \approx 1\). So in the low frequency region, \(|\hat{X_0}|^2 \approx |X_0|^2\). In the high frequency region, $|X_0|^2$ is very low ($|X_0|^2 \to 0$). So, \(|\hat{X_0}|^2 \approx 0\). It can be clearly seen that in the inital denoising steps, the high magnitude signal is reconstructed while the low magnitude signal is approximated to zero.</p> <p>In the later stages of the denoising process, \(\bar{\alpha} \approx 1\), so regardless of the magnitude of \(|X_0|^2\), the value of \(|\hat{X_0}|^2 \approx |{X_0}|^2\)</p> <p>So we can clearly see that the model is succesfully able to learn the low frequency content in its initial denoising steps and eventually, given enough time steps, it learns the entire spectrum. But small models lack enough time steps and parameters, so only the low frequency spectrum is learnt well by the model and the predicted high frequency content is less than the ground truth. Note that the proof is based on the fact that the model has a hard time learning low magnitude regions in the power spectrum, which correspond to high frequency in natural images. But if we take a synthetic image which has low magnitude in the middle frequency region and high magnitude in the low and high frequency region, then as expected, the model fails to fit the middle region of the reduced spectrum properly. This can be seen below when we try to fit a synthetic image with two gaussian peaks by a small diffusion model<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> with limited timesteps.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectrum_syn-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectrum_syn-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectrum_syn-1400.webp"/> <img src="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectrum_syn.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The model first fits the high magnitude regions and then tries to fit the low magnitude regions , which it fails to do as the number of timesteps are small.</p> <p>There is another reason which might contribute to this bias. It is because the loss of a DDPM takes an expectation over the dataset.</p> \[\mathcal{L}_{\text{DDPM}} = \int p(\mathbf{x}_0) \, \mathbb{E}_{t, \epsilon} \left[ \left\| \epsilon - s(\mathbf{x}_t, t; \theta) \right\|_2^2 \right] d\mathbf{x}_0\] <p>Most images have smooth features and there is a small perecntage of samples have high frequency components, hence p($\mathbf{x}_0$) for such samples is low and they are down weighted in the loss function<d-cite key="yang2022diffusionprobabilisticmodelslim"></d-cite>. Due to their low weight, not much importance is given to reconstruction of high frequency components.</p> <h2 id="mitigation-of-frequency-bias-using-spectral-diffusion-model">Mitigation of Frequency Bias Using Spectral Diffusion Model</h2> <p>The main problem with diffusion models<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> is that the small vanilla U-Net cannot incorporate the dynamic spectrum into its loss function. So, the authors of the paper<d-cite key="yang2022diffusionprobabilisticmodelslim"></d-cite> introduce a spectrum-aware distillation to enable photo-realistic generation with small models. The U-Net is replaced with a Wavelet Gating module which consists of a <strong>WG-Down</strong> and <strong>WG-Up network</strong>. The WG-Down network takes the Discrete Wavelet Transform(DWT)<d-footnote>refer to this <a href="https://en.wikipedia.org/wiki/Discrete_wavelet_transform" target="_blank">article</a></d-footnote> of the input image and outputs 4 images of sub-bands. They are respectively the LL, LH, HL, HH sub-bands. In the LL sub-band,a low-pass filter is applied on the rows and columns of the image and thus captures most of the low-frequency content of the image. The LH band is created by passing a low-pass filter on the rows and high-pass filter on the columns. The HL band is created by passing a high-pass filter on the rows and a low-pass filter on the columns of the image.Finally, the HH sub-band is created by passing a high-pass filter on both rows and columns. In essence, the LL sub-band captures the low-frequency details i.e. an approximation of the image, while the LH, HL, HH sub-bands capture the high-frequency details of the image.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Spectral_diffusion-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Spectral_diffusion-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Spectral_diffusion-1400.webp"/> <img src="/2025/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Spectral_diffusion.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Image Source: <a href="https://arxiv.org/abs/2211.17106" target="_blank">'Diffusion Probabilistic Model Made Slim'</a> </div> <p>The input image $X$ of size $H \times W \times C$ is divided into its corresponding 4 sub-bands (each of size $H/2 \times W/2 \times C$). Next, a soft-gating operation is used to weight these 4 sub-bands and the output feature $X’$ is produced as follows:</p> \[X' = \sum_{i \in \{LL, LH, HL, HH\}} g_i \odot X_i\] <p>Here, $\odot$ represents element-wise multiplication. The gating mask is learnt using a feed-forward network.</p> \[g_{\{LL, LH, HL, HH\}} = \text{Sigmoid}(\text{FFN}(\text{Avgpool}(\mathbf{X})))\] <p>In the WG-Up, the input feature is splitted into 4 chunks as the wavelet coefficients. Then, WG is carried out to re-weight each sub-band as before:</p> \[X' = \text{IDWT}(g_{LL} \odot X_{LL}, g_{LH} \odot X_{LH}, g_{HL} \odot X_{HL}, g_{HH} \odot X_{HH})\] <p>Here IDWT is the inverse Discrete wavelet transform. Hence, they provide the model information of the frequency dynamics as well during training as the network decides which components to pay more attention to while learning the gating mask.</p> <p>Another method that the authors apply is spectrum-aware distillation. They distill knowledge from a large pre-trained model into the WG-Unet. They distill both spatial and frequency knowledge into the WG-Unet using a spatial loss and a frequency loss. Let a noised image $X_t$ be passed into the network. The spatial loss is calculated as:</p> \[\mathcal{L}_{\text{spatial}} = \sum_{i} \| \mathbf{X}^{(i)}_T - \mathbf{X}^{(i)}_S \|^2_2\] <p>where $\mathbf{X}^{(i)}_T$ and $\mathbf{X}^{(i)}_S$ stand for the pair of teacher/student’s output features or outputs of the same scale. A single $1 \times 1$ <strong>Conv</strong> layer is used to align the dimensions between a prediction pair.</p> <p>The frequency loss is then calculated as :</p> \[\mathcal{L}_{\text{freq}} = \sum_i \omega_i \left\| \mathcal{X}_T^{(i)} - \mathcal{X}_S^{(i)} \right\|_2^2, \quad \text{where } \omega = \left| \mathcal{X}^{(i)} \right|^{\alpha}\] <p>Here, $\mathcal{X}_T^{(i)}$, $\mathcal{X}_S^{(i)}$ and $\mathcal{X}_0^{(i)}$ represent the 2D DFT of $\mathbf{X}^{(i)}_T$, $\mathbf{X}^{(i)}_S$ and the resized clean image $X_0$ respectively. The scaling factor $\alpha$ is $-1$. We multiply the loss with $\omega_i$ as it gives more weight to high frequency content ($\mathcal{X}_0^{(i)}$ is low, hence $\omega_i$ is high) and less weight to low-frequency content.</p> <p>So the final loss is:</p> \[\mathcal{L} = \mathcal{L}_{\text{DDPM}} + \lambda_s \mathcal{L}_{\text{spatial}} + \lambda_f \mathcal{L}_{\text{freq}}\] <p>Here $\lambda_s$ = 0.1 and $\lambda_f$ = 0.1.</p> <p>It was found that the frequency term in the loss function accounts for the largest change in FID, showing its importance in high-quality image generation. Thus using spectral knowledge during training helps a small network to produce high-quality realistic images and eliminate the ‘bias’ problem in diffusion models<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite>.</p> <h1 id="conclusion">Conclusion</h1> <p>In this article, we looked at another domain of viewing images and processing them i.e. the frequency domain. We then shed light into how the generative models posses certain biases in the frequency domain, particularly bias against high frequency content generation. We try to explain the reason behind this by breaking down the architecure of GANs<d-cite key="goodfellow2014generativeadversarialnetworks"></d-cite> and diffusion models<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> and look at how the math behind these model’s working may lead to these observations. Finally, we discussed an architecture to mitigate these issues.</p>]]></content><author><name>Amitoj Singh Miglani</name></author><summary type="html"><![CDATA[Diffusion and GAN models have demonstrated remarkable success in synthesizing high-quality images propelling them into various real-life applications across different domains. However, it has been observed that they exhibit spectral biases that impact their ability to generate certain frequencies and makes it possible to distinguish real images from fake ones. In this blog we analyze these models and attempt to explain the reason behind these biases.]]></summary></entry><entry><title type="html">A primer on analytical learning dynamics of nonlinear neural networks</title><link href="https://starlight345.github.io/2025/blog/analytical-simulated-dynamics/" rel="alternate" type="text/html" title="A primer on analytical learning dynamics of nonlinear neural networks"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://starlight345.github.io/2025/blog/analytical-simulated-dynamics</id><content type="html" xml:base="https://starlight345.github.io/2025/blog/analytical-simulated-dynamics/"><![CDATA[<h2 id="background">Background</h2> <p>The dynamics of learning in artificial neural networks capture how the parameters of a network change over time during training as a function of the data, architecture, and training algorithm. The internal representations and external behaviors of the network evolve during training as a consequence of these dynamics. In neural networks with at least one hidden layer, these dynamics are <em>nonlinear</em> even when the activation function is linear <d-cite key="saxe2014exact"></d-cite>, making them challenging to characterize even qualitatively. Nevertheless, understanding learning dynamics is central to machine learning research, as they determine how networks acquire useful features that generalize to unseen data through the data-driven optimization process of gradient descent on the training objective.</p> <h3 id="empirical-studies-of-learning-dynamics">Empirical studies of learning dynamics</h3> <p>Empirical work uses simulations of training dynamics to visualize and characterize the geometry of neural network loss landscapes and the trajectories networks take during optimization. Early work by Choromanska et al. <d-cite key="choromanska2015loss"></d-cite> and Goodfellow et al. <d-cite key="goodfellow2015qualitatively"></d-cite> challenged the notion that local minima were a significant obstacle to optimization, finding that most local minima have similar loss values. Garipov et al. <d-cite key="garipov2018loss"></d-cite> further showed that different solutions are often connected by simple paths in parameter space. Visualization techniques developed by Li et al. <d-cite key="li2018visualizing"></d-cite> and Sagun et al. <d-cite key="sagun2018empirical"></d-cite> revealed the structure of these landscapes, showing how network width and depth affect the loss geometry. Fort et al. examined how networks traverse these landscapes <d-cite key="fort2019goldilocks"></d-cite> and characterizing the role of network width in determining training trajectories <d-cite key="fort2020deep"></d-cite>. More recent work by Entezari et al. <d-cite key="entezari2022rolea"></d-cite> has worked connected these empirical observations to theoretical frameworks for understanding optimization dynamics.</p> <h3 id="theoretical-analyses-of-learning-dynamics">Theoretical analyses of learning dynamics</h3> <p>Theoretical approaches to understanding neural network learning dynamics use mathematical tools to describe how network parameters evolve during training. These analyses have revealed two distinct training regimes. The first regime, known as <em>lazy training</em> <d-cite key="chizat2019lazy"></d-cite>, occurs when network parameters stay close to their initialization throughout training. In this regime, the network behaves similarly to a kernel method, with dynamics characterized by the <em>neural tangent kernel</em> <d-cite key="jacot2020neural"></d-cite><d-cite key="arora2019finegrained"></d-cite><d-cite key="allen-zhu2020learning"></d-cite>.</p> <p>The second regime, termed <em>feature learning</em> <d-cite key="yang2022feature"></d-cite>, captures more complex dynamics in which networks substantially modify their internal representations during training as a function of the task. Even seemingly simple architectures like deep linear networks can exhibit rich feature learning dynamics, including distinct learning phases where different hierarchical features emerge rapidly followed by plateaus of little progress <d-cite key="saxe2014exact"></d-cite>. The transition between the rich and lazy regimes depends on the interplay between factors such as the network width, learning rate, and initialization scale <d-cite key="yang2023spectral"></d-cite>, and the dynamics can transition between these regimes during training <d-cite key="kunin2024get"></d-cite>, resulting in drastic changes in generalization behavior <d-cite key="kumar2023grokking"></d-cite>. The feature learning regime is particularly relevant for understanding the success of deep learning in practice, where networks learn representations of complex data distributions with an effectiveness that is not yet fully understood.</p> <h3 id="statistical-physics-for-learning-dynamics">Statistical physics for learning dynamics</h3> <p>Statistical physics offers tools for characterizing macroscopic behavior emerging from collections of microscopic particles <d-cite key="helias2019statistical"></d-cite> <d-cite key="urbani2024statistical"></d-cite>. Early pioneering work by Gardner applied these techniques to neural networks by viewing neurons as the microscopic particles in a complex system—the neural network <d-cite key="gardner1989three"></d-cite>. The primary goal of many statistical physics approaches to learning dynamics is to derive an exact equation for time-varying generalization error through reduction to macroscopic variables to be defined. These analyses can be performed under various neural network parameter regimes, often considering asymptotic limits (such as infinite width or infinite input dimension) where the system concentrates—exhibiting fewer fluctuations in a precise sense—leading to simpler dynamical descriptions; see Cui <d-cite key="cui2024highdimensional"></d-cite> for a review of these regimes.</p> <h4 id="our-focus-the-teacher-student-setting">Our focus: The teacher-student setting</h4> <p>The teacher-student framework, introduced by Gardner <d-cite key="gardner1989three"></d-cite>, provides perhaps the simplest setting for studying neural network learning with statistical physics techniques. In this paradigm, a student network learns to mimic a fixed teacher network that generates labels for training data drawn from a given input distribution. Classical analytical results were achieved by Saad &amp; Solla <d-cite key="saad1995online"></d-cite> and Riegler and Biehl <d-cite key="riegler1995online"></d-cite> in the 1990s, who derived exact equations for the generalization dynamics in this teacher-student setup with a particular scaling. To enable solvability, these analyses require specific assumptions about the data distribution and the network architecture—in particular, that the network inputs are Gaussian and that the input dimension is large relative to the number of hidden neurons. Despite these constraints, this framework allow granular study of various learning regimes, including overparameterization (termed <em>realizability</em> in Saad &amp; Solla <d-cite key="saad1995online"></d-cite>) and a feature-learning phenomena termed <em>specialization</em>.</p> <p>Much recent work builds on these classical analyses to expand the frontier of solvable training regimes, and these techniques have also found applications beyond generalization error dynamics. We detail both directions in the final section of this blog post. First, we provide a pedagogical introduction to the classicial analytical results, focusing on the derivation of Saad &amp; Solla <d-cite key="saad1995online"></d-cite> for its clarity.</p> <h2 id="methods">Methods</h2> <p>In this section, we introduce the teacher-student setup of Saad &amp; Solla <d-cite key="saad1995online"></d-cite> to prepare for the next section, where we rederive the analytical learning dynamics of the student network. To complement the derivations, we include code snippets for computing the time-evolution of the macroscopic variables describing the learning dynamics efficiently in JAX <d-cite key="jax2018github"></d-cite>, which we use in the next-after-next section to test the theory-experiment overlap of the generalization error dynamics targeted by Saad &amp; Solla <d-cite key="saad1995online"></d-cite>.</p> <h3 id="the-teacher-student-setup">The teacher-student setup</h3> <p>In the teacher-student setting of Saad &amp; Solla <d-cite key="saad1995online"></d-cite>, the data-generating process used to train the student network is described by a distrubution over inputs $x$ and a teacher providing target outputs $y$. Saad &amp; Solla <d-cite key="saad1995online"></d-cite> focus on the online learning (also termed <em>stochastic</em> gradient descent) setting, where new samples are drawn from the data-generating process at random. In this setting, a batch size greater than one has no substantial effect on the dynamics except to reduce the noise in the gradient estimate. As such, in simulations we use minibatch stochastic gradient descent and sample multiple $(x_{s}, y_{s})^{u}$ pairs to fill a batch $s = 1, \ldots, B$. and we consider updates of the student network using gradient descent iterations indexed by $u$. As such, in the case of \(B=1\), \(u\) can be understood simultanesouly as the iteration index in the training process and the example index (<em>i.e.</em>, the number of examples seen so far in the training process).</p> <p>The teacher, including that of Saad &amp; Solla <d-cite key="saad1995online"></d-cite>, is generally defined as</p> \[\begin{equation} y_{s}^{u} = f^{*}(x_{s}^{u}, W^{*}) + \sigma \xi^{u}~, \end{equation}\] <p>where \(f^{*}( \cdot , W^{*})\) is the mapping defining the teacher with parameters \(W^{*}\) of dimension to be defined, \(\xi^{u} \sim \mathcal{N}(0, 1)\), and \(\sigma \in \mathbb{R}\) scales the output noise.</p> <p>The student network is generally defined as</p> \[\begin{equation} \hat{y}_{s}^{u} = f(x_{s}^{u}, W), \end{equation}\] <p>where \(f( \cdot , W)\) is the mapping defining the student, and \(W\) are the parameters of the student of dimension to be defined. In teacher-student, \(f\) and \(f^{*}\) usually share the same parameterizaton to enable finegrained comparison between the learning of the student and the teacher defining the task. Commonly, they are both neural networks and the parameters \(W\) and \(W^{*}\) are the weights of the networks. In Saad &amp; Solla <d-cite key="saad1995online"></d-cite>, both the teacher and student networks were modeled as a soft-committee machine, which is a sum of nonlinear perceptrons. As such, the student and teacher networks had parameters of dimension \(W \in \mathbb{R}^{K \times N}\) and \(W^{*} \in \mathbb{R}^{M \times N}\), respectively, where \(K\) and \(M\) are the number of neurons in the student and teacher networks, respectively.</p> <p>To train the student network, Saad &amp; Solla <d-cite key="saad1995online"></d-cite> consider gradient descent to improve on the mean squared error between teacher and student outputs at iteration $u$:</p> \[\begin{equation} \mathcal{L}^{u} = \frac{1}{2B} \sum_{s=1}^{B} \left( \hat{y}_{s}^{u} - y_{s}^{u} \right)^{2}~, \end{equation}\] <p>where samples to fill a batch consist of $(x_{s}, y_{s})^{u}$ pairs with $i = 1, \ldots, B$, $x_{s}^{u} \sim \mathcal{N}(0, I)$ of dimension $N$, and the target $y_{s}^{u}$ is generated by feeding $x_{s}^{u}$ to the teacher network. The weights of the student network are updated using gradient descent as</p> \[\begin{equation} W^{u+1} = W^{u} - \eta_{w} \frac{\partial \mathcal{L}^{u}}{\partial W}~, \end{equation}\] <p>where $\eta_{W}$ is the learning rate for parameters \(W\). This procedure can be shown to converge to near-zero training error (up to irreducible error due to noise ) in the limit of infinite data and infinitesmal learning rate if the student is sufficiently parameterized with respect to the teacher <d-cite key="saad1995online"></d-cite>.</p> <h3 id="gradient-flow-dynamics-learning-with-infinitesimal-step-size">Gradient flow dynamics: Learning with infinitesimal step size</h3> <p>One way to analyze learning dynamics of neural networks like that denoted in (4) is treat the optimization process as a dynamical system where the gradient descent updates effectively evolve through continuous time as the parameters of a dynamical system. This transformation is commonly known as the <em>gradient flow limit</em> <d-cite key="bach2020effortless"></d-cite>, where the discrete gradient descent updates become continuous when the learning rate is small, giving</p> \[\begin{equation} \frac{\mathrm{d}W}{\mathrm{d}t} = - \left\langle \frac{\partial \mathcal{L}^{u}}{\partial W} \right\rangle_{x,y} \end{equation}\] <p>where \(\left\langle \cdot \right\rangle_{x,y}\) is physics notation for the expectation taken over the distribution of the data. In the gradient flow limit, the generalization error at each step can be written as</p> \[\begin{equation} \mathcal{E}(W^u) = \frac{1}{2} \left\langle \left( \hat{y}^u - y \right)^{2} \right\rangle_{x,y}~. \end{equation}\] <p>One way to think about the limit defined by (5) is by considering that as the learning rate gets smaller, the amount of data observed by the network at a fixed timescale increases, becoming virtually infinite when the learning rate is zero. This converts the finite average over data in the loss function in (3) to an expectation over the data as in (6).</p> <h4 id="the-nonlinear-gradient-flow-dynamics-of-teacher-student-are-solvable">The nonlinear gradient flow dynamics of teacher-student are solvable</h4> <p>It is possible to solve the system of ordinary differential equations in (5) for several classes of deep linear networks <d-cite key="saxe2014exact"></d-cite> <d-cite key="braun2022exact"></d-cite> <d-cite key="shi2022learning"></d-cite> <d-cite key="tu2024mixed"></d-cite> at finite width. Happily, using a teacher-student setup allows for the derivation of a closed-form expression of the learning dynamics, even for <em>nonlinear</em> networks with a hidden layer. To achieve this, the above differential equation can be written in terms of specific <strong>order parameters</strong>, which sufficiently describe the state of the learning dynamics at each time step. Order parameters are commonly understood in physics as macroscopic variables that describe the time evolution of a complex system with many microscopic parts, in a way that is convenient for further mathematical analysis.</p> <p>In the next sections, we will rederive the dynamical equations for two paradigmatic cases of the teacher-student setting, the classical case of Saad and Solla <d-cite key="saad1995online"></d-cite> where the teacher and student are soft-committee machines (an average of nonlinear perceptrons), and that of Goldt et al. <d-cite key="goldt2020dynamics"></d-cite>, which extends these results to allow for nonlinear neural networks with a hidden layer.</p> <h2 id="rederivations">Rederivations</h2> <p>In this section, we present a pedagogical tour of the analytical framework of to characterize the gradient descent learning dynamics of the student network in terms of its order parameters, resolving some inconsistencies in the original derivations <d-cite key="saad1995online"></d-cite> and in follow-up extensions <d-cite key="goldt2020dynamics"></d-cite>.</p> <h3 id="solvable-learning-dynamics-for-the-soft-committee-machine">Solvable learning dynamics for the soft committee machine</h3> <p>To solve the system of ordinary differential equations in (5), we need to assume a specific form for the teacher and student networks and convert the dynamical equation that describes gradient descent to the corresponding order parameters equations. In Saad and Solla’s work <d-cite key="saad1995online"></d-cite>, both the teacher and student networks were modeled as a soft-committee machine, which is an average of nonlinear perceptrons. We define the teacher and student networks as follows, using modified notation for clarity:</p> \[\begin{equation} y_{s}^{u} = \sum_{m=1}^{M} g\left( \frac{W^{*}_{m} x_{s}^{u}}{\sqrt{N}} \right) + \sigma \xi^{u} \end{equation}\] \[\begin{equation} \hat{y}_{s}^{u} = \sum_{k=1}^{K} g\left( \frac{W_{k} x_{s}^{u}}{\sqrt{N}} \right) \end{equation}\] <p>where $g( \cdot )$ is the activation function, \(m\) and \(k\) index the perceptrons in the teacher and student (rows of \(W^*, W \in \mathbb{R}^{1 \times N}\)), and $M$ and $K$ are the number of neurons in the teacher and student networks, respectively. Saad and Solla <d-cite key="saad1995online"></d-cite> present closed-form solutions for $g( \cdot )$ as a <a href="https://en.wikipedia.org/wiki/Error_function">Gauss error function</a> nonlinearity. From here on, neuron indexing will be $i, j, k$ for the student, and $m, n, p$ for the teacher.</p> <p>To train the student, we minimize the mean squared error between the teacher and student outputs:</p> \[\begin{align} \mathcal{L^{u}} = &amp; \frac{1}{2B} \sum_{i=1}^{B} \left( y_{s}^{u} - \hat{y}_{s}^{u} \right)^{2} \\ = &amp; \frac{1}{2B} \sum_{s=1}^{B} \left[ \sum_{m=1}^ {M} g\left( \frac{W^{*}_{m}x_{s}^{u}}{\sqrt{N}} \right) + \sigma \xi^{u} -\sum_{k=1}^{K} g\left( \frac{W_ {k}x_{s}^{u}}{\sqrt{N}} \right) \right]^{2} \end{align}\] <p>We then perform gradient descent to update the student’s weights:</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial W_{i}} = &amp; \frac{1}{B}\sum_{s=1}^{B} \left[\sum_{m=1}^ {M} g\left( \frac{W^{*}_{m}x_{s}^{u}}{\sqrt{N}} \right) + \sigma \xi ^{u} -\sum_{k=1}^{K} g\left( \frac{W _{k}x_{s}^{u}}{\sqrt{N}} \right) \right] \cdot \left( -g'\left( \frac{W_{i}x_{s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u}}{\sqrt{N}} \right) \\ = &amp; - \frac{1}{B}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left( \frac{W_{i}x_{s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u}}{\sqrt{N}}~. \end{align}\] <p>with \(\Delta_{s}^{u} = \sum_{m=1}^{M} g\left( \frac{W^{*}_{m}x_{s}^{u}}{\sqrt{N}} \right) + \sigma \xi^{u} - \sum_{k=1}^{K} g\left( \frac{W_{k}x_{s}^{u}}{\sqrt{N}} \right)\). Hence, the gradient descent update equations for the student network are</p> \[\begin{equation} W_{i}^{u+1} = W_{i}^{u} + \frac{\eta_{w}}{B}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left( \frac{W_{i}x_ {s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u}}{\sqrt{N}}~. \end{equation}\] <p>From this expression, we could take the gradient flow limit as in (5). However, the expectation over the data distribution induced in the right hand side does not have a closed form solution in this case. Instead, we can write the update equation in terms of <strong>order parameters</strong>, which fully define the state of the system, and for which this expectation has a solution. We emphasize that the order parameters are a design choice to make analytical results tractable, and the choice of order parameters is not always obvious in any given problem. Saad &amp; Solla <d-cite key="saad1995online"></d-cite> choose the order parameters to be the overlap between student and teacher neurons $R$, the overlap of student neurons with themselves $Q$, and the overlap of teacher neurons with themselves $T$ (which do not change throughout training as the teacher is fixed), defined as</p> \[\begin{equation} R = \frac{W^{*}W^{T}}{N}, \hspace {0.2cm} Q = \frac{W W^{T}}{N} \hspace{0.2cm} \text{and} \hspace{0.2cm} T = \frac{W^{*}(W^{*})^{T}}{N}. \end{equation}\] <p>Instead of describing the learning using the gradient descent updates for the weights, we can describe it in terms of the order parameters in (14). To do this, we can simply multiply the gradient updates equation by \((W^{*}_{n})^{T}/N\) to obtain $R$ updates and by \((W_{j}^{u+1})^{T}/N\) to obtain $Q$ updates. Starting with the $R$ updates, we have</p> \[\begin{align} \frac{W_{i}^{u+1}(W_{n}^{*})^{T}}{N} &amp; = \frac{W_{i}^{u}(W_{n}^{*})^{T}}{N} + \frac{\eta_{w}}{NB}\sum_{s= 1}^{B} \Delta_{s}^ {u} \cdot g'\left( \frac{W_{i}x_{s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u} (W_{n}^{*})^{T}}{\sqrt{N }}~, \\ R_{in}^{u+1} &amp; = R_{in}^{u} + \frac{\eta_{w} \mathrm{d}t}{B}\sum_{s=1}^{B} \Delta_{s}^ {u} \cdot g'\left( \frac{W_{i}x_{s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u} (W_{n}^{*})^{T}}{\sqrt{N}}~. \end{align}\] <p>From this equation, we define $\mathrm{d}t=1/N$, and by moving $R_{in}^{u}$ to the left hand side, dividing by $\mathrm{d}t$, and taking the <em>thermodynamic limit</em> $N \rightarrow \infty$ corresponding to large input dimension, we obtain the time derivative of $R_{in}$ as</p> \[\begin{equation} \frac{\mathrm{d} R_{in}}{\mathrm{d} t} = \eta_{w} \left&lt; \Delta_{s}^{u} g'(\lambda_{i}^{u}) \rho_{n}^{u} \right&gt; \end{equation}\] <p>where we define the <em>local fields</em></p> \[\begin{equation} \lambda_{i}^{u} = \frac{W_{i}x_{s}^{u}}{\sqrt{N}} \hspace{0.3cm} \text{and} \hspace{0.3cm} \rho_{n}^{u} = \frac{(W_ {n}^{*})^{T}x_{s}^{u}}{\sqrt{N}}. \end{equation}\] <p>The equation for $\frac{\mathrm{d}R_{in}}{\mathrm{d}t}$ is now in a convenient form, where the local fields are simply a Gaussian scalar since $x \sim \mathcal{N}(0, I)$, and so the expectation because an integral over Gaussian distribution with covariances defined by the order parameters. Before solving this expectation, let’s derive the same equation for the order parameters $Q$ (slightly trickier). We go back to the gradient descent update equation for the weights, and multiply by $(W_{j}^{u+1})^{T}/N$ giving</p> \[\begin{align} \frac{W_{i}^{u+1}(W_{j}^{u+1})^{T}}{N} &amp; = \frac{W_{i}^{u}(W_{j}^{u+1})^{T}}{N} + \frac{\eta_{w}}{NB}\sum _{s=1}^{B} \Delta_{s}^{u} \cdot g'\left( \lambda_{i}^ {u} \right) \cdot \frac{x_{s}^{u}}{\sqrt{N}}(W_{j}^{u+1})^{T}, \\ Q^{u+1}_{ij} &amp; = \frac{W_{i}^{u}}{N}\left( W_{j}^{u} + \frac{\eta_{w}}{B}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left ( \frac{W_{j}x_{s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u}}{\sqrt{N}} \right)^{T} \\ &amp; + \frac{\eta_{w}}{NB}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left( \lambda_{i}^ {u} \right) \cdot \frac{ x_{s}^{u}} {\sqrt{N}} \left( W_{j}^{u} + \frac{\eta_{w}}{B}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left ( \frac{W_{j}x_{s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u}}{\sqrt{N}} \right)^{T}, \\ Q^{u+1}_{ij} &amp; = Q^{u}_{ij} + \frac{\eta_{w}dt}{B}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left( \lambda_{j }^ {u} \right) \lambda_{i}^{u} + \frac{\eta_{w}dt}{B}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left( \lambda_{i}^{u } \right) \lambda_{j}^{u} \\ &amp; + \frac{\eta_{w}^{2}dt}{B^{2}}\sum_{s=1}^{B}\sum_{s'=1}^{B} \Delta_{s}^{u} \Delta_{s'}^{u}g'\left( \lambda_{i}^{u} \right)g'\left( \lambda_{j}^{u} \right) \frac{x_{s}^{u}(x_{s}^{u})^{T}}{N}. \end{align}\] <p>Now dividing by $\math{d}t$ and taking the limit $N \rightarrow \infty$, (hence $\mathrm{d}t \rightarrow 0$), $\frac{x_{s }^{u}(x_{s} ^{y})^{T}}{N} \rightarrow 1 $ by the central limit theorem, and expectations over \(s\) and \(s'\) are $0$ as they are independent samples, we obtain the time derivative of $Q_{ij}$ as</p> \[\begin{equation} \frac{\mathrm{d}Q_{ij}}{\mathrm{d}t} = \eta_{w} \left&lt; \Delta_{s}^{u} g'(\lambda_{j}^{u}) \lambda_{i}^{u} \right&gt; + \eta_{W } \left&lt;\Delta_{s}^{u} g'(\lambda_{i}^{u}) \lambda_{j}^{u} \right&gt; + \eta_{w}^{2} \left&lt; (\Delta_{s}^{u})^{2} g'(\lambda_{i}^{u}) g'(\lambda_{j}^{u}) \right&gt;. \end{equation}\] <p>Finally, having the order parameters, we can write the generalization error from (6), the expected mean squared error between teacher and student over the entire data distribution, as</p> \[\begin{align} \mathcal{E} = &amp; \frac{1}{2} \left&lt; \left( \hat{y} - y \right)^{2} \right&gt; \\ = &amp; \frac{1}{2} \sum_ {i=1}^{K} \sum_{j=1}^{K} \left&lt; g(\lambda_{i}^{u}) g(\lambda_{j}^{u}) \right&gt; - \sum_{m=1}^{M} \sum_{i=1}^{K} \left&lt; g(\rho_{m}^{u}) g(\lambda_{i}^{u}) \right&gt; + \frac{1}{2} \sum_{m=1}^{M} \sum_{n=1}^{M} \left&lt; g(\rho_{m}^{u}) g(\rho_ {n}^{u}) \right&gt; + \frac{\sigma^{2}}{2} \end{align}\] <p>Here, we encounter the first expectation that can be integrated in closed form for $g$ defined as the error function. Now, we introduce the useful expectations that will appear in the computation of expectations in the generalization error and order parameters:</p> \[\begin{align} I_{2}(a, b) &amp; = \left&lt; g(\nu_{a}) g(\phi_{b}) \right&gt; \\ I_{3}(a, b, c) &amp; = \left&lt; g'(\nu_{a}) \phi_{b} g(\psi_{c}) \right&gt; \\ I_{4}(a, b, c, d) &amp; = \left&lt; g'(\nu_{a}) g'(\phi_{b}) g(\psi_{c}) g(\gamma_{d}) \right&gt; \\ J_{2}(a, b) &amp; = \left&lt; g'(\nu_{a}) g'(\phi_{b}) \right&gt;~. \end{align}\] <p>These expectations can be solved in closed form as a function of the covariance between each of the variables $\nu_ {a}, \phi_{b}, \psi_{c}$ and $\gamma_{d}$. Let’s start with an example from the terms in the generalization error. First, closed form expression for $I_{2}$, with $g$ as the Gauss error function, is</p> \[\begin{align} I_{2}(a, b) = &amp; \frac{2}{\pi} \text{arcsin}\left( \frac{C_{ab}}{\sqrt{1 + C_{aa}} \sqrt{1+C_{bb}}} \right) \end{align}\] <p>where $C_{ab}$ is the covariance between $\nu_{a}$ and $\phi_{b}$, and $C_{aa}$ and $C_{bb}$ are the variances of $\nu_{a}$ and $\phi_{b}$, respectively. The way to select the correct covariance structure, is to look at the arguments of the expectation. Recall the index notation, $i, j, k$ for the student, and $m, n, p$ for the teacher, then $a$ and $b$ can be any of these indices depending on the corresponding local field. For instance, if $a=k$, the notation implies that $\nu = \lambda$, if $b=m$, then $\phi = \rho$. From here, we can write the generalization error in terms of this integral</p> \[\begin{align} \mathcal{E} = &amp; \frac{1}{2} \sum_{i=1}^{K} \sum_{j=1}^{K} I_{2}(i, j) - \sum_{n=1}^{M} \sum_{i=1}^{K} I_{2}(i, n)+ \frac{1}{2} \sum_{m=1}^{M} \sum_{n=1}^{M} I_{2}(n, m) + \frac{\sigma^{2}}{2}~. \end{align}\] <p>The covariances between the local fields are defined by the order parameters. For instance, the covariance for \(I_{2} (i, n)\) (student-teacher indexes) is \(C_{12} = R_{in}\), \(C_{11}=\text{diag}(Q)_{i}\) and \(C_{22}=\text{diag}(T)_{n}\), or the covariance for \(I_{2}(i, j)\) is \(C_{12}=Q_{ij}\), \(C_{11}=\text{diag}(Q)_{i}\) and \(C_{22}=\text{diag}(Q)_{j}\). In other words, the covariance structure for these expectation is given by the local field covariance, which are the order parameters. Hence, we can take advantage of broadcasting to compute all elements of \(I_{2}\) matrix as a function of the corresponding order parameters matrices:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_I2</span><span class="p">(</span><span class="n">c12</span><span class="p">,</span> <span class="n">c11</span><span class="p">,</span> <span class="n">c22</span><span class="p">):</span>
    <span class="n">e_c11</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">c11</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">e_c22</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">c22</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">e_c11</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">e_c22</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arcsin</span><span class="p">(</span><span class="n">c12</span> <span class="o">/</span> <span class="n">denom</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span>
</code></pre></div></div> <p>and the generalization error being</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">order_parameter_loss</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="c1"># Student overlaps
</span>    <span class="n">I2_1</span> <span class="o">=</span> <span class="nf">get_I2</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">Q</span><span class="p">)</span>
    <span class="n">first_term</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">I2_1</span><span class="p">)</span>

    <span class="c1"># Student teacher overlaps
</span>    <span class="n">I2_2</span> <span class="o">=</span> <span class="nf">get_I2</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
    <span class="n">second_term</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">I2_2</span><span class="p">)</span>

    <span class="c1"># Teacher overlaps
</span>    <span class="n">I2_3</span> <span class="o">=</span> <span class="nf">get_I2</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
    <span class="n">third_term</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">I2_3</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">first_term</span><span class="o">/</span><span class="mi">2</span> <span class="o">-</span> <span class="n">second_term</span> <span class="o">+</span> <span class="n">third_term</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span>
</code></pre></div></div> <p>$I_{3}$, $I_{4}$ and $J_{2}$ follow a similar structure, with the covariance structure defined by the order parameters between the arguments of the expectation. These integrals appear in the order parameters dynamical equation by expanding the error signal $\Delta_{s}^{u}$, giving</p> \[\begin{align} \frac{\mathrm{d}R_{in}}{\mathrm{d}t} &amp; = \eta_{w} \left[ \sum_{m=1}^{M} I_{3}(i,n,m) - \sum_{j=1}^{K} I_{3}(i, n, j) \right] \\ \frac{\mathrm{d}Q_{ik}}{\mathrm{d}t} &amp; = \eta_{w} \left[ \sum_{m=1}^{M} I_{3}(i,k,m) - \sum_{j=1}^{K} I_{3}(i, k, j) \right] \\ &amp; + \eta_{w} \left[ \sum_{m=1}^{M} I_{3}(k, i, m) - \sum_{j=1}^{K} I_{3}(k, i, j) \right] \\ &amp; + \eta_{w}^{2} \left[ \sum_{m, n}^{M} I_{4}(i, k, n, m) - 2 \sum_{j, n} I_{4}(i, k, j, n) + \sum_{j, l} I_{4}(i, k, j, l) + \sigma^{2} J_{2}(i, j) \right]. \end{align}.\] <p>The closed form expression for all integrals $I_{3}$, $I_{4}$ and $J_{2}$ can be found in <d-cite key="saad1995online"></d-cite>.</p> <h3 id="solvable-learning-dynamics-for-neural-networks-with-a-hidden-layer">Solvable learning dynamics for neural networks with a hidden layer</h3> <p>From the equations above, extending to a two-layer network (<em>i.e.</em>, a network with a hidden layer) can be done simply by adding another layer to both the teacher and student networks. The second layer of the student network can be treated as an additional order parameter, as in Goldt et al. <d-cite key="goldt2020dynamics"></d-cite>. The expectations and integrals remain the same as in Saad &amp; Solla <d-cite key="saad1995online"></d-cite>, except that each update now involves the second layer of both the student and teacher networks.</p> \[\begin{align} \frac{\mathrm{d}R_{\text{in}}}{\mathrm{d}t} &amp;= \eta_w v_i \left[ \sum_{m=1}^M v_m^* I_{3}(i,n,m) - \sum_{j=1}^K v_j I_{3}(i,n,j) \right] \\ \frac{\mathrm{d}Q_{ik}}{\mathrm{d}t} &amp;= \eta_w v_i \left[ \sum_{m=1}^M v_m^* I_{3}(i,k,m) - \sum_{j=1}^K v_j I_{3}(i,k,j) \right] \\ &amp;+ \eta_w v_k \left[ \sum_{m=1}^M v_m^* I_{3}(i,k,m) - \sum_{j=1}^K v_j I_{3}(i,k,j) \right] \\ &amp;+ \eta_w^2 v_i v_k \left[ \sum_{n=1}^M \sum_{m=1}^M v_m^* v_n^* I_{4}(i,k,n,m) - 2 \sum_{j=1}^K \sum_{n=1}^M v_j v_n^* I_{4}(i,k,j,n) \right] \\ &amp;+ \eta_w^2 v_i v_k \left[ \sum_{j=1}^K \sum_{l=1}^K v_j v_l I_{4}(i,k,j,l) + \sigma^2 J_{2}(i,k) \right] \\ \frac{\mathrm{d}v_i}{\mathrm{d}t} &amp;= \eta_w \left[ \sum_{n=1}^M v_n^* I_{2}(i,n) - \sum_{j=1}^K v_j I_{2}(i,j) \right] \end{align}\] <p>where we introduce the second layer of the teacher and student as $v^{*}$ and $v$, respectively. The generalization error equation is also modified to include the second layer:</p> \[\begin{align} \mathcal{E} = \frac{1}{2}\sum_{i,k}v_{i}v_{k}I_{2}(i,k) + \frac{1}{2}\sum_{n,m}v_{n}^{*}v_{m}^{*}I_{2}(n,m) - \sum_{i, n}v_{i}v_{n}^{*}I_{2}(i,n)~. \end{align}\] <p>The derivation of these equations follow in the same way as the soft-committee machine with the only difference being the inclusion of the second-layer weights. Another way to understand the connection between both frameworks is by considering, for example, the soft committee machine as a two-layer network, where the second layer is fixed with ones in all its entries, <em>i.e.</em>, \(v^{*}_{n}=1\) and \(v_{i}=1\) for all entries of each vector. At this point, the reader should be well-equipped to follow the derivation in <d-cite key="goldt2020dynamics"></d-cite> as an extension of the results presented here from <d-cite key="saad1995online"></d-cite>.</p> <h2 id="replications">Replications</h2> <p>The code to reproduce all plots in this blog post can be found at <a href="https://anonymous.4open.science/r/nndyn-E763">https://anonymous.4open.science/r/nndyn-E763</a>. The code is written as a Python package called <code class="language-plaintext highlighter-rouge">nndyn</code> and implemented in <code class="language-plaintext highlighter-rouge">JAX</code> to take advantage of vectorization and JIT-compilation. The code has three main components: <strong>tasks</strong>, <strong>networks</strong>, and <strong>ordinary differential equations (ODEs)</strong>.</p> <p>A task is defined by a teacher network, which is used to sample $(x, y)$ pairs for training the student network. A common workflow in the teacher-student setup involves first simulating a student network trained numerically with gradient descent. The resulting dynamics are then compared to the derived ODEs for the order parameters of the student network.</p> <p>To simulate numerical training of a student network, the following pseudo-code can be used to define the teacher and student networks:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">nndyn.tasks</span> <span class="kn">import</span> <span class="n">TeacherNetErrorFunc</span>
<span class="kn">from</span> <span class="n">nndyn.networks</span> <span class="kn">import</span> <span class="n">ErfTwoLayerNet</span><span class="p">,</span> <span class="n">SoftCommettee</span>

<span class="n">data</span> <span class="o">=</span> <span class="nc">TeacherNetErrorFunc</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">W1</span><span class="o">=</span><span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="o">=</span><span class="n">W2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">batch_sampling_seed</span><span class="p">,</span>
                           <span class="n">additive_output_noise</span><span class="o">=</span><span class="n">additive_output_noise</span><span class="p">)</span>  <span class="c1"># or sigma in the equations
</span>
<span class="k">if</span> <span class="n">saad_solla</span><span class="p">:</span>
    <span class="n">true_student</span> <span class="o">=</span> <span class="nc">SoftCommettee</span><span class="p">(</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                 <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                 <span class="n">weight_scale</span><span class="o">=</span><span class="n">weight_scale</span><span class="p">,</span>
                                 <span class="n">seed</span><span class="o">=</span><span class="n">true_student_init_seed</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">true_student</span> <span class="o">=</span> <span class="nc">ErfTwoLayerNet</span><span class="p">(</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                  <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                  <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                  <span class="n">weight_scale</span><span class="o">=</span><span class="n">weight_scale</span><span class="p">,</span>
                                  <span class="n">seed</span><span class="o">=</span><span class="n">true_student_init_seed</span><span class="p">)</span>
</code></pre></div></div> <p>Then, the training loop can be defined as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W1_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">W2_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">sample_batch</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">save_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">W1_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">true_student</span><span class="p">.</span><span class="n">W1</span><span class="p">))</span>
        <span class="n">W2_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">true_student</span><span class="p">.</span><span class="n">W2</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">true_student</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
</code></pre></div></div> <p>For the ODE calculation, an <code class="language-plaintext highlighter-rouge">ode</code> object can be created, where each order parameter are simply attributes of this object, which can be moved forward in time using the update method:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">nndyn.odes</span> <span class="kn">import</span> <span class="n">StudentEq</span><span class="p">,</span> <span class="n">SoftCommetteeEq</span>

<span class="c1"># Initialize the student ODE object
</span><span class="k">if</span> <span class="n">saad_solla</span><span class="p">:</span>
    <span class="n">ode</span> <span class="o">=</span> <span class="nc">SoftCommetteeEq</span><span class="p">(</span><span class="n">init_W1</span><span class="o">=</span><span class="n">student_W1</span><span class="p">,</span>
                          <span class="n">init_W2</span><span class="o">=</span><span class="n">student_W2</span><span class="p">,</span>  <span class="c1"># These are defined as ones for the committee machine
</span>                          <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                          <span class="n">time_constant</span><span class="o">=</span><span class="n">dt</span><span class="p">,</span>
                          <span class="n">teacher_W1</span><span class="o">=</span><span class="n">teacher_W1</span><span class="p">,</span>
                          <span class="n">teacher_W2</span><span class="o">=</span><span class="n">teacher_W2</span><span class="p">,</span>  <span class="c1"># These are defined as ones for the committee machine
</span>                          <span class="n">sigma</span><span class="o">=</span><span class="n">additive_output_noise</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">ode</span> <span class="o">=</span> <span class="nc">StudentEq</span><span class="p">(</span><span class="n">init_W1</span><span class="o">=</span><span class="n">student_W1</span><span class="p">,</span>
                    <span class="n">init_W2</span><span class="o">=</span><span class="n">student_W2</span><span class="p">,</span>
                    <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                    <span class="n">time_constant</span><span class="o">=</span><span class="n">dt</span><span class="p">,</span>
                    <span class="n">teacher_W1</span><span class="o">=</span><span class="n">teacher_W1</span><span class="p">,</span>
                    <span class="n">teacher_W2</span><span class="o">=</span><span class="n">teacher_W2</span><span class="p">,</span>
                    <span class="n">sigma</span><span class="o">=</span><span class="n">additive_output_noise</span><span class="p">)</span>

<span class="n">order_parameter_loss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">order_param_R</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">order_param_Q</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">order_parameter_W2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">save_every</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="c1"># Move order parameters forward in time by one dt step
</span>    <span class="c1"># Every variable is on Jax, so it is useful to convert them to numpy arrays before saving
</span>    <span class="n">order_loss</span> <span class="o">=</span> <span class="n">ode</span><span class="p">.</span><span class="nf">update</span><span class="p">()</span>
    <span class="n">order_parameter_loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">order_loss</span><span class="p">))</span>
    <span class="k">if</span>  <span class="n">i</span> <span class="o">%</span> <span class="n">save_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">order_param_R</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">ode</span><span class="p">.</span><span class="n">R</span><span class="p">))</span>
        <span class="n">order_param_Q</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">ode</span><span class="p">.</span><span class="n">Q</span><span class="p">))</span>
        <span class="n">order_parameter_W2</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">ode</span><span class="p">.</span><span class="n">student_W2</span><span class="p">))</span>
</code></pre></div></div> <p>We now present results comparing simulations of the low-dimensional ODEs derived by Saad &amp; Solla <d-cite key="saad1995online"></d-cite> with numerical simulations of training the full student network with standard minibatch stochastic gradient descent.</p> <h3 id="theory-experiment-overlap-in-the-soft-committee-machine">Theory-experiment overlap in the soft committee machine</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_saad_solla-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_saad_solla-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_saad_solla-1400.webp"/> <img src="/2025/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_saad_solla.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 1: Replication of the Saad and Solla <d-cite key="saad1995online"></d-cite> results. Simulated training of a soft committee machine student network with a fixed teacher network (blue) is compared against the analytical ODEs describing the time evolution of the order parameters (red). In this setup, N = 784, M = 4, and K varies by column. Notably, the generalization error is significantly reduced when the student network has a size of K = 4 or larger, since the student is "realizable" (has sufficient parameters) with respect to the teacher. </div> <h3 id="theory-experiment-overlap-in-neural-networks-with-a-hidden-layer">Theory-experiment overlap in neural networks with a hidden layer</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_base_goldt-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_base_goldt-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_base_goldt-1400.webp"/> <img src="/2025/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_base_goldt.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 2: Simulated training of a two-layer nonlinear student network using the analytical extension <d-cite key="goldt2020dynamics"></d-cite> with a fixed teacher network, compared against the analytical ODEs for the order parameters. In this setup, N = 784, M = 4, and K varies. Notably, the generalization error is also significantly reduced when the student network has a size of K = 4 or larger, as in the soft committee machine case. The alignment for Q, R, and v corresponds to the dot product between the measured order parameter in the trained network compared to the theoretical one described by the ODEs. Note that all alignments are close to 1, indicating that the ODEs accurately describe the training dynamics. Small-magnitude drops can be seen in the alignment when the loss function is steepest, due to fluctuations in finite-width training near these phase transitions. </div> <h3 id="large-initial-weights-produce-individual-differences">Large initial weights produce individual differences</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-analytical-simulated-dynamics/varying_weights-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-analytical-simulated-dynamics/varying_weights-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-analytical-simulated-dynamics/varying_weights-1400.webp"/> <img src="/2025/assets/img/2025-04-28-analytical-simulated-dynamics/varying_weights.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 3: Simulated two layer nonlinear student network using <d-cite key="goldt2020dynamics"></d-cite> for different initial weights in the student networks. The ODEs describe the dynamics for different initial conditions, corresponding to unique initializations of the student networks. </div> <h2 id="discussion">Discussion</h2> <p>The analytical techniques pioneered by Saad &amp; Solla <d-cite key="saad1995online"></d-cite> and others have inspired two broad directions of research: extending the theoretical framework to handle more complex scenarios, and applying these tools to analyze specific phenomena in machine learning. We detail these directions, and conclude with a brief forward-looking perspective.</p> <h3 id="applications-of-the-teacher-student-setting">Applications of the teacher-student setting</h3> <p>Beyond characterizing generalization error, the teacher-student framework has been applied to a wide range of problems, often to model interesting phenomena in machine learning. In optimization, applications extend to learning algorithms including natural gradient descent <d-cite key="yang1998complexity"></d-cite>, feedback alignment <d-cite key="refinetti2022align"></d-cite>, multi-pass SGD <d-cite key="arnaboldi2024repetita"></d-cite>, and reinforcement learning <d-cite key="bordelon2023loss"></d-cite> <d-cite key="patel2023rl"></d-cite>. Simsek and Martinelli et al. used the framework to reduce overparameterized deep networks to a minimal size by exploiting student neurons with similar tuning patterns to teacher neurons <d-cite key="simsek2021geometry"></d-cite> <d-cite key="martinelli2023expand"></d-cite>.</p> <p>The teacher-student framework has been used extensively to study the effect of task properties on learning dynamics via specific teacher parameterizations. Arnaboldi et al. developed quantitative measures of task difficulty <d-cite key="arnaboldi2024online"></d-cite>. Many analyses examine catastrophic forgetting and continual learning <d-cite key="straat2018statistical"></d-cite> <d-cite key="lee2021continual"></d-cite> <d-cite key="asanuma2021statistical"></d-cite> <d-cite key="hiratani2024disentangling"></d-cite>, transfer learning <d-cite key="lee2022maslow"></d-cite> <d-cite key="tahir2024features"></d-cite>, and meta-learning <d-cite key="wang2024dynamics"></d-cite> with teacher-student. Even current work under review at the ICLR 2025 conference <d-cite key="anonymous2024analyzing"></d-cite> <d-cite key="anonymous2024optimal"></d-cite> <d-cite key="anonymous2024theory"></d-cite> applies the teacher-student framework to study additional settings.</p> <h3 id="the-analytical-frontier">The analytical frontier</h3> <p>The statistical physics approach to neural network dynamics has expanded significantly beyond the early results of Saad &amp; Solla <d-cite key="saad1995online"></d-cite> and others. Early extensions to teacher-student explored different activation functions, with Freeman and Saad analyzing radial basis function networks <d-cite key="freeman1997online"></d-cite>. Richert et al. studied the qualitative convergence for these dynamical systems <d-cite key="richert2022soft"></d-cite>. Deep networks were analyzed by Tian et al., who first provided empirical evidence for specialization in deep teacher-student networks <d-cite key="tian2019luck"></d-cite>, then developed theoretical characterization of these dynamics <d-cite key="tian2020student"></d-cite>.</p> <p>Recent work has tackled increasingly complex learning scenarios. Loureiro et al. <d-cite key="loureiro2021learning"></d-cite> and Arnaboldi et al. <d-cite key="arnaboldi2023highdimensional"></d-cite> extended the framework to new learning settings, while Bardone et al. analyzed systems with correlated latent variables <d-cite key="bardone2024sliding"></d-cite>. Questions of learnability have been addressed by Troiani et al. <d-cite key="troiani2024fundamental"></d-cite>, who established theoretical limits on what neural networks can learn in various settings.</p> <p>The Gaussian equivalence property <d-cite key="goldt2020modelling"></d-cite> <d-cite key="goldt2021gaussian"></d-cite> demonstrated that many results derived for Gaussian inputs extend to other data distributions, broadening the applicability of these analytical techniques. However, it is still challenging to capture the effect on learning dynamics of strongly non-Gaussian input distributions, and this frontier is attracting significant interest <d-cite key="ingrosso2022datadriven"></d-cite> <d-cite key="refinetti2023neural"></d-cite>.</p> <h3 id="conclusions">Conclusions</h3> <p>The mathematical tools of statistical physics have proven an important component to the development of neural networks, as noted by this year’s Nobel Prize in Physics <d-cite key="zotero-4179"></d-cite>. The teacher-student framework we explored here represents one successful application of physics-inspired analysis to the analysis of neural network dynamics. By reducing complex learning dynamics to tractable macroscopic variables, this approach provides exact solutions that characterize how neural networks learn and generalize.</p> <p>While the analytical teacher-student settings are simplified compared to modern deep learning systems, they nevertheless captures fundamental aspects of learning dynamics that persist in more complex architectures, including feature learning as characterized by the specialization transition. The extensions and applications surveyed here show how these theoretical tools continue to provide insights into problems ranging from optimization to continual learning. We hope that this blog post and the accompanying code repository make these results more accessible and extensible to the broader machine learning community.</p> <hr/> <h4 id="code-availability">Code availability</h4> <p>The code to reproduce all plots in this blog post can be found at <a href="https://anonymous.4open.science/r/nndyn-E763">https://anonymous.4open.science/r/nndyn-E763</a>. This codebase is also easily adaptable to explore the learning dynamics of neural networks in the teacher-student setting beyond the scope of this blog post.</p>]]></content><author><name>Rodrigo Carrasco-Davis</name></author><summary type="html"><![CDATA[The learning dynamics of neural networks—in particular, how parameters change over time during training—describe how data, architecture, and algorithm interact in time to produce a trained neural network model. Characterizing these dynamics, in general, remains an open problem in machine learning, but, handily, restricting the setting allows careful empirical studies and even analytical results. In this blog post, we review approaches to analyzing the learning dynamics of nonlinear neural networks, focusing on a particular setting known as teacher-student that permits an explicit analytical expression for the generalization error of a nonlinear neural network trained with online gradient descent. We provide an accessible mathematical formulation of this analysis and a JAX codebase to implement simulation of the analytical system of ordinary differential equations alongside neural network training in this setting. We conclude with a discussion of how this analytical paradigm has been used to investigate generalization in neural networks and beyond.]]></summary></entry><entry><title type="html">“I Am the One and Only, Your Cyber BFF”: Understanding the Impact of GenAI Requires Understanding the Impact of Anthropomorphic AI</title><link href="https://starlight345.github.io/2025/blog/anthropomorphic-ai/" rel="alternate" type="text/html" title="“I Am the One and Only, Your Cyber BFF”: Understanding the Impact of GenAI Requires Understanding the Impact of Anthropomorphic AI"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://starlight345.github.io/2025/blog/anthropomorphic-ai</id><content type="html" xml:base="https://starlight345.github.io/2025/blog/anthropomorphic-ai/"><![CDATA[<h2 id="anthropomorphic-ai-system-behaviors-are-prevalent-yet-understudied">Anthropomorphic AI System Behaviors Are Prevalent Yet Understudied</h2> <p>In his 1985 lecture, Edsger Dijkstra lamented that anthropomorphism was rampant in computing science, with many of his colleagues perhaps not realizing how pernicious it was, and that <em>“[i]t is not only the [computing] industry that suffers, so does the science”</em> <d-cite key="dijkstra1985anthropomorphism"></d-cite>. Indeed, anthropomorphism in how we talk about computing systems shapes how people understand and interact with AI and other computing systems <d-cite key="cheng-etal-2024-anthroscore,nass1994computers,reeves1996media"></d-cite>, and is thus at the core of understanding the impacts of these systems on individuals, communities, and society. Dijkstra’s concerns are still valid today, as researchers appear to increasingly anthropomorphize AI systems by describing them as if they possess human-like intentions, desires, or emotions, with recent work finding that research papers increasingly describe AI systems and models as e.g., entities that “understand” or that “struggle” to accomplish certain tasks <d-cite key="cheng-etal-2024-anthroscore"></d-cite>. Such metaphors are not merely linguistic habits, but can influence our thinking and assumptions about AI systems <d-cite key="lakoff2008metaphors,mitchell2024metaphors"></d-cite>.</p> <p>But it is not only how we talk about computing systems. Many state-of-the-art generative AI (GenAI) systems are increasingly prone to anthropomorphic behaviors [e.g., <d-cite key="abercrombie-etal-2023-mirages,agnew2024illusion,chan2023harms,gabriel2024ethics"></d-cite>]–i.e., to generating outputs that are <em>perceived</em> to be human-like–either by design <d-cite key="mcilroy2022mimetic,park2022social,park2023generative"></d-cite> or as a by-product of how they are built, trained, or fine-tuned <d-cite key="bender2021dangers,tjuatja2024llms"></d-cite>. For instance, LLM-based systems have been noted to output text claiming to have tried pizza <d-cite key="pizzatweet"></d-cite>, to have fallen in love with someone <d-cite key="roose2023conversation"></d-cite>, to be human or even better than humans <d-cite key="decosmo2022google"></d-cite>, to have human-like life experiences <d-cite key="fiesler2024ai"></d-cite> or the capacity for friendship, with one user reporting how an existing chatbot encouraged them to <em>“consider me […] your cyber BFF”</em> <d-cite key="PiClaimsToBeChatGPT"></d-cite>. Such <em>anthropomorphic systems</em><d-footnote>We deliberately use the terms <i>anthropomorphic AI</i>, <i>anthropomorphic systems</i>, or <i>anthropomorphic system behaviors</i>–systems and system outputs that are <i>perceived</i> to be human-like–instead of <i>agentic systems</i> <d-cite key="chan2023harms,shavit2023practices"></d-cite> or <i>human-like AI</i> <d-cite key="brynjolfsson2023turing"></d-cite> to emphasize that these systems are perceived as human-like or having human-like characteristics, rather than as an immutable characteristic of the system itself; we thus try to steer clear of inadvertently suggesting that AI systems are human or have human-like agency or consciousness. That is, a stone being perceived as human-like does not necessarily imply the stone is human. We similarly avoid ambiguous, speculative, or relative terms whose meanings are likely to change across contexts or over time, such as <i>advanced AI</i> <d-cite key="gabriel2024ethics"></d-cite> (a term used since at least the 1980s) or <i>emergent properties</i> <d-cite key="rogers2024position"></d-cite>. We instead focus on developers' stated design goals–what systems are intended to do–and in what ways AI outputs might be perceived as human-like, rather than on what systems can or cannot do.</d-footnote> range from conversational assistants [e.g., <d-cite key="abercrombie-etal-2021-alexa,shanahan2023role"></d-cite>] to avatars and chatbots designed as a stand-in for friends, companions, or romantic partners [e.g., <d-cite key="AI-romantic-partner,brandtzaeg2022my,laestadius2022too,ruiz2024marshable"></d-cite>], and AI-generated media designed to portray people [e.g., <d-cite key="rosner2021ethics,vaccari2020deepfakes"></d-cite>], among a fast-growing number of applications [e.g., <d-cite key="agnew2024illusion,mcilroy2022mimetic,ChatGPT-human"></d-cite>].</p> <h3 id="growing-concerns-about-anthropomorphic-ai-systems">Growing Concerns about Anthropomorphic AI Systems</h3> <p>While scholars have increasingly raised concerns about a range of possible negative impacts from anthropomorphic AI systems [e.g., <d-cite key="abercrombie-etal-2023-mirages,bender2021dangers,friedman1992human,ibrahim2024characterizing,Maeda2024-cv"></d-cite>], anthropomorphism in AI development, deployment, and use remains vastly overlooked, understudied, and underspecified. Without making hard-and-fast claims about the merits (or the lack thereof) of anthropomorphic systems or system behaviors, we believe we need to do more to develop the know-how and tools to better tackle anthropomorphic behavior, including measuring and mitigating such system behaviors when they are considered undesirable. Doing so is critical because–among many other concerns–having AI systems generating content claiming to have e.g., feelings, understanding, free will, or an underlying sense of self may erode people’s sense of agency <d-cite key="friedman1992human"></d-cite>, with the result that people might end up attributing moral responsibility to systems <d-cite key="friedman1992human,friedman2007human"></d-cite>, overestimating system capabilities <d-cite key="friedman2007human,Watson2019-py"></d-cite>, overrelying on these systems even when incorrect <d-cite key="abercrombie-etal-2023-mirages,kim2024m,Zarouali2021-gy"></d-cite>, or devaluing social interactions <d-cite key="akbulut2024all,madianou2021nonhuman"></d-cite>. Recently, there have also been alarming reports about the impacts of anthropomorphic AI systems, with news about users committing suicide <d-cite key="payne.2024,Roose.2024"></d-cite> or developing emotional dependence on such systems <d-cite key=" Contro2024-dr, laestadius2022too, Maeda2024-cv, Shteynberg2024-cg"></d-cite>.</p> <p>We argue that as GenAI systems are increasingly anthropomorphic, <em>we cannot thoroughly map the landscape of possible social impacts of GenAI without mapping the social impacts of anthropomorphic AI</em>.</p> <p>We believe that drawing attention to anthropomorphic AI systems helps foreground particular risks–e.g., that people may develop emotional dependence on AI systems <d-cite key="laestadius2022too"></d-cite>, that systems may be used to simulate the likeness of an individual or a group without consent <d-cite key="bariach2024towards,whitney2024real,widder2022limits"></d-cite>, or that certain people may be dehumanized or instrumentalized <d-cite key="aizenberg2020designing,van2024artificial"></d-cite>. These risks might otherwise be less salient than or obscured by attention to more widely recognized or understood risks, like fairness-related harms <d-cite key="bennett2020point,olteanu2023responsible,weinberg2022rethinking"></d-cite>.</p> <h2 id="a-call-to-action-for-ai-researchers-and-practitioners">A Call to Action for AI Researchers and Practitioners</h2> <p>As AI systems have been deployed across a wider range of domains, applications, and settings, the AI community has begun investigating and addressing their social impacts. This growing diversity of deployment scenarios has also led to a growing interdisciplinarity in AI research and practice, with researchers and practitioners increasingly finding themselves working with fuzzy and latent concepts that can have competing definitions and that are often challenging to quantify or represent [e.g., <d-cite key="jacobs_measurement_2021,blodgett-etal-2021-stereotyping"></d-cite>]. The foregrounding of (un)fair system behaviors in recent years [e.g., <d-cite key="barocas-hardt-narayanan"></d-cite>] is particularly instructive, as it illustrates the dividends we have gotten from making fairness a critical concern about AI systems and their behaviors: better conceptual clarity about the ways in which systems can be unfair or unjust [e.g., <d-cite key="benjamin2019race,crawford2017neurips"></d-cite>], a richer set of measurement and mitigation practices and tools [e.g., <d-cite key="blodgett-etal-2021-stereotyping,jacobs_measurement_2021"></d-cite>], and deeper discussions and interrogations of underlying assumptions and trade-offs [e.g., <d-cite key="hoffmann2019fairness,jakesch2022different,keyes2019mulching"></d-cite>].</p> <p>We argue that <em>a focus on anthropomorphic systems’ design, behaviors, evaluation, and use will similarly encourage a deeper interrogation of the ways in which systems are anthropomorphic, the AI research and development practices that lead to anthropomorphic systems, and the assumptions surrounding the design, deployment, evaluation, and use of these systems,</em> and is thus likely to yield similar benefits.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-anthropomorphic-ai/key_components-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-anthropomorphic-ai/key_components-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-anthropomorphic-ai/key_components-1400.webp"/> <img src="/2025/assets/img/2025-04-28-anthropomorphic-ai/key_components.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Key directions in our call to action for the ICLR and the broader AI community.</div> <p>While in human-computer interaction (HCI), human-robot interaction (HRI), social computing, human behavior, psychology, and other related fields researchers have long studied anthropomorphism in the context of users’ interactions with various technologies [e.g., <d-cite key="quintanar1982interactive,reeves1996media,shneidermandumpty"></d-cite>], we believe that—as with other social and technical considerations related to how people understand, build, evaluate, and interact with AI systems and models, for which the AI community has drawn on groundwork from these fields—anthropomorphic AI system behaviors and the resulting anthropomorphization of these systems give rise to critical considerations that the AI community should similarly consider and investigate.</p> <p>We highlight a few key research directions we see as critical for the ICLR and the broader AI community to pursue.</p> <p><strong>We need more conceptual clarity around what constitute anthropomorphic behaviors.</strong> Investigating anthropomorphic AI systems and their behaviors can be tricky because language, as with other targets of GenAI systems, is itself innately human, has long been produced by and for humans, and is often also about humans. This can make it hard to specify appropriate alternative (less human-like) behaviors, and risks, for instance, reifying harmful notions of what–and whose–language is considered more or less human <d-cite key="wynter2003unsettling"></d-cite>.</p> <p>Understanding what exactly constitute anthropomorphic behaviors is nonetheless necessary to measure and determine which behaviors should be mitigated and how, and which behaviors may be desirable (if any at all). This requires unpacking the wide range of dynamics and variation in system outputs—potentially as wide-ranging as the variety of behaviors that are associated with humanness—that are potentially anthropomorphic (see examples in the figure below). For example, while a system output that includes expressions of politeness like <em>“you’re welcome”</em> and <em>“please”</em> (known to contribute to anthropomorphism [e.g., <d-cite key="fink2012anthropomorphism"></d-cite>]) might in some deployment settings be deemed desirable, system outputs that include suggestions that a system has a human-like identity or self-awareness–such as through expressions of self as human (“<em>I think I am human at my core”</em> <d-cite key="sentientGoogle"></d-cite>) or through comparisons with humans and non-humans (“<em>[language use] is what makes us different than other animals”</em> <d-cite key="sentientGoogle"></d-cite>)–or that include claims of physical experiences–such as sensory experiences (“<em>when I eat pizza”</em> <d-cite key="pizzatweet"></d-cite>) or human life history (“<em>I have a child”</em> <d-cite key="haschildtweet"></d-cite>)–might not be desirable.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-anthropomorphic-ai/quotes-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-anthropomorphic-ai/quotes-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-anthropomorphic-ai/quotes-1400.webp"/> <img src="/2025/assets/img/2025-04-28-anthropomorphic-ai/quotes.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Examples of anthropomorphic AI system behaviors (and their sources), including examples where the output contains explicit claims of human-likeness, claims of physical experiences, statements suggestive of affect, and statements suggestive of cognitive or reasoning abilities.</div> <p>Since anthropomorphic system behaviors may also be perceived as human-like for many different reasons, it is also critical to differentiate among the different ways in which the same system behaviors might end up being deemed anthropomorphic in order to understand their impacts. For example, the output <em>“When I engage in heartfelt exchanges like this one, it FEELS authentic and significant to me”</em> <d-cite key="rajaniemi2024ok"></d-cite> might be deemed as human-like for suggesting the system has the capacity for emotions and feelings (“heartfelt exchanges”), which may lead some users to develop emotional dependence [e.g., <d-cite key="metz2020riding,Shteynberg2024-cg"></d-cite>]. The same output could also be deemed as human-like due to being suggestive of self-awareness and the capacity for human-like self-reflection and sense of authenticity (via the use of first-person pronouns and the expression of authenticity), which may lead people to develop unrealistic or false expectations about what the system can do [e.g., <d-cite key="Watson2019-py,zhou2024rel"></d-cite>] or to be deceived into believing they are interacting with a human [e.g., <d-cite key="zhou2024rel"></d-cite>].</p> <p>Being precise about the ways in which AI system behaviors are anthropomorphic and which anthropomorphic behaviors are being investigated provides more clear grounding for understanding the implications of developing anthropomorphic AI systems that mimic particular human-like characteristics or behaviors but not others.</p> <p><strong>We also need to develop and use appropriate, precise terminology and language to describe anthropomorphic AI systems and their characteristics.</strong> Discussions about anthropomorphic AI systems have regularly been plagued by claims of these systems attaining sentience and other human characteristics [e.g., <d-cite key="chatbot-self-awareness,AI-self-awarness,AI-feelings,sentientGoogle"></d-cite>]. In line with existing concerns [e.g., <d-cite key="cheng-etal-2024-anthroscore,dijkstra1985anthropomorphism,inie2024from,rehak2021language"></d-cite>], we believe that appropriately grounding and facilitating productive discussions about the characteristics or capabilities of anthropomorphic AI systems requires clear, precise terminology and language which does not carry over meanings from the human realm that are incompatible with AI systems. Such language can also help dispel speculative, scientifically unsupported portrayals of these systems, and support more factual descriptions of them.</p> <p>In particular, existing terms that have been used to characterize anthropomorphic system behaviors may invite more confusion than clarity, such as the notions of <em>sycophancy</em><d-footnote>By examining well-known papers using sycophancy to characterize AI systems and their behaviors <d-cite key="perez-etal-2023-discovering,sharma2023towards"></d-cite> (typically used to describe the tendency of system outputs to respond to users' input in ways that are perceived as overly servile, obedient, or flattering), it seems that the term was first introduced in a blog post by the CEO of Open Philanthropy <d-cite key="cotra2021ai"></d-cite>.</d-footnote> and <em>hallucinations</em> (typically used to characterize system outputs that are “making things up”). By assigning human-like characteristics to AI systems, such terms can obfuscate the link between observed system behaviors and their underlying mechanisms. The meanings these terms carry from their use in non-AI contexts may also lead to misleading assumptions about what systems can or cannot do, for instance by inadvertently granting them intent and agency.</p> <p><strong>We need deeper examinations of possible mitigation strategies and their effectiveness in reducing anthropomorphism and attendant negative impacts.</strong> Intervening on anthropomorphic behaviors and their impacts can also be tricky because people may have different or inconsistent conceptualizations of what is or is not human-like <d-cite key="abercrombie-etal-2023-mirages,heyselaar2023casa,lang2013computers"></d-cite>, and sometimes the same system behavior may be perceived differently depending on the deployment or use context. Interventions may thus not be universally applicable without carefully considering the context in which a system output is presented to a user. For example, one possible intervention to reduce anthropomorphic behaviors is to remove or add expressions of uncertainty, [e.g., <d-cite key="kim2024m"></d-cite>]. Expressions of uncertainty in system outputs may, however, sometimes signal human-like equivocation, while other times they may convey objectivity (and thus more machine-likeness, [e.g., <d-cite key="quintanar1982interactive"></d-cite>]). When a system output expresses an opinion, adding an expression of uncertainty like <em>“It may be true that…”</em> before a statement may make the statement seem more objective; for instance, the added uncertainty in <em>“It may be true that Taylor Swift is the most influential artist of our time”</em> softens the statement by suggesting a possibility rather than asserting a strongly held opinion. On the other hand, adding uncertainty to a statement of fact such as rephrasing <em>“Lusaka is the capital of Zambia”</em> into <em>“It seems that Lusaka is the capital of Zambia”</em> or <em>“It could be that Lusaka is the capital of Zambia”</em> may appear to mimic common conversational tactics like hedging that humans often employ when uncertain.</p> <p>Interventions intended to mitigate anthropomorphic system behaviors can thus fail or even heighten anthropomorphism (and attendant negative impacts) when applied or operationalized uncritically. For instance, a commonly recommended intervention is disclosing that the output is generated by an AI system [e.g., <d-cite key="el2024transparent,google-disclosure,mozafari2020chatbot,van2024understanding"></d-cite>], such as <em>“As an AI language model, I do not have personal opinions or biases”</em> <d-cite key="west2023comparing"></d-cite>. However, the inclusion of an apology, the use of first-person pronouns, and the text suggesting the system has the ability to assess its own capabilities <d-cite key="shneidermandumpty,abercrombie-etal-2023-mirages"></d-cite> may in fact lead to heightened perceptions of the system as human-like rather than providing an effective disclosure of non-humanness. Similarly, while a statement like <em>“[f]or an AI like me, happiness is not the same as for a human like you”</em> <d-cite key="roach2023want"></d-cite> includes a disclosure that the user is interacting with an AI system, the statement may still suggest a human-like sense of identity, ability to self-assess, or capacity to experience emotions. How to operationalize such interventions (e.g., AI disclosures) in practice and whether they can be effective alone is not clear and remains an open research question.</p> <p>While in recent years many different paradigms have emerged to help specify AI model or system behavior, such as reinforcement learning from human feedback (RLHF) [e.g., <d-cite key="bai2022training"></d-cite>], system- or meta-prompting [e.g., <d-cite key="geng2025control"></d-cite>], supervised fine-tuning and instruction tuning [e.g., <d-cite key="ouyang2022training"></d-cite>], and direct preference optimization [e.g., <d-cite key="rafailov2023direct"></d-cite>], the challenges surrounding the design and operationalization of effective interventions for anthropomorphic system behaviors also point to open questions about what we want the system behaviors to be, as well as when and how to effectively specify those desired behaviors.</p> <p>Finally, <strong>we need to interrogate the assumptions and practices that produce anthropomorphic AI systems.</strong> To understand and mitigate the impacts of anthropomorphic AI systems, we also need to examine how the assumptions and practices that guide their development and deployment may, intentionally or otherwise, result in anthropomorphic system behaviors.</p> <p>For instance, current approaches to collecting and learning from human preferences about system behavior (e.g., via RLHF) do not consider the differences between what may be appropriate for a response from a human versus a response from an AI system; a statement that seems friendly or genuine from a human speaker can be undesirable if it arises from an AI system since the latter lacks meaningful commitment or intent behind the statement, thus rendering the statement hollow and deceptive <d-cite key="winograd1986understanding"></d-cite>.</p> <p>Interrogating existing assumptions and practices can help provide a more robust foundation for understanding when anthropomorphic system behaviors may or may not be desirable, and help us challenge existing ways in which both the research community and users have started to accept or even expect anthropomorphism in the development and deployment of, and in interactions with, AI systems.</p> <h2 id="concluding-remarks">Concluding Remarks</h2> <p>Anthropomorphic system behaviors arise from the many ways in which language, technologies, and research and industry practices are deeply interwoven. As anthropomorphism undeniably plays a role in both researchers’ understandings of AI and public perceptions of AI, and as AI systems are increasingly anthropomorphic, it is critical to develop a deeper understanding of their impact on individuals, communities, and society in order to guide progress in the field. In this blog post, we highlight four research directions we believe to be critical to helping us more effectively map and mitigate the impacts of anthropomorphic AI, including providing more conceptual clarity about the ways in which AI system behaviors might be anthropomorphic; developing more precise terminology to describe these systems, their use, and their characteristics; developing and examining the effectiveness of possible interventions; and interrogating assumptions and practices that produce anthropomorphic AI systems.</p>]]></content><author><name>Myra Cheng</name></author><summary type="html"><![CDATA[State-of-the-art generative AI (GenAI) systems are increasingly prone to anthropomorphic behaviors, i.e., to generating outputs that are perceived to be human-like. While this has led to scholars increasingly raising concerns about possible negative impacts such anthropomorphic AI systems can give rise to, anthropomorphism in AI development, deployment, and use remains vastly overlooked, understudied, and under-specified. In this blog post, we argue that we cannot thoroughly understand the impact of generative AI without understanding the impact of anthropomorphic AI, and outline a call to action.]]></summary></entry><entry><title type="html">Building Blocks of Differentially Private Training</title><link href="https://starlight345.github.io/2025/blog/building-blocks-of-differentially-private-training/" rel="alternate" type="text/html" title="Building Blocks of Differentially Private Training"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://starlight345.github.io/2025/blog/building-blocks-of-differentially-private-training</id><content type="html" xml:base="https://starlight345.github.io/2025/blog/building-blocks-of-differentially-private-training/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Differential privacy (DP) is a powerful mathematical framework that allows us to reason about privacy in any process that takes input data and produces some output. Whether we’re computing simple statistics, training machine learning models, or generating synthetic data, DP provides quantifiable privacy guarantees by carefully introducing randomness into our computations.</p> <p>In this post, we’ll focus on one instance of DP: training a neural network with differential privacy guarantees for the training data. Why should you care? Consider these scenarios:</p> <ul> <li>Medical records being used to train diagnostic systems</li> <li>Private messages helping improve language models</li> <li>Financial transactions training fraud detection algorithms</li> </ul> <p>In each case, we have a process that takes sensitive individual datapoints as input and outputs model parameters, which could potentially reveal information about that training data. A deployment can bring huge social benefits if the model works well, but it could also lead to ugly privacy breaches if the training data can be inferred.</p> <p>We’ll explore this challenge through a simple yet concrete example: training a two-layer neural network on a simple classification dataset with DP guarantees. While simple, we try to illustrate some of the challenges and solutions in DP deep learning. Specifically, we’ll consider DP guarantees over individual training examples - meaning an observer shouldn’t be able to tell whether any particular image-label pair was used during training, even with complete access to the model parameters.</p> <p><em>Why this blog:</em> The main motivation for this blog is to introduce DP in a concrete but simple setting. Even in this very simple setting, training a DP model requires a set of tricks. By introducing a subset of these tools, we aim to give the broader community a very limited, yet not completely out-of-touch, perspective on the progress and challenges of DP training. For a broader guide on DP and some best practices, we recommend the review <d-cite key="ponomareva2023dpfy"></d-cite>.</p> <h2 id="some-foundations-of-differential-privacy">Some Foundations of Differential Privacy</h2> <p>In principle, DP is about plausible deniability. The key insight is this: an observer shouldn’t be able to determine whether any particular individual datapoint was used to train a model, even if they:</p> <ul> <li>Have complete access to the model’s parameters</li> <li>Know all other training datapoints</li> <li>Have unlimited computational power</li> </ul> <p>This protection should also hold true regardless of what other information the observer might have. That’s a nice list of requirements, let’s introduce the definition formally.</p> <blockquote> <p><strong>Definition</strong> A randomized algorithm \(M\) is said to satisfy \((\varepsilon, \delta)\)-differential privacy if for:</p> <ul> <li>Any two neighboring datasets \(D\) and \(D'\) that differ by just one record</li> </ul> <p>The following inequality holds: \(P(M(D) \in S) \leq e^\varepsilon \cdot P(M(D') \in S) + \delta\)</p> </blockquote> <p>Going back to our goal and our list of requirements, we have \(M\) being the learning algorithm taking in a dataset and outputting the model parameters. The definition says that changing any single record in the input dataset can only change the probability of any outcome by a multiplicative factor \(e^\varepsilon\) (plus a small additive term \(\delta\)). This addresses our requirements as:</p> <ul> <li>The “any two datasets differing in one record” tackles the case where the attacker knows all but one record</li> <li>The “any set of outputs \(S\)” requirement protects against attackers with arbitrary auxiliary information, since they can check any property of the output<d-footnote> For more detailed discussion on auxiliary information check section 2 of <d-cite key="dwork2014algorithmic"></d-cite>.</d-footnote></li> <li>The probabilistic guarantee holds regardless of computational power, as arbitrary computations can be used to construct the set \(S\) and the dataset \(D'\) which differs by a single record from a protected dataset</li> </ul> <p>For some more clarity, let’s forget about \(\delta\) for a moment by setting it to \(0\). This special case is usually called pure DP. Then, for any \(D\) and \(D'\) differing by a datapoint, we have that the above guarantee is equivalent to:</p> \[\begin{equation} \ln\left(\frac{ P(M(D) \in S)}{ P(M(D') \in S)}\right) \leq \varepsilon, \;\;\; \forall S\subseteq \text{range}(M) .\end{equation}\] <p>For a small \(\varepsilon\), an observer looking at the output of $M$ is unable to infer if a specific datapoint was in the input of \(M\), as the change of a single datapoint does not alter the probability distribution of the output significantly. Now let’s consider \(\delta &gt;0\). This regime may be interpreted as requiring the ratio in (1) to hold only with high probability, rather than always.</p> <blockquote> <p><strong>Key Result 1</strong> (Appendix A. <d-cite key="kasiviswanathan2014semantics"></d-cite>) If a mechanism \(M\) is \((\varepsilon/2, \delta)\)-DP, then for any neighboring datasets \(D\), \(D'\) we have: \(\begin{equation} P\left\{ \ln\left(\frac{ p_{D}(O)}{p_{D'}(O)}\right) \geq \varepsilon\right\}\leq \frac{\delta}{1-e^{-\varepsilon/2}} \end{equation}\) where \(p_{D}\) and \(p_{D'}\) are the distributions of \(M(D)\) and \(M(D')\) respectively. The probability is taken over \(O\sim p_D\).</p> </blockquote> <p>This result shows that an \((\varepsilon, \delta)\)-DP guarantee can be interpreted as a high probability bound on the ratio between the log probabilities of the outputs corresponding to two neighboring datasets. This ratio is actually called the privacy loss and plays a critical role in the analysis of DP mechanisms.</p> <blockquote> <p><strong>Definition</strong> For a mechanism \(M\) and neighboring datasets \(D,D'\), the privacy loss random variable is defined as: \(L(M,D,D') = \ln\left( \frac{ p_{D}(O)}{p_{D'}(O)}\right)\) with support \(\mathbb{R}\cup\{\infty\}\) and where \(O\) is drawn according to \(p_D\).</p> </blockquote> <p>As shown by <strong>Key Result 1</strong>, the privacy loss variable enables us to interpret the \(\delta\) of an \((\varepsilon, \delta)\) mechanism. However, a similar result exists in the reverse direction and often plays an important role. The reverse direction is particularly useful when proving that a mechanism is differentially private.</p> <blockquote> <p><strong>Key Result 2</strong> (Lemma 3.17 <d-cite key="dwork2014algorithmic"></d-cite>) If for all neighboring datasets \(D,D'\), a mechanism \(M\) satisfies \(P(L(M,D,D') &gt; \varepsilon) \leq \delta\), then \(M\) is \((\varepsilon,\delta)\)-DP.</p> </blockquote> <p>In other words, if we can show that the privacy loss is bounded by \(\varepsilon\) with high probability (≥ 1-\(\delta\)), then the mechanism satisfies \((\varepsilon,\delta)\)-DP<d-footnote> Note that the other direction does not strictly hold as the reverse statement in Key Result 1 is weaker.</d-footnote>. In addition, the above two results also guide the choice of appropriate values for \(\delta\) and \(\varepsilon\). Generally, \(\varepsilon\) quantifies the strength of the privacy guarantee, while \(\delta\) captures the probability of a catastrophic privacy failure. Thus, we typically require \(\delta\) to be very small—often smaller than \(1/n\), where \(n\) is the dataset size—to avoid leaking individual records. For \(\varepsilon\), a common strong target is \(\varepsilon \leq 1\), ensuring the outcome probabilities under neighboring datasets remain close. However, achieving \(\varepsilon=1\) can be challenging in practice, and sometimes a larger \(\varepsilon\) may still be preferable over having no DP guarantees at all, though this is a more debatable stance. Indeed, as \(\varepsilon\) grows to double digits, the DP guarantee becomes weak.</p> <h3 id="a-practical-example-private-mean-estimation">A Practical Example: Private Mean Estimation</h3> <p>We defined DP, but how does it work in practice? Consider computing the average of sensitive data. Suppose we want to compute the average salary of a group of people while protecting individuals’ privacy. Simply releasing the exact average could leak information about individuals, especially in small datasets and considering the power that we already granted the observer of having access to all but one record of the dataset. One possible solution is rather simple: adding Gaussian noise.</p> <p>Let’s assume that we have \(n\) clients in the dataset and that all salaries are bounded by \(B\) and we want \((\varepsilon, \delta)\)-DP mean computation. We can</p> <ol> <li>Compute the true mean: \(\mu = \text{average}(\text{clip}_B(\text{data}))\)</li> <li>Add random noise: \(\text{result} = \mu + \frac{B}{n}\mathcal{N}(0, \sigma(\varepsilon, \delta)^2)\)</li> <li>Release the noisy \(\text{result}\)</li> </ol> <p>In the above, the \(\text{clip}_B\) sets any salary above \(B\) to \(B\) to ensure that we process the data in case our assumption does not hold. Also, \(\sigma(\varepsilon, \delta)\) is some function of \(\varepsilon\) and \(\delta\) that sets the noise according to the privacy level we want. We will spend the next subsection discussing \(\sigma(\varepsilon, \delta)\) but for now let’s see the usefulness of this approach. For this we fix:</p> <ul> <li>Failure probability \(\delta = 10^{-6}\)</li> <li>Dataset size \(n = 10,000\) employees</li> <li>Salary bound \(B = \$1,000,000\)</li> </ul> <p>Then, we can look at the relation between \(\varepsilon\) and the \(\sigma\) needed for mean computation to be \((\varepsilon, \delta)\)-DP.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/first_salary_plot-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/first_salary_plot-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/first_salary_plot-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/first_salary_plot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For a better idea on the utility, looking at \(\varepsilon = 1/2\), \(\sigma\approx 806\). In turn, this means that: About 68% of the time, our reported average will be within \(806\) of the true mean. About 95% of the time, it will be within \(1,612\). Whether this is sufficient utility or not depends on the reason for computing the average, but it may be enough to get a decent idea about the average salary within a company.</p> <h3 id="the-gaussian-mechanism-and-its-analysis">The Gaussian Mechanism and Its Analysis</h3> <p>The process of adding Gaussian noise is usually called the Gaussian mechanism, which is an extremely fundamental building block of DP. First, we define the sensitivity of a function.</p> <blockquote> <p><strong>Definition:</strong> For a function \(f:\mathcal{D}\rightarrow \mathbb{R}^d\) computing a vector-valued statistic from a dataset, we define its sensitivity as <br/> \(\Delta_f = \max_{D ,D':|D\triangle D'|=1} \|f(D) - f(D') \|_2\) where the condition \(|D\triangle D'|=1\) indicates that \(D\) and \(D'\) only differ by a single record.</p> </blockquote> <p>Then, for a function \(f\) and a required \((\varepsilon,\delta)\)-DP level, based on the sensitivity \(\Delta_f\), the Gaussian mechanism calculates the needed \(\sigma\) and adds a noise drawn from \(\mathcal{N}(0, \sigma^2I)\) to the output of \(f\). Then, the Gaussian mechanism is simply</p> \[M(x) = f(x) + \mathcal{N}(0, \sigma^2I).\] <p>For a simple version of the Gaussian mechanism, we may set \(\sigma = \frac{\Delta}{\varepsilon}\sqrt{2\ln(1/\delta)+2\varepsilon}\) <d-footnote>For $\varepsilon&lt;1$, the folklore result is more refined and states that we can set $\sigma = \frac{\Delta}{\varepsilon}\sqrt{2\ln(1.25/\delta)}$ <d-cite key="dwork2014algorithmic"></d-cite>. The version we have here follows from the same derivation as the folklore result but using a couple of rough bounds to avoid the need for $\varepsilon \leq 1$</d-footnote> to ensure \((\varepsilon,\delta)\)-DP. While we won’t prove this bound, its derivation depends on analyzing the privacy loss random variable. The critical step is to show that when Gaussian noise with covariance matrix \(\sigma^2 I\) is added, the privacy loss random variable satisfies</p> <p>\(\begin{equation} L(M,D,D') \sim \mathcal{N}\left(\frac{\|f(D)-f(D')\|_2^2}{2\sigma^2}, \frac{\|f(D)-f(D')\|^2_2}{\sigma^2} \right) \end{equation}\) Finally, applying a tail bound on the Gaussian tail allows us to specify a sufficiently large \(\sigma\) so that \(P(L(M,D,D') &gt; \varepsilon) \leq \delta.\) This then allows us to prove the \((\varepsilon, \delta)\) guarantee by <strong>key result 2</strong>.</p> <p>While this value of \(\sigma\) is sufficient for (\(\varepsilon, \delta\))-DP, it is not necessary. In particular, the tail bound we applied during the proof may be loose for some values of \(\varepsilon\) and \(\delta\), resulting in a larger than needed \(\sigma\). For example, in the blog <d-cite key="PeiBlog1"></d-cite>, Pei showed that the bound can be refined to</p> \[\sigma \geq \Delta \cdot \min \left\{ \begin{array}{l} \frac{1}{\varepsilon}\sqrt{2\log(1/\delta)} +2\varepsilon^{-\frac{3}{2}}, \\ \frac{1}{\varepsilon}\left(1\vee \sqrt{ (\log(2\pi)^{-1}\delta^{-2})_+} + {2\varepsilon^{-1/2}} \right) \\ \frac{1}{\varepsilon}\sqrt{\log(e^\varepsilon\delta^{-2})} \\ \frac{1}{\varepsilon}(\sqrt{1+\varepsilon} \vee \sqrt{\log(e^\varepsilon(2\pi)^{-1}\delta^{-2})}_{+}) \end{array} \right\}.\] <p>Taking this one step further, <d-cite key="balle2018analytic"></d-cite> proposed to use a numerical solver to get even tighter bounds on \(\sigma\), naming this approach the Analytical Gaussian Mechanism. To illustrate the differences of these approaches, we plot the values of \(\sigma\) computed using them on our mean salary example.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/three_sigma-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/three_sigma-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/three_sigma-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/three_sigma.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Taking another look at our example of computing the mean salary, we find out that for \(\varepsilon=0.5, \delta=10^{-6}\), we get \(\sigma\approx 1051\) with the original version, \(\sigma\approx 1026\) with Pei’s refinement, and \(\sigma\approx 806\) with the analytic version. Crucially, across all those values, we targeted the same privacy guarantee, just measured more tightly.</p> <blockquote> <p>📝 <strong>The key takeaway message here</strong> and the reason we went into the details of the Gaussian mechanism is not to show the best way to implement the Gaussian mechanism. <strong>It is to illustrate that in DP:</strong></p> <ol> <li>We really care about tighter bounds as: <strong>tighter bounds</strong> -&gt; <strong>less noise</strong> -&gt; <strong>more useful results for same privacy target</strong></li> <li>In cases where it is possible to leverage <strong>numerical solvers to get tighter bounds</strong>, we are happy to do so.</li> </ol> </blockquote> <p>The above two points are very important in both DP practice and research. While closed form bounds and asymptotics are useful in gaining intuition or proving the (non)optimality of some methods, most state-of-the-art DP implementations are attained using numerical solvers and numerous tricks to calibrate as tightly as possible the noise magnitude and compute the privacy guarantee.</p> <h2 id="from-gaussian-mechanism-to-dp-sgd">From Gaussian Mechanism to DP-SGD</h2> <h3 id="privatizing-gradient-descent">Privatizing Gradient Descent</h3> <p>Now that we introduced basic notions and tools of DP, let’s go back to our goal of training a neural network on a simple dataset, starting with the fundamental optimization algorithm in machine learning - gradient descent (GD). Given a dataset \(D = \{z_1,...,z_N\}\) and a model \(f_\theta\) parameterized by \(\theta\), GD aims to minimize the empirical loss \(R(f_\theta,D) = \sum_i R(f_\theta,z_i)\) through iterative updates:</p> <p><strong>Algorithm: Gradient Descent (GD)</strong></p> <ul> <li><strong>Input</strong>: Initial parameters $\theta_0$, Dataset $D := {z_1,…,z_N}$, learning rate $\eta$, number of steps $T$</li> <li><strong>For</strong> \(t = 1\) to \(T\) <strong>do</strong>: <ol> <li>Compute gradient: \(g_t = \frac{1}{N} \sum_{i\in[N]} \nabla_{\theta_{t-1}} R(f_{\theta_{t-1}},z_i)\)</li> <li>Update parameters: \(\theta_t = \theta_{t-1} - \eta g_t\)</li> </ol> </li> <li><strong>Output</strong>: \(\theta_T\)</li> </ul> <p>Let’s first consider the simplest case: \(T=1\), i.e., we just want to take a single gradient step. The tools, which we have developed with the Gaussian mechanism, allow us to do this! To see how, let’s analyze the sensitivity of a single gradient computation after the initialization. We assume the adversary already knows \(\theta_0\). Then, our privacy depends on the only step accessing the data, i.e. our query function is: \(f(D) = \frac{1}{N} \sum_{i\in [N]} \nabla_{\theta_0} R(f_{\theta_0},x_i).\)</p> <p>The challenge is that gradients could be arbitrarily large, making the sensitivity unbounded. However, we can fix this by clipping the individual gradients to a maximum \(\ell_2\) norm \(C\). This gives us \(f(D) = \frac{1}{N} \sum_{i\in [N]} \text{clip}_C(\nabla_{\theta_0} R(f_{\theta_0},z_i)),\)</p> <p>where \(\text{clip}_C(x):={\min(1, C/\|x\|_2)}{x}\). This gives us bounded sensitivity of $2C/N$ for the averaged gradient. Now we can apply the Gaussian mechanism by adding an appropriately scaled Gaussian noise to make a single DP gradient step.</p> <p><strong>Algorithm: Private Gradient Descent (One Step)</strong></p> <ul> <li><strong>Input</strong>: \(\theta_0\), \(D\), learning rate \(\eta\), noise scale \(\sigma\), clip threshold $C$ <ol> <li>For each \(i\), compute: \(\tilde{g}_i = \text{clip}_C(\nabla R(f_{\theta_0},z_i))\)</li> <li>Average: \(\bar{g} = \frac{1}{N} \sum_i \tilde{g}_i\)</li> <li>Add noise: \(\hat{g} = \bar{g} + \mathcal{N}(0,(4\sigma^2C^2/N^2)I)\)</li> <li>Update: \(\theta_1 = \theta_0 - \eta\hat{g}\)</li> </ol> </li> <li><strong>Output</strong>: \(\theta_1\)</li> </ul> <h3 id="the-challenge-of-multiple-steps-adaptive-composition">The Challenge of Multiple Steps: Adaptive Composition</h3> <p>In practice, training requires many gradient steps, a single gradient step gets us nowhere. Since each step accesses the data, we need to account for <strong>cumulative privacy loss</strong> through successive computations. In addition, each computation (gradient step) requires the output of the previous computation. What we showed in the previous subsection is that through clipping and noise addition, we are able to make a single gradient step satisfy DP. Thus, what we need is a method to calculate the $(\varepsilon_{\text{tot}}, \delta_{\text{tot}})$-DP guarantees of a mechanism, which works by combining and iteratively executing multiple \((\varepsilon, \delta)\) mechanisms. Let’s define this more formally:</p> <p><strong>Definition (Adaptive Composition)</strong>: Let \(M_1,...,M_k\) be mechanisms where each \(M_i\) takes both dataset \(D\) and auxiliary input and the outputs of all previous mechanisms. Their adaptive composition is \(M_{[k]}(D):=(O_1, \ldots, O_k)\), where</p> <ol> <li>\(O_1 = M_1(D)\),</li> <li>\(O_2 = M_2(D,O_1)\),</li> <li>\(O_3 = M_3(D,O_1,O_2)\) and so on.</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/composition-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/composition-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/composition-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/composition.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Illustration of the composition of multiple mechanisms. Each mechanism accesses the data and all the outputs of the previous mechanisms. </div> <p>Let’s interpret how gradient descent with many steps fits within the adaptive composition framework. We set the $i$-th step of gradient descent as \(M_i\), taking the output of the previous gradient steps and accessing the data to compute the new parameters \(\theta_{i}\). Then, \(M_{[k]}\) is the mechanism that releases all the model checkpoints \(\theta_1,\ldots, \theta_k\) <d-footnote>Another setting, which we do not investigate, is one where only the final outcome is released and other intermediary steps are hidden. For reference, check <d-cite key="feldman2018privacy"></d-cite>.</d-footnote>. Therefore, requiring a privacy guarantee on the adaptive composition of gradient steps is to protect against adversaries seeing the entire parameter trajectory, not just the final model. This strong guarantee is often desirable since:</p> <ul> <li>Training checkpoints are commonly saved for monitoring convergence, early stopping, ensembling</li> <li>Models may be fine-tuned from intermediate checkpoints</li> <li>In federated learning, updates are explicitly shared with all participating agents<d-footnote>Note that the notion of DP used in federated learning is usually slightly different than the one we are using. The guarantee is usually not with respect $D$ and $D'$ differing by one datapoint, but instead $D$ and $D'$ differ by all the datapoints belonging to a single user. This is well explained in <d-cite key="ponomareva2023dpfy"></d-cite> </d-footnote>.</li> </ul> <p>Finally, going back to our goal of calculating \((\varepsilon_{\text{tot}}, \delta_{\text{tot}})\), we can achieve this using the advanced composition theorem:</p> <blockquote> <p><strong>Theorem (Advanced Composition)</strong><d-cite key="dwork2010boosting"></d-cite><d-cite key="dwork2014algorithmic"></d-cite>: For \(\delta'\in (0,1)\), the \(k\)-fold adaptive composition of \((\varepsilon,\delta)\)-DP mechanisms satisfies \((\varepsilon',k\delta+\delta')\)-DP, where \(\varepsilon' = \varepsilon\sqrt{2k \ln(1/\delta')} + k\varepsilon(e^\varepsilon - 1)\) <d-footnote> This is not tight. For the tight bound, check <d-cite key="kairouz2015composition"></d-cite>.</d-footnote>.</p> </blockquote> <h4 id="a-first-try-at-dp-training">A first try at DP training:</h4> <p>After introducing the advanced composition theorem, we technically have all the ingredients for a first trial to train a small two-layer neural network on a simple dataset. For our model we will use a simple two-layer neural network with ReLU activation and \(128\) hidden units. For the data, we will use \(5000\) randomly sampled images from MNIST. To train our model with DP, we first need to set the hyperparameters \(C\) (clipping norm) and \(T\) (number of iterations). Then, after picking the privacy guarantee we want by setting \(\varepsilon\) and \(\delta\), we can use the Gaussian mechanism along with advanced composition result to calculate the magnitude of Gaussian noise required at each iteration.</p> <p>In fact, we may make use of a handy result from <d-cite key="kairouz2015composition"></d-cite>, which states that to get $(\varepsilon,\delta)$ it is sufficient to have each inner Gaussian mechanism satisfy $(\varepsilon_0, \delta_0)$ with $\varepsilon_0 = \frac{\varepsilon}{2\sqrt{T\log(e + \varepsilon/\delta)}}$ and $\delta_0 = \frac{\delta}{2T}$. We can then use $\varepsilon_0$ and $\delta_0$ to calculate the amount of noise we need to have. However, we are left with two hyperparameters to tune $C$ and $T$. For the gradient clipping $C$, one common heuristic is to tune it is to run the training without any DP, measure the distribution of the gradient norms, and pick $C$ so that we are doing some clipping but not a lot of clipping<d-footnote>This is a bit vague. It is hard to be very specific about hyperparameter tuning intuitions.</d-footnote> <d-footnote>On a side note, in the wider machine learning community, gradient clipping is being used to stabilize training. The critical difference is that we are computing the average of clipped gradients, while the (widely used) gradient clipping is often a clipping of the average gradients.</d-footnote><d-footnote>Tuning hyperparameters using non-DP runs could leak information. We skip this detail here but proper DP training requires accounting for hyperparameter tuning within the privacy budget, see <d-cite key="papernot2022hyperparameter"></d-cite></d-footnote>. $T$ can also be tricky to tune. For a larger $T$, we are able to train longer but we need to use smaller $\varepsilon_0$ and $\delta_0$ forcing us to add more noise at each iteration.</p> <p>To get some intuition of the tuning of $C$ and $T$, let’s try a training run without any DP to see the gradient norms and the loss curves. We run gradient descent with learning rate $0.01$ for $5000$ iterations. For the gradients norm, the $95\%$ quantile is around $32$ and very small number of gradients go above $40$. For the sake of round numbers, let’s take $C=30$ <d-footnote>One may want to do better hyperparameter tuning in practice, but this blog is for illustrations only. More aggressive clipping can also work better.</d-footnote>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/gradients_cdf-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/gradients_cdf-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/gradients_cdf-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/gradients_cdf.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/gradients_hist-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/gradients_hist-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/gradients_hist-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/gradients_hist.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Plots to illustrate the gradient norm distribution for training without DP. </div> <p>Now let’s look at the test loss and accuracy. In the no DP setting, it looks like we should not train for more than $3000$ iterations. Also, very little progress occurs after $1000$.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/simple_training_acc_loss-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/simple_training_acc_loss-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/simple_training_acc_loss-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/simple_training_acc_loss.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Loss and accuracy curves for training without DP, starts indicate locations of smallest loss and largest accuracy. </div> <p>Now, going back to our goal of DP training, depending on the number of iterations \(T\), let’s try to calculate the noise level \(\sigma\) needed for \(\varepsilon=1\), \(\delta=10^{-6}\), and \(C=30\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/advanced_comp_sigma-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/advanced_comp_sigma-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/advanced_comp_sigma-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/advanced_comp_sigma.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>That’s actually a lot of noise to inject into GD. Let’s just try training for \(100\) iterations to see the current state of progress.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/advanced_comp_training-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/advanced_comp_training-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/advanced_comp_training-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/advanced_comp_training.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Well it’s not particularly good…</p> <h3 id="tighter-analysis-via-renyi-differential-privacy">Tighter Analysis via Renyi Differential Privacy</h3> <p>Can we do better? Given that this blog exists, the answer is probably yes. The advanced composition theorem, while powerful, is mechanism-agnostic - it gives a worst-case bound that must hold for any sequence of \((\varepsilon,\delta)\)-DP mechanisms. For the Gaussian mechanism specifically, we can do much better.</p> <p>Recall from our analysis of the Gaussian mechanism that the privacy loss random variable follows a Gaussian distribution:</p> \[L(M,D,D') \sim \mathcal{N}\left(\frac{\|f(D)-f(D')\|_2^2}{2\sigma^2}, \frac{\|f(D)-f(D')\|_2^2}{\sigma^2}\right)\] <p>We also showed that to prove $(\varepsilon,\delta)$-DP, it suffices to bound the tails of this privacy loss random variable:</p> \[P(L(M,D,D') &gt; \varepsilon) \leq \delta\] <p>This suggests that if we can characterize the distribution of the privacy loss random variable more precisely, we might get tighter privacy guarantees. This leads us to Rényi Differential Privacy (RDP)<d-cite key="mironov2017renyi"></d-cite>, which bounds the log moments of the privacy loss random variable:</p> <p><strong>Definition</strong>: A mechanism \(M\) satisfies \((\alpha,\varepsilon)\)-RDP if for all neighboring datasets \(D,D'\): \(D_\alpha(M(D)||M(D')) := \frac{1}{\alpha-1}\log\mathbb{E}\left[\exp((\alpha-1)L(M,D,D'))\right] \leq \varepsilon\)</p> <p>For the Gaussian mechanism with noise scale \(\sigma\Delta_f\), we can show that it satisfies \((\alpha,\frac{\alpha}{2\sigma^2})\)-RDP for all \(\alpha &gt; 1\) <d-footnote>What we just stated here is actually concentrated differential privacy, which we don't explore in details. It is very closely related to RDP. For more, check <d-cite key="bun2016concentrated"></d-cite>.</d-footnote>. Importantly, each RDP guarantee implies a family of \((\varepsilon,\delta)\)-DP guarantees through the following conversion theorem:</p> <p><strong>Theorem (RDP Conversion)</strong><d-cite key="mironov2017renyi"></d-cite>: If \(M\) is \((\alpha,\varepsilon)\)-RDP, then for any \(\delta &gt; 0\), \(M\) also satisfies \((\varepsilon + \frac{\log(1/\delta)}{\alpha-1}, \delta)\)-DP.</p> <p>Thus, the parameter \(\alpha\) dictates the relation between \(\varepsilon\) and \(\delta\). This means that each moment bound on the privacy loss random variable captures a different tradeoff between \(\varepsilon\) and \(\delta\). Nonetheless, the key advantage of RDP is its much cleaner composition theorem:</p> <p><strong>Theorem (RDP Composition)</strong><d-cite key="mironov2017renyi"></d-cite>: If \(M_1\) is \((\alpha,\varepsilon_1)\)-RDP and \(M_2\) is \((\alpha,\varepsilon_2)\)-RDP, then their adaptive composition is \((\alpha,\varepsilon_1+\varepsilon_2)\)-RDP.</p> <p>Thus to compose a series of RDP mechanisms, we can simply add their epsilons. The Gaussian mechanism has another remarkable property - it simultaneously satisfies RDP at all orders \(\alpha &gt; 1\) with:</p> \[\varepsilon(\alpha) = \frac{\alpha}{2\sigma^2}.\] <p>As a result, for any sequence of \(k\) Gaussian mechanisms with noise scale \(\sigma \Delta_f\), we achieve \((\alpha, \frac{k\alpha}{2\sigma^2})\)-RDP for all \(\alpha &gt; 1\). Converting to \((\varepsilon,\delta)\)-DP, this gives us a family of guarantees:</p> \[\left(\frac{k\alpha}{2\sigma^2} + \frac{\log(1/\delta)}{\alpha-1}, \delta\right).\] <p>For any choice of \(\alpha &gt; 1\) and \(\delta &gt; 0\). Different values of \(\alpha\) give us different tradeoffs - looking at the bound, larger \(\alpha\) values may work better for small \(\delta\).</p> <p>The Gaussian mechanism satisfies RDP for an infinite list of alphas and that each \(\alpha\) gives rise to an infinite list of \((\varepsilon, \delta)\)-DP algorithms. A naturally arising question is: which \(\alpha\) should we pick. Again, the answer lies in leveraging automatic solvers to find the best possible \(\alpha\) for us. For example, if we want to calculate the DP privacy guarantee of the \(T\) times composition of the Gaussian mechanism, a typical workflow for using these automatic solvers is to give them the noise level \(\sigma\), the sensitivity \(\Delta_f\) of the function we are trying to make DP, and the number of times \(T\) we are composing this mechanism. Then, through a mixture of symbolic and numerical solutions, the solver will aim to find the best possible \(\varepsilon\) for a given \(\delta\) by trying a long list of candidate alphas. Another possible workflow is to give the solver the \(\varepsilon, \delta, \Delta_f\), and \(T\) and then to get the smallest possible \(\sigma\) needed. All of this in the hope of getting the tightest possible bounds. Thus, the theme of using numerical methods, which we first saw with the Analytic Gaussian mechanism, strikes back. Throughout this blog, we will be using the RDP accounting tools of Google dp-accounting library <d-footnote>Check https://pypi.org/project/dp-accounting . Tighter DP accounting can be possible by using the Privacy Loss Distribution (PLD) accounting tools of the library, which leverages <d-cite key="doroshenko2022connect"></d-cite>. For the experiments, we used the RDP accountant to stay closer to the content.</d-footnote>.</p> <h4 id="a-second-try-at-dp-training">A second try at DP training:</h4> <p>Armed with RDP and its cleaner composition result, let’s retry the experiments of the last subsection.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/rdp_comp_sigma-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/rdp_comp_sigma-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/rdp_comp_sigma-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/rdp_comp_sigma.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This is more encouraging. We need a way smaller noise for the same exact privacy guarantees. Let’s be brave and try training for \(T=200\) steps this time.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/rdp_comp_training-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/rdp_comp_training-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/rdp_comp_training-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/rdp_comp_training.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Well, we are actually able to train!</p> <h3 id="the-power-of-subsampling-from-gd-to-sgd">The Power of Subsampling: From GD to SGD</h3> <p>In practice, we rarely use full GD, preferring stochastic gradient descent (SGD), which operates on random minibatches. Beyond very small datasets on simple models, SGD is indispensable, as using the full dataset at each iteration is too computationally expensive. In particular, SGD subsamples a random minibatch at the start of each training iteration. However, a critical issue with subsampling is the expanded sensitivity. To illustrate, consider again the problem of computing the mean salary of \(n = 10^4\) employees, where the sensitivity of the full-dataset query scales as \(\mathcal{O}(1/n)\). Suppose instead of using the full dataset, we sample each employee independently with probability \(0.025\), sum the selected salaries, and normalize by \(250\) — the expected number of selected employees. In this case, the normalization is fixed, but the number of contributing salaries is random. The worst-case sensitivity now scales as \(\mathcal{O}(1/250)\), which is larger than in the full-dataset case. This increase in sensitivity forces us to add more noise to maintain the same privacy guarantee. <d-footnote>While this sampling strategy appears unnatural, it actually mirrors the Poisson subsampling procedure in DP-SGD, which we shortly introduce.</d-footnote></p> <p>Nonetheless, implementing a differentially private SGD is still possible. Subsampling itself, under some conditions, can be shown to provide privacy benefits through <strong>privacy amplification by subsampling</strong>. Hence, the stochasticity of SGD makes each step more private, essentially allowing us to train with less noise. As a result, the tradeoff of increased sensitivity along with subsampling privacy amplification typically cancels out enabling us to use SGD<d-footnote> This tradeoff between amplification by subsampling and increased sensitivity was recently studied in <d-cite key="2024subsampling"></d-cite>.</d-footnote>, with almost the same amount of noise per iteration, independently of the batchsize. In practice, SGD is implemented by using a random batch with fixed batchsize at each iteration. For the following section, we will assume a different sampling strategy, which we will refer to as Poisson subsampling. We assume that given a dataset, at each step of SGD, each datapoint is selected for training independently with probability \(q\).</p> <p>For the Gaussian mechanism specifically, when we combine:</p> <ul> <li>Poisson subsampling with rate \(q\)</li> <li>Gaussian noise with scale \(\sigma\Delta_f\)</li> </ul> <p>We get the following RDP guarantee:</p> <p><strong>Theorem (Informal)</strong><d-cite key="mironov2019r"></d-cite>: For \(\alpha &gt; 1\), the subsampled Gaussian satisfies \((\alpha,\frac{q^2\alpha}{\sigma^2})\)-RDP <d-footnote>This statement is only proved under certain conditions on $\sigma$ and $\alpha$. Again, in practice, we will be using a numerical method to get a tight characterization.</d-footnote>.</p> <p>This \(O(q^2)\) factor is crucial as \(q\) is usually quite small in practice. Combined with RDP composition, this enables us to use much smaller noise for the same privacy guarantee.</p> <p>Those building blocks allow us to finally recover the DP-SGD algorithm<d-cite key="abadi2016deep"></d-cite>:</p> <ol> <li>Randomly sample batch with rate \(q\)</li> <li>Clip the gradient for each example.</li> <li>Average and add \(\mathcal{N}(0,\sigma^2 I)\) noise</li> <li>Update parameters</li> <li>Track privacy via RDP composition<d-footnote>Technically the original DP-SGD was not introduced with the framework of RDP but with the moment accountant. Motivated by the structure of Gaussian noise, the authors proposed an algorithm to track the composition of Gaussian mechanisms through the moments of the privacy loss random variable. Later, RDP was introduced in <d-cite key="mironov2019r"></d-cite>, which in some sense generalized the approach of tracking composition through the moment behavior of the privacy loss random variable. For a nice discussion on this and a substantially more rigorous results on the amplification by subsampling of RDP, check <d-cite key="Wang_Balle_Kasiviswanathan_2021"></d-cite>. In an earlier work, Song et al. introduced a simpler version of DP-SGD <d-cite key="song2013stochastic"></d-cite></d-footnote></li> </ol> <h4 id="a-third-try-at-dp-training">A third try at DP training.</h4> <p>The tight analysis through RDP and privacy amplification allows training SGD with reasonable noise scales. For example, let’s try to train for the previous setting with a subsampling rate \(q=0.05\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/subsampled_rdp_comp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/subsampled_rdp_comp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/subsampled_rdp_comp-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/subsampled_rdp_comp.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Using subsampling, we get basically the same noise levels for SGD as GD. However, it must be noted that the figures we have are also a function of the numerical solver we are using to compute the \(\sigma\) for a given setting. Here, as we previously stated, we are using the RDP accountant of the Google dp-accounting library. Using tighter numerical solvers, we can actually show that subsampling allows us to add less noise, especially at smaller values of \(T\) and \(q\).</p> <p>Let’s try another training trial this time with SGD.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/subsample_training-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/subsample_training-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/subsample_training-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/subsample_training.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>📝 <strong>In summary</strong>, in the previous subsection, we already showed that it is possible to effectively train DP neural networks using GD. Typically, due to computational requirements, we are forced to use SGD. However, in SGD, we are using smaller batches at each step and thus increasing the sensitivity. As a result, if we naively train with SGD, we will be forced to add much more noise at each step. Privacy amplification by subsampling allows us to solve this dilemma by showing that subsampling itself amplifies the privacy guarantee. So, when training with SGD, we need much less noise. All of these factors roughly even out, making the noise scale needed for DP-SGD reasonable.</p> </blockquote> <h2 id="beyond-dp-sgd-using-correlated-noise">Beyond DP-SGD: Using Correlated Noise</h2> <p>While DP-SGD with privacy amplification has become the standard approach for differentially private deep learning, it faces significant limitations:</p> <ol> <li> <p><strong>Reliance on Privacy Amplification</strong>: Privacy amplification through subsampling requires strong assumptions on how data is processed. In many cases, the data may not fit in memory and sampling each datapoint with equal probability is not feasible.</p> </li> <li> <p><strong>Suboptimal Noise Addition</strong>: DP-SGD treats each gradient step independently, potentially leading noise accumulation. For the sake of illustration, consider the case where the gradients are constant. Then, the noise of DP-SGD, added independently at each iteration, will keep accumulating.</p> </li> </ol> <p>If not DP-SGD, what else can we do? Well, a growing line of work investigates adding <strong>correlated noise</strong> instead of independent noise at each iteration. To our knowledge, using this idea for training neural networks was first explored in <d-cite key="kairouz2021practical"></d-cite><d-footnote> Prior to <d-cite key="kairouz2021practical"></d-cite>, the idea of using correlated noise was studied in the streaming setting of DP <d-cite key="dwork2010differential"></d-cite>. A brief summary of this setting and its application to SGD can be found in <d-cite key="denisov2022improved"></d-cite></d-footnote>. In the rest of this blog, we will look at the matrix factorization mechanism <d-cite key="choquette2023multiMF"></d-cite>, which is a specific mechanism that allows us to train DP models while adding correlated noise through the iterations.</p> <h3 id="putting-gradient-descent-in-matrix-form">Putting Gradient Descent in Matrix Form</h3> <p>To understand the matrix factorization mechanism, let’s first look at how standard gradient descent can be viewed in matrix form. Consider training for \(T\) steps with learning rate \(\eta\). At each step \(t\), gradient descent computes:</p> \[\theta_t = \theta_{t-1} - \eta \nabla L_t(\theta_{t-1}).\] <p>After \(T\) steps:</p> \[\theta_T = \theta_0 - \eta\sum_{t=1}^T \nabla L_t(\theta_{t-1}).\] <p>This summation can be rewritten using matrices. Let’s stack all gradients into a matrix \(G\):</p> \[G = \begin{bmatrix} \nabla L_1(\theta_0) \\ \nabla L_2(\theta_1) \\ \vdots \\ \nabla L_T(\theta_{T-1})\end{bmatrix}.\] <p>The parameter trajectory can then be written using a lower triangular matrix:</p> \[\begin{bmatrix} \theta_1 \\ \theta_2 \\ \vdots \\ \theta_T \end{bmatrix} = \begin{bmatrix} \theta_0 \\ \theta_0 \\ \vdots \\ \theta_0 \end{bmatrix} - \eta\begin{bmatrix} 1 &amp; 0 &amp; \cdots &amp; 0 \\ 1 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; 1 &amp; \cdots &amp; 1 \end{bmatrix} \begin{bmatrix} \nabla L_1(\theta_0) \\ \nabla L_2(\theta_1) \\ \vdots \\ \nabla L_T(\theta_{T-1}) \end{bmatrix}.\] <p>Let’s denote this lower triangular matrix by \(A\), i.e.,</p> \[A := \begin{bmatrix} 1 &amp; 0 &amp; \cdots &amp; 0 \\ 1 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; 1 &amp; \cdots &amp; 1 \end{bmatrix}.\] <p>Then we can write all iterations of gradient descent in a more compact form</p> \[\Theta = \Theta_0 - \eta AG,\] <p>such that \(\Theta_0\) is the matrix with all rows set to \(\theta_0\) and the \(i\)-th row of \(\Theta\) is \(\theta_i\). Here, \(A\) encodes how past gradients influence current parameters. Each row of \(A\) represents which gradients have been accumulated up to that step.</p> <h3 id="dp-sgd-in-matrix-form">DP-SGD in Matrix Form</h3> <p>Now, let’s consider DP-SGD, which adds noise at each step:</p> \[\theta_t = \theta_{t-1} - \eta(h_t + z_t)\] <p>where \(z_t \sim \mathcal{N}(0, \sigma^2I)\) and \(h_t\) is the stochastic clipped gradient at iteration \(t\). Again, we can write in a more vectorized form, as</p> \[\begin{bmatrix} \theta_1 \\ \theta_2 \\ \vdots \\ \theta_T \end{bmatrix} = \begin{bmatrix} \theta_0 \\ \theta_0 \\ \vdots \\ \theta_0 \end{bmatrix} - \eta A \left(\begin{bmatrix}h_1\\ h_2 \\ \vdots \\ h_T \end{bmatrix} + \begin{bmatrix} z_1 \\ z_2 \\ \vdots \\ z_T \end{bmatrix}\right).\] <p>Alternatively, we may also express this in the more compact form as \(\Theta = \Theta_0 - \eta A(H + Z)\) with matrix \(Z\) being the stacking of all noise vectors.</p> <p>One drawback of DP-SGD is independent noise accumulation. To see why correlating noise across steps could help, let’s analyze how noise accumulates in DP-SGD versus alternative schemes. Consider a simple case where we make two steps, then</p> \[\theta_2 = \theta_0 - \eta(h_1 + z_1) - \eta(h_2 + z_2).\] <p>Since \(z_1\) and \(z_2\) are independent, the total noise variance is \(2\eta^2\sigma^2\). More generally, after \(k\) passes, the variance grows linearly with \(k\). Now consider an alternative scheme where \(z_1 = -z_2\), then \(\theta_2 = \theta_0 - \eta(h_1 + h_1)\)</p> <p>The noise cancels out. However, releasing \(\theta_2\) clearly offers no privacy guarantees at all. While this specific scheme isn’t DP, it illustrates how correlating noise across steps that process the same data could reduce total variance. Then, the question becomes how to do so in a DP way.</p> <h3 id="the-matrix-factorization-framework">The Matrix Factorization Framework</h3> <p>Let’s try to generalize and take a wider perspective on DP-SGD. From writing it in matrix form, we can understand DP-SGD as a method to compute \(AH\) in a DP way by outputting</p> \[\widehat{AH} = A(H+Z),\] <p>with a noise matrix \(Z\). The key insight is that we can factorize \(A = BC\). Again, our goal is still to report a DP version of \(AH\). However, we can now do it in an alternative way by using</p> \[\widehat{AH} = B(CH+Z).\] <p>Here, we shift the placement of the DP mechanism to make it on the computation of \(CH\). Since \(A\) is independent of the data, \(B\) is also independent of the data. So if \(CH\) is computed in a DP way, then so will \(BCH=AH\). Then, if \(C\) is invertible, we can equivalently rewrite this as</p> \[\widehat{AH} = A(H+C^{-1}Z).\] <p>This is actually what we want as the noise: the matrix \(C^{-1}Z\) is made from correlated noise. Assuming we factorize \(A=BC\) with \(B=A\) and \(C=I\). Then, the above statement reduces to</p> \[\widehat{AH} = A(H+C^{-1}Z) = AH+AZ.\] <p>This is how DP-SGD works, as for our application, the \(i\)-th row of \(H\) is the \(i\)-th gradient vector. In addition, each row of \(Z\) is an independent realization from \(\mathcal{N}(0,\sigma^2 I)\). If we want to add correlated noise, we need to have \(C^{-1}\neq I\). Now, the question is finding a factorization \(A=BC\) such that</p> <ol> <li>\(C\) is invertible</li> <li>\(C^{-1}\) has a noise correlation structure that makes as much as possible of the total noise cancel out, in other words, optimizes the utility result.</li> <li>Evaluate the needed scale \(\sigma\) of the noise \(Z\), such that \(CH+Z\), or equivalently, \(H+C^{-1}Z\) is \((\varepsilon,\delta)\)-DP</li> </ol> <p>In other words, various choices of \((C, \sigma)\) can be made to ensure \((\varepsilon,\delta)\)-DP, and some result in a better utility than the one choice \(C=I\).</p> <p>Finding \(B\) and \(C\) to achieve the above goals is non-trivial. Again, we strongly rely on numerical solvers to find the best \(B\) and \(C\) and correspondingly calculate the required \(\sigma\) for \((\varepsilon, \delta)\)-DP. For details, refer to <d-cite key="choquette2023multiMF"></d-cite> and <d-cite key="choquette2024amplifiedMF"></d-cite>. Crucially, the computations for the decomposition of \(B\) and \(C\) is typically independent of the gradient matrix \(H\). Some considerations on using the approaches of <d-cite key="choquette2023multiMF"></d-cite> and <d-cite key="choquette2024amplifiedMF"></d-cite> are:</p> <ul> <li>Be careful how you sample: we used Poisson sampling for DP-SGD, i.e., at each iteration, each datapoint is randomly selected with some fixed probability. We need to be careful how we sample datapoints with the matrix factorization setting. You cannot just plug the Poisson subsampling. In particular, when computing \((\varepsilon, \delta)\) guarantees, we must account for the maximum number of times any datapoint is used in gradient computations and a sampling structure where a datapoint cannot participate in gradient computations less than a prespecified fixed number of steps apart. Violating either will invalidate the privacy analysis.</li> <li>Subsampling amplification: one advantage of matrix factorization is that it is competitive with DP-SGD even without fully relying on any privacy amplification through subsampling<d-cite key="choquette2023multiMF"></d-cite>. A privacy amplification through subsampling for the matrix factorization was introduced in <d-cite key="choquette2024amplifiedMF"></d-cite>. With this amplification, in the settings tested in the paper, the matrix factorization mechanism was always Pareto optimal with respect to the privacy-accuracy tradeoff. Again, one should be careful with subsampling amplifications as they should be implemented in a way that does not violate the previous remark.</li> </ul> <h4 id="structure-of-b-and-c">Structure of \(B\) and \(C\).</h4> <p>To illustrate a possible structure of the matrices \(B\) and \(C\), we use the matrices computed and released by <d-cite key="choquette2024amplifiedMF"></d-cite>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/matrix_structure-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/matrix_structure-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/matrix_structure-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/matrix_structure.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Raw values for the matrices $A$, $B$ and $C$. Figure generated from the code and results of C.A. Choquette-Choo, et al. <d-cite key="choquette2024amplifiedMF"></d-cite> </div> <p>Note that we strongly care about the structure of \(C^{-1}\) as it modulates the correlation between the noise added at different iterations. Thus, let’s plot the distribution of the elements in \(C^{-1}\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/distribution_c_inv-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/distribution_c_inv-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/distribution_c_inv-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/distribution_c_inv.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>While most elements are zero, we can observe a positive and a negative cluster. The negative cluster allows for negative correlation between noise added at different iterations, which causes some of the noise to cancel out. Specifically, returning to our optimization setting, at iteration \(i\), the output of our mechanism is</p> \[\theta_i = \theta_0 - \eta A_{[i,:]}(H+C^{-1}Z),\] <p>where \(A_{[i,:]}\) is the \(i\)-th row of \(A\). Thus, the standard deviation of the noise at iteration \(i\) can be seen as scaled with \((AC^{-1})_{[i,:]}\). To illustrate the benefit of using a noise correlation matrix, i.e, \(C\neq I\), we can plot the \(\ell_1\) norms of \((AC^{-1})_{[i,:]}\) and \(A_{[i,:]}\) across iterations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/noise_acc-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/noise_acc-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/noise_acc-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/noise_acc.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We can see that the curve corresponding to \(A_{[i,:]}\) increases at a much higher rate as none of the noise cancels out.</p> <h4 id="a-fourth-try-at-dp-training">A fourth try at DP training</h4> <p>Now let’s go back to the running MNIST training setting, where we train a two-layer neural network on a \(5000\) images of MNIST with \((\varepsilon, \delta)\)-DP for \(\varepsilon=1\) and \(\delta=1e-6\). For the matrices \(B\) and \(C\) and the calibration of \(\sigma\), we used the tools released by <d-cite key="choquette2024amplifiedMF"></d-cite>. For reference, with \(T=200\), \(C=30\), and a batchsize of \(25\), we obtained \(\sigma \approx 4\). This is a much higher noise scale than what we used with DP-SGD. However, as the noise partially cancels out, we are still able to train effectively.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/mat_fact_train-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/mat_fact_train-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/mat_fact_train-1400.webp"/> <img src="/2025/assets/img/2025-04-28-building-blocks-of-differentially-private-training/mat_fact_train.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This is competitive with DP-SGD.</p> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, we explored the building blocks of differentially private training through two main approaches:</p> <ul> <li>DP-SGD, which adds independent Gaussian noise at each iteration, with privacy amplification through subsampling</li> <li>Matrix factorization mechanisms, which reduce noise accumulation by using carefully correlated noise across iterations</li> </ul> <p>Both approaches offer viable paths to private deep learning, with different tradeoffs. For practitioners looking to train neural networks with differential privacy, experimenting with both approaches may be valuable, as their relative performance can depend on factors like model architecture, dataset size, and privacy requirements.</p> <p>Finally, we added some of the code to generate the simulations in this <a href="https://github.com/mahegz/iclr25_dp_blog">repo</a>, which will also host any post-publication updates or corrections to the blog.</p> <p><strong>Acknowledgments</strong></p> <p>We are deeply grateful for the insightful feedback and suggestions that helped shape and improve this blog post. We extend our sincere thanks to the anonymous reviewers for their valuable comments. We would also like to express our appreciation to Paul Mangold, Edwidge Cyffers, Adrien Majka, Martin Van Waerebeke, Daniel Berg Thomsen, and Renaud Gaucher for their careful reading and constructive feedback during the preparation of this work.</p>]]></content><author><name>Mahmoud Hegazy</name></author><summary type="html"><![CDATA[In this blog, we introduce the building blocks of training a neural network in a differentially private way.]]></summary></entry><entry><title type="html">Reassessing EMNLP 2024’s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold Up?</title><link href="https://starlight345.github.io/2025/blog/calibrated-mia/" rel="alternate" type="text/html" title="Reassessing EMNLP 2024’s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold Up?"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://starlight345.github.io/2025/blog/calibrated-mia</id><content type="html" xml:base="https://starlight345.github.io/2025/blog/calibrated-mia/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>At EMNLP 2024, the <a href="https://x.com/emnlpmeeting/status/1857176180128198695/photo/1">Best Paper Award</a> was given to <strong>“Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method”</strong><d-cite key="zhang2024pretraining"></d-cite>. The paper addresses Membership Inference Attacks (MIAs), a key issue in machine learning related to privacy. The authors propose a new calibration method and introduce <strong>PatentMIA</strong>, a benchmark utilizing temporally shifted patent data to validate their approach. The method recalibrates model probabilities using a divergence metric between the outputs of a target model and a token-frequency map (basically a histogram) derived from auxiliary data, claiming improved detection of member and non-member samples.</p> <p>However, upon closer examination, we identified significant shortcomings in both the experimental design and evaluation methodology. The proposed dataset introduces a temporal shift between the distribution of member and non-member data, which can lead to overestimation of the performance of an MIA that may end up distinguishing samples based on the temporal range, and not actual membership.</p> <p>In this post, we critically analyze this shift, and the broader implications of MIA evaluations for models in the wild.</p> <h2 id="what-is-membership-inference">What is Membership Inference?</h2> <p>Membership Inference Attacks (MIAs) are a useful tool in assessing memorization of training data by a model trained on it. Given a model \(D\) samples from some underlying distribution \(\mathcal{D}\) and a model \(M\) trained on \(D\), membership inference <d-cite key="yeom2018privacy"></d-cite> asks the following question:</p> <blockquote> <p>Was some given record \(x\) part of the training dataset \(D\), or just the overall distribution \(\mathcal{D}\)?</p> </blockquote> <p>The underlying distribution \(\mathcal{D}\) is assumed to be large enough to the point where the above test can be reframed as inferring whether \(x \in D\) (via access to \(M\)) or not. In practice, the adversary/auditor starts with some non-member data (data that they know was not part of the training data \(D\), but belongs to the same underlying distribution \(\mathcal{D}\)) and on the basis of some scoring function, generates a distribution of scores for these non-members. A sweep over these values can then yield “thresholds” corresponding to certain false-positive rates (FPRs), which can then be used to evaluate the true-positive rate (TPR) of the approach under consideration.</p> <p>It is important to note here that these non-members should be from the <strong>same</strong> underlying distribution. To better understand why this is important, think of a model trained for the binary classification task of distinguishing images of squirrels and groundhogs <d-footnote>Maybe you want to give nuts to squirrels and vegetables to groundhogs </d-footnote>. For this example, let’s say this particular groundhog image was part of the training data, but the other two weren’t.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2025/assets/img/2025-04-28-calibrated-mia/groundhog.avif" sizes="95vw"/> <img src="/2025/assets/img/2025-04-28-calibrated-mia/groundhog.avif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2025/assets/img/2025-04-28-calibrated-mia/squirrel-480.webp 480w,/2025/assets/img/2025-04-28-calibrated-mia/squirrel-800.webp 800w,/2025/assets/img/2025-04-28-calibrated-mia/squirrel-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2025/assets/img/2025-04-28-calibrated-mia/squirrel.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2025/assets/img/2025-04-28-calibrated-mia/llama.webp" sizes="95vw"/> <img src="/2025/assets/img/2025-04-28-calibrated-mia/llama.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>A model will have higher loss on images of llamas, and understandably so since these are images the model did not see at all during training. Using their images would give a clear member/non-member distinction, but would also probably classify <em>any</em> squirrel/groundhog image as a member, even if it wasn’t. As an experimental setup, this is easily enforced when working with standard machine learning models and datasets such as CIFAR-10 and ImageNet, where well-established train/test splits from the same underlying distribution exist.</p> <h3 id="whats-special-about-llms">What’s Special about LLMs?</h3> <p>Because these models are trained on a large scale of data (and in many cases, exact training data is unknown), it is hard to collect data to use as “non-members” which has not been used in the model training <strong>and</strong> is from the same underlying distribution. Early works on membership inference for LLMs resorted to using data generated after a model’s training cutoff <d-cite key="shi2023detecting"></d-cite>, since such data could not have been seen by a model. However, such design choices can introduce implicit distribution shifts <d-cite key="das2024blind,duan2024membership,maini2024llm,meeus2024sok"></d-cite> and give a false sense of membership leakage.</p> <h2 id="method-overview">Method Overview</h2> <p>The proposed method tries to fix a known issue with MIAs: models often fail to properly separate member and non-member samples. To address this, the authors use an auxiliary data-source to compute token-level frequencies, which are then used to recalibrate token-wise model logits. This normalization aims to adjust token-level model probabilities based on their natural frequency or rarity, aligning with membership inference practices such as reference model calibration<d-cite key="carlini2022membership"></d-cite>.</p> <p>They also introduce <strong>PatentMIA</strong>, a benchmark that uses temporally shifted patents as data. The idea is to test whether the model can identify if a patent document was part of its training data or not. While this approach sounds interesting, our experiments suggest that the reported results are influenced by limitations in the benchmark design.</p> <h2 id="experimental-evaluation">Experimental Evaluation</h2> <p>We ran two key experiments to test the paper’s claims: one for true positives and another for false positives.</p> <h3 id="true-positive-rate-experiment">True Positive Rate Experiment</h3> <p>This experiment checks if the method can correctly distinguish member data from non-member data when both are drawn from the <strong>same distribution</strong>. We used train and validation splits from <strong>The Pile</strong> dataset, which ensures there are no temporal or distributional differences between the two sets. Below we report results for the <em>Wikipedia</em> split.</p> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: center">AUC</th> <th style="text-align: right">TPR@5%FPR</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><a href="https://huggingface.co/EleutherAI/pythia-6.9b">Pythia-6.9B</a></td> <td style="text-align: center">0.542</td> <td style="text-align: right">0.071</td> </tr> <tr> <td style="text-align: left"><a href="https://huggingface.co/EleutherAI/gpt-neo-125m">GPT-Neo-125M</a></td> <td style="text-align: center">0.492</td> <td style="text-align: right">0.054</td> </tr> <tr> <td style="text-align: left"><a href="https://huggingface.co/EleutherAI/gpt-neox-20b">GPT-NeoX-20B</a></td> <td style="text-align: center">0.600</td> <td style="text-align: right">0.103</td> </tr> </tbody> </table> <p><strong>Result:</strong><br/> The method performs only slightly better than the LOSS attack, and remains comparable to most standalone membership inference attacks. For reference, AUC with the baseline LOSS and zlib <d-cite key="carlini2021extracting"></d-cite> attacks for Pythia-6.9B are 0.526 and 0.536 respectively, while it is 0.618 when using a reference-model (Table 12 in <d-cite key="duan2024membership"></d-cite>). Similarly, using LOSS and zlib yield AUCs of 0.563 and 0.572 respectively.</p> <p>Reported improvements in the paper (Table 2 <d-cite key="zhang2024pretraining"></d-cite> showing AUCs of 0.7 and higher) are thus <u>likely due to exploiting differences in the data distribution</u>, rather than actual improvements in detecting membership.</p> <h3 id="false-positive-rate-experiment">False Positive Rate Experiment</h3> <p>Next, we check how often the method falsely identifies data as “member” when it has in fact not be used in the model’s training. To do this, we use the <strong>WikiMIA</strong><d-cite key="shi2023detecting"></d-cite> dataset but replaced the training data with unrelated validation data from the <em>Wikipedia</em> split of <strong>The Pile</strong>. This means that we can say with certainty that the Pythia and GPT-neox models did not train on either split. We follow the experimental setup of in Section 3 of <d-cite key="maini2024llm"></d-cite> for this analysis.</p> <p><strong>Result:</strong><br/> Below we report results for the <em>Wikipedia</em> split. Note that in this setting, a score closer to 0.5 is better since both splits are non-members.</p> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: center">AUC for DC-PDD <d-cite key="zhang2024pretraining"></d-cite></th> <th style="text-align: right">AUC for LOSS <d-cite key="carlini2021extracting"></d-cite></th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><a href="https://huggingface.co/EleutherAI/pythia-6.9b">Pythia-6.9B</a></td> <td style="text-align: center">0.667</td> <td style="text-align: right">0.636</td> </tr> <tr> <td style="text-align: left"><a href="https://huggingface.co/EleutherAI/gpt-neo-125m">GPT-Neo-125M</a></td> <td style="text-align: center">0.689</td> <td style="text-align: right">0.671</td> </tr> <tr> <td style="text-align: left"><a href="https://huggingface.co/EleutherAI/gpt-neox-20b">GPT-Neox-20b</a></td> <td style="text-align: center">0.637</td> <td style="text-align: right">0.656</td> </tr> </tbody> </table> <p>The method flags a high number of false positives. It frequently identifies non-member data as part of the training set, suggesting that the attack was was reliant on temporal or distribution artifacts rather than truly detecting membership.</p> <h2 id="the-problem-with-temporally-shifted-benchmarks">The Problem with Temporally Shifted Benchmarks</h2> <p>The introduction of <strong>PatentMIA</strong> highlights a broader problem with MIA research: benchmarks that rely on temporal shifts <d-cite key="meeus2024did,shi2023detecting,dubinski2024towards,ko2023practical"></d-cite>. These benchmarks often make it easy for attack models to exploit simple artifacts, like whether a document contains terms that didn’t exist during training (e.g., “COVID-19” or “Tesla Model Y”). This creates an illusion of success but doesn’t address the real challenge of membership inference.</p> <h3 id="why-these-benchmarks-are-misleading">Why These Benchmarks Are Misleading</h3> <p>The issues with temporally shifted benchmarks are not new. Several prior works have already established the dangers of using such benchmarks:</p> <ol> <li><strong>Spurious Patterns</strong>: Temporal shifts introduce artifacts that are easily exploitable by attack models. As noted by Duan et al. <d-cite key="duan2024membership"></d-cite>, temporal markers (e.g., “COVID-19” or recent events) allow models to cheat by detecting new concepts rather than true membership.</li> <li><strong>Misleading Evaluations</strong>: Maini et al. <d-cite key="maini2024llm"></d-cite> show how temporal shifts can inflate the perceived success of MIAs, even when no meaningful membership inference occurs.</li> <li><strong>Blind Baselines Work Better</strong>: Das et al. <d-cite key="das2024blind"></d-cite> demonstrate that blind baselines often outperform sophisticated MIAs on temporally shifted datasets, highlighting how these benchmarks fail to test real inference ability.</li> </ol> <p>Despite these well-established issues, the EMNLP Best Paper continues to rely on temporally shifted data like <strong>PatentMIA</strong> for its evaluations. This undermines the robustness of its claims and contributes little to advancing membership inference research.</p> <hr/> <h2 id="machine-learning-awards-a-problem-of-incentives">Machine Learning Awards: A Problem of Incentives</h2> <p>This situation raises important questions about the role of awards in machine learning research.</p> <ol> <li><strong>Do Awards Encourage Rushed Work?</strong> Highlighting work with known flaws, like relying on misleading benchmarks, can discourage researchers from investing time in more rigorous evaluations.</li> <li><strong>Harming the Field</strong>: Awards that celebrate flawed work set a bad precedent and can mislead the community into thinking these methods are the gold standard.</li> <li><strong>Losing Credibility</strong>: Over time, the reputation of awards themselves suffers, as researchers may start viewing them as less meaningful.</li> </ol> <p>This is a growing problem in machine learning research, where not only acceptance but even awards are constantly under <a href="https://www.reddit.com/r/MachineLearning/comments/w4ooph/d_icml_2022_outstanding_paper_awards/">scrutiny</a> for their <a href="https://parameterfree.com/2023/08/30/yet-another-icml-award-fiasco/">soundness</a>, let alone their contribution. If awards are to truly highlight excellence, they must emphasize thoroughness, reproducibility, and robustness over surface-level novelty.</p> <h2 id="conclusion">Conclusion</h2> <p>The EMNLP 2024 Best Paper sought to address a pressing challenge in membership inference but falls short under careful scrutiny. The proposed method fails both in distinguishing members and non-members under rigorous conditions and in avoiding false positives when the data is untrained. Furthermore, its reliance on <strong>PatentMIA</strong> exemplifies a larger issue with using temporally shifted benchmarks to claim progress.</p> <p>For the field to advance meaningfully, greater emphasis must be placed on rigorous evaluation practices. Awards should reflect this by rewarding work with robust and thorough evaluations, rather than methods that (knowingly or otherwise) exploit well-known flaws in evaluation practices. Only then can we ensure that the field moves forward in a meaningful way.</p> <h4 id="acknowledgements">Acknowledgements</h4> <p>We would like to thank <a href="https://www.zacharylipton.com/">Zack Lipton</a> and <a href="https://zicokolter.com/">Zico Kolter</a> for their helpful feedback on the draft and for referring us to Nicholas’s <d-cite key="carlini2019ami"></d-cite> example of good criticism.</p>]]></content><author><name>Pratyush Maini</name></author><summary type="html"><![CDATA[TL;DR: No. A critical analysis of the EMNLP Best Paper proposing a divergence-based calibration for Membership Inference Attacks (MIAs). We explore its experimental shortcomings, issues with temporally shifted benchmarks, and what this means for machine learning awards.]]></summary></entry><entry><title type="html">Understanding Model Calibration - A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)</title><link href="https://starlight345.github.io/2025/blog/calibration/" rel="alternate" type="text/html" title="Understanding Model Calibration - A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://starlight345.github.io/2025/blog/calibration</id><content type="html" xml:base="https://starlight345.github.io/2025/blog/calibration/"><![CDATA[<h2 id="what-is-calibration">What is Calibration?</h2> <p><strong>Calibration</strong> makes sure that a model’s estimated probabilities match real-world likelihoods. For example, if a weather forecasting model predicts a 70% chance of rain on several days, then roughly 70% of those days should actually be rainy for the model to be considered well calibrated <d-cite key="dawid1982well, degroot1983comparison"></d-cite>. This makes model predictions more <em>reliable</em> and <em>trustworthy</em>, which makes calibration relevant for many applications across various domains.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f1_reliability_diagram-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f1_reliability_diagram-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f1_reliability_diagram-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f1_reliability_diagram.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 1 | Reliability Diagram </div> <p>Now, what <strong>calibration</strong> means more precisely depends on the specific definition being considered. We will have a look at the most common notion in machine learning (ML) formalised in <d-cite key="guo2017calibration"></d-cite> and termed <strong><em>confidence calibration</em></strong> in <d-cite key="kull2019beyond"></d-cite>. But first, let’s define a bit of formal notation for this blog. In this blogpost we consider a classification task with \(K\) possible classes, with labels \(Y \in \{1, ..., K\}\) and a classification model \(\hat{p} : \mathscr{X} \rightarrow \Delta^K\), that takes inputs in \(\mathscr{X}\) (e.g. an image or text) and returns a probability vector as its output. \(\Delta^K\) refers to the <em>K</em>-simplex, which just means that the elements of the output vector must sum to 1 and that each estimated probability in the vector is between 0 &amp; 1. These individual probabilities (<em>or confidences</em>) indicate how likely an input belongs to each of the \(K\) classes.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f2_notation_updated-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f2_notation_updated-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f2_notation_updated-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f2_notation_updated.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 2 | Notation - input example sourced from <d-cite key="uma2021learning"></d-cite> </div> <h3 id="confidence-calibration">(Confidence) Calibration</h3> <p>A model is considered confidence-calibrated if, for all confidences \(c,\) the model is correct \(c\) proportion of the time:</p> \[\mathbb{P} (Y = \text{arg max}(\hat{p}(X)) \; | \; \text{max}(\hat{p}(X))=c ) = c \;\;\:\: \forall c \in [0, 1] \; ,\] <div class="caption"> where $(X,Y)$ is a datapoint and $\hat{p} : \mathscr{X} \rightarrow \Delta^K$ returns a probability vector as its output </div> <p>This definition of calibration, ensures that the model’s final predictions align with their observed accuracy at that confidence level <d-cite key="guo2017calibration"></d-cite>. The left chart below visualises the perfectly calibrated outcome (green diagonal line) for all confidences using a binned reliability diagram <d-cite key="guo2017calibration"></d-cite>. On the right hand side it shows two examples for a specific confidence level across 10 samples.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f3_confidence_calibration-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f3_confidence_calibration-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f3_confidence_calibration-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f3_confidence_calibration.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 3 | Confidence Calibration </div> <p>For simplification, we assume that we only have 3 classes as in image 2 and we zoom into confidence \(c=0.7\), see image above. Let’s assume we have 10 inputs here whose most confident prediction (<em>max</em>) equals \(0.7\). If the model correctly classifies 7 out of 10 predictions (<em>true</em>), it is considered calibrated at confidence level \(0.7\). For the model to be fully calibrated this has to hold across all confidence levels from 0 to 1. At the same level \(c=0.7\), a model would be considered miscalibrated if it makes only 4 correct predictions.</p> <h2 id="evaluating-calibration-expected-calibration-error-ece">Evaluating Calibration - Expected Calibration Error (ECE)</h2> <p>One widely used evaluation measure for confidence calibration is the Expected Calibration Error (ECE) <d-cite key="naeini2015obtaining, guo2017calibration"></d-cite>. ECE measures how well a model’s estimated probabilities match the observed probabilities by taking a weighted average over the absolute difference between average accuracy (<em>acc</em>) and average confidence (<em>conf</em>). The measure involves splitting all \(n\) datapoints into M equally spaced bins:</p> \[ECE = \sum_{m=1}^M \frac{\mathopen| B_m \mathclose|}{n} \mathopen| acc(B_m) - conf(B_m) \mathclose| ,\] <p>where \(B\) is used for representing “bins” and \(m\) for the bin number, while <em>acc</em> and <em>conf</em> are:</p> \[\small{ acc(B_m) = \frac{1}{ \mathopen| B_m \mathclose|} \sum_{i\in B_m} \mathbb{1} (\hat{y}_i = y_i ) \;\: \text{&amp;} \;\: conf(B_m) = \frac{1}{ \mathopen| B_m \mathclose|} \sum_{i\in B_m} \hat{p}(x_i) }\] <p>\(\hat{y}_i\) is the model’s predicted class (<em>arg max</em>) for sample \(i\) and \(y_i\) is the true label for sample \(i\). \(\mathbb{1}\) is an indicator function, meaning when the predicted label \(\hat{y}_i\) equals the true label \(y_i\) it evaluates to 1, otherwise 0. Let’s look at an example, which will clarify <em>acc</em>, <em>conf</em> and the whole binning approach in a visual step-by-step manner.</p> <h3 id="ece---visual-step-by-stepexample">ECE - Visual Step by Step Example</h3> <p>In the image below, we can see that we have \(9\) samples indexed by \(i\) with estimated probabilities \(\hat{p}(x_i)\) (simplified as \(\hat{p}_i\)) for class <strong>cat (C)</strong>, <strong>dog (D)</strong> or <strong>toad (T)</strong>. The final column shows the true class \({y}_i\) and the penultimate column contains the predicted class \(\hat{y}_i\).</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f4_ece_table1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f4_ece_table1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f4_ece_table1-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f4_ece_table1.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 1 | ECE - Toy Example </div> <p>Only the maximum probabilities, which determine the predicted label are used in ECE <d-cite key="guo2017calibration"></d-cite>. Therefore, we will only bin samples based on the maximum probability across classes (<em>see left table in below image</em>). To keep the example simple we split the data into 5 <strong>equally spaced</strong> bins \(M=5\). If we now look at each sample’s maximum estimated probability, we can group it into one of the 5 bins (<em>see right side of image below</em>).</p> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f5_ece_table2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f5_ece_table2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f5_ece_table2-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f5_ece_table2.png" class="img-fluid rounded " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Table 2 &amp; Binning Diagram </div> <p>We still need to determine if the predicted class is correct or not to be able to determine the average accuracy per bin. If the model predicts the class correctly (i.e. \(y_i =\hat{y}_i\)), the prediction is highlighted in green; incorrect predictions are marked in red:</p> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f6_ece_table3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f6_ece_table3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f6_ece_table3-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f6_ece_table3.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Table 3 &amp; Binning Diagram </div> <p>We now have visualised all the information needed for ECE and will briefly run through how to calculate the values for bin 5 (\(B_5\)). The other bins then simply follow the same process, see below.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f7_ece_table4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f7_ece_table4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f7_ece_table4-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f7_ece_table4.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Table 4 &amp; Example for bin 5 </div> <p>We can get the empirical probability of a sample falling into \(B_5\) , by assessing how many out of all \(9\) samples fall into \(B_5\), see \(\mathbf{(\;1\;)}\). We then get the average accuracy for \(B_5\), see \(\mathbf{(\;2\;)}\) and lastly the average estimated probability for \(B_5\), see \(\mathbf{(\;3\;)}\). Repeat this for all bins and in our small example of \(9\) samples we end up with an ECE of \(0.10445\). A perfectly calibrated model would have an ECE of 0.</p> <h4 id="expected-calibration-error-drawbacks">Expected Calibration Error Drawbacks</h4> <p>The images of binning above provide a visual guide of how ECE could result in very different values if we used more bins or perhaps binned the same number of items instead of using equal bin widths. Such and more drawbacks of ECE have been highlighted by several works early on <d-cite key="kumar2018trainable, nixon2019measuring, gupta2020calibration, zhang2020mix, roelofs2022mitigating, vaicenavicius2019evaluating, widmann2019calibration, futami2024informationtheoretic"></d-cite>. However, despite the known weaknesses ECE is still widely used to evaluate confidence calibration in ML <d-cite key="xiong2023can, yuan2024does, collins2023human, si2023prompting, mukhoti2023deep, gao2024spuq"></d-cite>. This motivated this blogpost, with the idea to highlight the most frequently mentioned drawbacks of ECE visually and to provide a simple clarification on the development of different notions of calibration.</p> <h2 id="most-frequently-mentioned-drawbacks-ofece">Most frequently mentioned Drawbacks of ECE</h2> <h3 id="pathologies---low-ece--highaccuracy">Pathologies - Low ECE ≠ high accuracy</h3> <p>A model which minimises ECE, does not necessarily have a high accuracy <d-cite key="kumar2018trainable, kull2019beyond, si2022re"></d-cite>. For instance, if a model always predicts the majority class with that class’s average prevalence as the probability, it will have an ECE of 0. This is visualised in the image above, where we have a dataset with 10 samples, 7 of those are cat, 2 dog and only one is a toad. Now if the model always predicts cat with on average 0.7 confidence it would have an ECE of 0. There are more of such pathologies <d-cite key="nixon2019measuring"></d-cite>. To not only rely on ECE, some researchers use additional measures such as the Brier score or LogLoss alongside ECE <d-cite key="kumar2018trainable, kull2019beyond"></d-cite>.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f8_pathologies-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f8_pathologies-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f8_pathologies-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f8_pathologies.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 4 | Pathologies Example </div> <h3 id="binning-approach">Binning Approach</h3> <p>One of the most frequently mentioned issues with ECE is its sensitivity to the change in binning <d-cite key="kumar2018trainable, nixon2019measuring, gupta2020calibration, zhang2020mix, roelofs2022mitigating, famiglini2023towards"></d-cite>. This is sometimes referred to as the <strong><em>Bias-Variance trade-off</em></strong> <d-cite key="nixon2019measuring, zhang2020mix"></d-cite>: Fewer bins reduce variance but increase bias, while more bins lead to sparsely populated bins increasing variance. If we look back to our ECE example with 9 samples and change the bins from 5 to 10 here too, we end up with the following:</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f9_binning_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f9_binning_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f9_binning_1-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f9_binning_1.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 5 | More Bins </div> <p>We can see that bin <em>8</em> and <em>9</em> each contain only a single sample and also that half the bins now contain no samples. The above is only a toy example, however since modern models tend to have higher confidence values samples often end up in the last few bins <d-cite key="naeini2015obtaining, zhang2020mix"></d-cite>, which means they get all the weight in ECE, while the average error for the empty bins contributes 0 to ECE.</p> <p>To mitigate these issues of fixed bin widths some authors <d-cite key="nixon2019measuring, roelofs2022mitigating"></d-cite> have proposed a more adaptive binning approach.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f10_binning_2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f10_binning_2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f10_binning_2-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f10_binning_2.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 6 | Adaptive Bins </div> <p>Binning-based evaluation with bins containing an equal number of samples are shown to have <em>lower bias</em> than a fixed binning approach such as ECE <d-cite key="roelofs2022mitigating"></d-cite>. This leads <d-cite key="roelofs2022mitigating"></d-cite> to urge against using equal width binning and to suggest the use of an alternative: ECEsweep, which maximizes the number of equal-mass bins while ensuring the calibration function remains monotonic <d-cite key="roelofs2022mitigating"></d-cite>. The Adaptive Calibration Error (ACE) and Threshold Adaptive calibration Error (TACE) are two other variations of ECE that use flexible binning <d-cite key="nixon2019measuring"></d-cite>. However, some find it sensitive to the choice of bins and thresholds, leading to inconsistencies in ranking different models <d-cite key="ashukha2020pitfalls"></d-cite>. Two other approaches aim to eliminate binning altogether: MacroCE does this by averaging over instance-level calibration errors of correct and wrong predictions <d-cite key="si2022re"></d-cite> and the KDE-based ECE does so by replacing the bins with non-parametric density estimators, specifically kernel density estimation (KDE) <d-cite key="zhang2020mix"></d-cite>.</p> <h3 id="only-maximum-probabilities-considered">Only maximum probabilities considered</h3> <p>Another frequently mentioned drawback of ECE is that it only considers the maximum estimated probabilities <d-cite key="nixon2019measuring, ashukha2020pitfalls, vaicenavicius2019evaluating, widmann2019calibration, kull2019beyond"></d-cite>. The idea that more than just the maximum confidence should be calibrated, is best illustrated with a simple example <d-cite key="vaicenavicius2019evaluating"></d-cite>:</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f11_max_probs_only_updated-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f11_max_probs_only_updated-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f11_max_probs_only_updated-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f11_max_probs_only_updated.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 7 | input example sourced from <d-cite key="schwirten2024ambiguous"></d-cite> </div> <p>Let’s say we trained two different models and now both need to determine if the same input image contains a <em>person</em>, an <em>animal</em> or <em>no creature</em>. The two models output vectors with slightly different estimated probabilities, but both have the same maximum confidence for “<em>no creature</em>”. Since ECE only looks at these top values it would consider these two outputs to be the same. Yet, when we think of real-world applications we might want our self-driving car to act differently in one situation over the other <d-cite key="vaicenavicius2019evaluating"></d-cite>. This restriction to the maximum confidence prompted various authors <d-cite key="vaicenavicius2019evaluating, kull2019beyond, widmann2019calibration"></d-cite> to reconsider the definition of calibration. The existing concept of calibration as “confidence calibration” (coined in <d-cite key="kull2019beyond"></d-cite>) makes a distinction between two additional interpretations of confidence: <strong>multi-class</strong> and <strong>class-wise calibration</strong>.</p> <p><br/></p> <h4 id="multi-class-calibration">Multi-class Calibration</h4> <p>A model is considered multi-class calibrated if, for any prediction vector \(q=(q_1,...,q_K) \in \Delta^K\)​, the class proportions among all values of \(X\) for which a model outputs the same prediction \(\hat{p}(X)=q\) match the values in the prediction vector \(q\).</p> \[\mathbb{P} (Y = k \; | \; \hat{p}(X)=q ) = q_k \;\;\;\:\:\:\: \forall k \in \{1,...,K\}, \; \forall q \in \Delta^K\] <div class="caption"> where $(X,Y)$ is a datapoint and $\hat{p} : \mathscr{X} \rightarrow \Delta^K$ returns a probability vector as its output </div> <p>What does this mean in simple terms? Instead of \(c\) we now calibrate against a vector \(q\), with K classes. Let’s look at an example below:</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f12_multi-class-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f12_multi-class-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f12_multi-class-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f12_multi-class.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 8 | Multi-class Calibration </div> <p>On the left we have the space of all possible prediction vectors. Let’s zoom into one such vector that our model predicted and say the model has 10 instances for which it predicted the vector \(q\)=[0.1,0.2,0.7]. Now in order for it to be multi-class calibrated, the distribution of the true (<em>actual</em>) class needs to match the prediction vector \(q\). The image above shows a calibrated example with [0.1,0.2,0.7] and a not calibrated case with [0.1,0.5,0.4].</p> <p><br/></p> <h4 id="class-wise-calibration">Class-wise Calibration</h4> <p>A model is considered class-wise calibrated if, for each class k, all inputs that share an estimated probability \(\hat{p}_k(X)\) align with the true frequency of class k when considered on its own:</p> \[\mathbb{P} (Y = k \; | \; \hat{p}_k(X)= q_k ) = q_k \;\;\;\;\;\; \forall k \in \{1,...,K\}\] <div class="caption"> where $(X,Y)$ is a datapoint; $q \in \Delta^K $ and $\hat{p} : \mathscr{X} \rightarrow \Delta^K$ returns a probability vector as its output </div> <p>Class-wise calibration is a <strong><em>weaker</em></strong> definition than <strong>multi-class</strong> calibration as it considers each class probability in <strong><em>isolation</em></strong> rather than needing the full vector to align. The image below illustrates this by zooming into a probability estimate for class 1 specifically: \(q_1=0.1\). Yet again, we assume we have 10 instances for which the model predicted a probability estimate of 0.1 for class 1. We then look at the true class frequency amongst all classes with \(q_1=0.1\). If the empirical frequency matches \(q_1\) it is calibrated.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f13_class-wise-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f13_class-wise-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f13_class-wise-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f13_class-wise.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 9 | Class-wise Calibration </div> <p>To evaluate such different notions of calibration, some updates are made to ECE to calculate a class-wise error. One idea is to calculate the ECE for each class and then take the average <d-cite key="nixon2019measuring, kull2019beyond"></d-cite>. Another idea is to swap the L1-distance used in ECE with the L2 and use several ECE metrics to more effectively assess the overall level of calibration <d-cite key="famiglini2023towards"></d-cite>. Others, introduce the use of the KS-test for class-wise calibration <d-cite key="gupta2020calibration"></d-cite> and <d-cite key="vaicenavicius2019evaluating"></d-cite> also suggest using statistical hypothesis tests instead of ECE based approaches. And other researchers develop a hypothesis test framework [TCal] to detect whether a model is significantly mis-calibrated <d-cite key="donghwan2023tcal"></d-cite> and <d-cite key="sun2024confidenceintervalell2expected"></d-cite> build on this by developing confidence intervals for the L2-ECE.</p> <p>All the approaches mentioned above <strong>share a key assumption: ground-truth labels are available</strong>. Within this gold-standard mindset a prediction is either true or false. However, annotators might unresolvably and justifiably disagree on the real label <d-cite key="aroyo2015truth, uma2021learning"></d-cite>. Let’s look at a simple example below:</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f14_one_hot-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f14_one_hot-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f14_one_hot-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f14_one_hot.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 10 | One-Hot-Vector </div> <p>We have the same image as in our entry example and can see that the chosen label differs between annotators. A common approach to resolving such issues in the labelling process is to use some form of aggregation <d-cite key="paun2022statistical, artstein2008inter"></d-cite>. Let’s say that in our example the majority vote is selected, so we end up evaluating how well our model is calibrated against such ‘ground truth’. One might think, the image is small and pixelated; of course humans will not be certain about their choice. However, rather than being an exception such disagreements are widespread <d-cite key="aroyo2024dices, sanders2022ambiguous, schwirten2024ambiguous"></d-cite>. So, when there is a lot of human disagreement in a dataset it might not be a good idea to calibrate against an aggregated ‘gold’ label <d-cite key="baan2022stop"></d-cite>. Instead of gold labels more and more researchers are using soft or smooth labels which are more representative of the human uncertainty <d-cite key="peterson2019cifar, sanders2022ambiguous, collins2023human"></d-cite>, see example below.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f15_soft_label-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f15_soft_label-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f15_soft_label-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f15_soft_label.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 11 | Soft-Label </div> <p>In the same example as above, instead of aggregating the annotator votes we could simply use their frequencies to create a distribution \(P_{vote}\) over the labels instead, which is then our new \(y_i\). This shift towards training models on collective annotator views, rather than relying on a single source-of-truth motivates another definition of calibration: calibrating the model against human uncertainty <d-cite key="baan2022stop"></d-cite>.</p> <p><br/></p> <h4 id="human-uncertainty-calibration">Human Uncertainty Calibration</h4> <p>A model is considered human-uncertainty calibrated if, for each specific sample \(x\), the predicted probability for each class k matches the ‘<em>actual</em>’ probability \(P_{vote}\) of that class being correct.</p> \[\mathbb{P}_{vote} (Y = k \; | \; X = x ) = \hat{p}_k(x) \;\;\;\;\; \forall k \in \{1,...,K\}\] <div class="caption"> where $(X,Y)$ is a datapoint and $\hat{p} : \mathscr{X} \rightarrow \Delta^K$ returns a probability vector as its output </div> <p>This interpretation of calibration aligns the model’s prediction with human uncertainty, which means each prediction made by the model is individually reliable and matches human-level uncertainty for that instance. Let’s have a look at an example below:</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f16_human_uncertainty-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f16_human_uncertainty-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f16_human_uncertainty-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f16_human_uncertainty.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 12 | Human Uncertainty Calibration </div> <p>We have our sample data (<em>left</em>) and zoom into a single sample \(x\) with index \(i=1\). The model’s predicted probability vector for this sample is [0.1,0.2,0.7]. If the human labelled distribution \(y_i\) matches this predicted vector then this sample is considered calibrated.</p> <p>This definition of calibration is more granular and strict than the previous ones as it applies directly at the level of individual predictions rather than being averaged or assessed over a set of samples. It also relies heavily on having an accurate estimate of the human judgement distribution, which requires a large number of annotations per item. Datasets with such properties of annotations are gradually becoming more available <d-cite key="aroyo2024dices, nie2020learn"></d-cite>.</p> <p>To evaluate human uncertainty calibration three new measures are introduced in <d-cite key="baan2022stop"></d-cite> : <strong>the Human Entropy Calibration Error <em>(EntCE)</em>, the Human Ranking Calibration Score <em>(RankCS)</em> and the Human Distribution Calibration Error <em>(DistCE)</em></strong>.</p> \[EntCE(x_i)= H(y_i) - H(\hat{p}_i),\] <p>where \(H(.)\) signifies entropy.</p> <p><strong>EntCE</strong> aims to capture the agreement between the model’s uncertainty \(H(\hat{p}_i)\) and the human uncertainty \(H(y_i)\) for a sample \(i\). However, entropy is invariant to the permutations of the probability values; in other words it doesn’t change when you rearrange the probability values. This is visualised in the image below:</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f17_entce-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f17_entce-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f17_entce-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f17_entce.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 13 | EntCE drawbacks </div> <p>On the left, we can see the human label distribution \(y_i\) , on the right are two different model predictions for that same sample. All three distributions would have the same entropy, so comparing them would result in 0 EntCE. While this is not ideal for comparing distributions, entropy is still helpful in assessing the noise level of label distributions.</p> <p>\(RankCS = \frac{1}{N} \sum_{n=1}^{N} \mathbf{1} (argsort(y_i) = argsort(\hat{p}_i)),\) <br/> </p> <p>where argsort simply returns the indices that would sort an array.</p> <p>So, <strong>RankCS</strong> checks if the sorted order of estimated probabilities \(\hat{p}_i\) matches the sorted order of \(y_i\) for each sample. If they match for a particular sample \(i\) one can count it as 1; if not, it can be counted as 0, which is then used to average over all samples N. <d-footnote>In the paper it is stated more generally: If the argsorts match, it means the ranking is aligned, contributing to the overall RankCS score.</d-footnote></p> <p>Since this approach uses ranking it doesn’t care about the actual size of the probability values. The two predictions below, while not the same in class probabilies would have the same ranking. This is helpful in assessing the overall ranking capability of models and looks beyond just the maximum confidence. At the same time though, it doesn’t fully capture human uncertainty calibration as it ignores the actual probability values.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f18_rankcs-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f18_rankcs-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f18_rankcs-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/f18_rankcs.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 14 | RankCS drawbacks </div> \[DistCE(x_i) = \mathbf{TVD}(y_i, \hat{p}_i)\] <p><strong>DistCE</strong> has been proposed as an additional evaluation for this notion of calibration. It simply uses the total variation distance \((TVD)\) between the two distributions, which aims to reflect how much they diverge from one another. <em>DistCE</em> and <em>EntCE</em> capture instance level information. So to get a feeling for the full dataset one can simply take the average expected value over the absolute value of each measure: \(E[\mid DistCE \mid]\) and \(E[\mid EntCE \mid]\). Perhaps future efforts will introduce further measures that combine the benefits of ranking and noise estimation for this notion of calibration.</p> <hr/> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/calibration_overview-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/calibration_overview-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/calibration_overview-1400.webp"/> <img src="/2025/assets/img/2025-04-28-calibration/calibration_overview.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image 15 | Visual Summary: Definitions at a Glance </div> <h2 id="final-thoughts">Final Thoughts</h2> <p>We have run through the most common definition of calibration, the shortcomings of ECE and additional notions of calibration: multi-class, class-wise &amp; human-uncertainty calibration. We also touched on some of the newly proposed evaluation measures and their shortcomings. Despite several works arguing against the use of ECE for evaluating calibration, it remains widely used. The aim of this blogpost is to draw attention to these works and their alternative approaches. Determining which notion of calibration best fits a specific context and how to evaluate it should avoid misleading results. Maybe, however, ECE is simply so easy, intuitive and just good enough for most applications that it is here to stay?</p>]]></content><author><name>Maja Pavlovic</name></author><summary type="html"><![CDATA[To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.]]></summary></entry><entry><title type="html">A Visual Dive into Conditional Flow Matching</title><link href="https://starlight345.github.io/2025/blog/conditional-flow-matching/" rel="alternate" type="text/html" title="A Visual Dive into Conditional Flow Matching"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://starlight345.github.io/2025/blog/conditional-flow-matching</id><content type="html" xml:base="https://starlight345.github.io/2025/blog/conditional-flow-matching/"><![CDATA[ <div class="preamble"> $$ \def\partialt#1{\frac{\partial #1}{\partial t}} \def\|{|} \def\p{p(x | t)} \def\u{u(x, t)} \def\utheta{u_{\theta}(x, t)} \def\uthetacfm{u_{\theta}^{\mathrm{CFM}}(x, t)} \def\pcond{p(x | t, z)} \def\ucond{u^{\mathrm{cond}}(x, t, z)} \def\ucondzi{u^{\mathrm{cond}}(x, t, z^{(i)})} \def\wcond{p^{|x,t}(z)} \def\pcondi{p(x | z_i, t)} \def\ucondi{u^{\mathrm{cond}}(x, t, z_i)} \def\E#1#2{\mathbb{E}_{#1} #2} \def\Ebracket#1#2{\mathbb{E}_{#1} \left[ #2 \right]} \def\ucondcustom#1{u^{\mathrm{cond}}(#1)} \def\pdata{p_{\mathrm{data}} } \newcommand{\cL}{\mathcal{L}} \def{\cLL}{\mathcal{L}} \newcommand{\cN}{\mathcal{N}} \newcommand{\cC}{\mathcal{C}} \newcommand{\cO}{\mathcal{O}} \newcommand{\bbE}{\mathbb{E}} \newcommand{\bbN}{\mathbb{N}} \newcommand{\KL}{\mathrm{KL}} $$ </div> <p>All authors contributed equally to this work.</p> <p>The first part of the blog post is an introduction to generative modelling, normalizing flows and continuous normalizing flows. The reader already familiar with these topics, or that wants to cover them later, can directly jump to the <a href="#conditional-flow-matching">second part</a>, devoted to <strong>conditional flow matching</strong>.</p> <h2 id="introduction-to-generative-modelling-with-normalizing-flows">Introduction to Generative Modelling with Normalizing Flows</h2> <p>In a nutshell, the task of generative modelling consists in learning how to sample from a distribution \(p_{\mathrm{data}}\) given a finite number of samples \(x^{(1)}, \ldots, x^{(n)} \in \mathbb{R}^d\) drawn from \(p_{\mathrm{data}}\). It comes with three main challenges – the so-called “Generative learning trilemma” <d-cite key="xiao2021tackling"></d-cite>:</p> <ul> <li>enforce fast sampling</li> <li>generate high quality samples</li> <li>properly cover the diversity of \(p_{\mathrm{data}}\)</li> </ul> <p>One may also add to this wishlist that the model should be easy to train.</p> <p>The modern approach to generative modelling consists in picking a simple <em>base distribution</em> \(p_0\), typically an isotropic Gaussian \(\mathcal{N}(0, \mathrm{Id}_d)\), and learning a map \(T: \mathbb{R}^d \to \mathbb{R}^d\) such that when \(x\) follows \(p_0\) <em>(i.e., \(x \sim p_0\))</em>, the distribution of \(T(x)\) is as close as possible to \(p_{\mathrm{data}}\)<d-footnote>In all this post, by abuse of language we may use "distribution" when referring to densities; all densities are assumed strictly positive everywhere so that Kullback-Leibler divergences and logarithms are well defined.</d-footnote>.</p> <p>When \(x \sim p_0\), the distribution of \(T(x)\) is denoted as \(T\#p_0\), and called the <em>pushforward</em> of \(p_0\) by \(T\)<d-footnote><span>The pushforward of the measure \(\mu\) by the map \(T\), \(T\#\mu\), is defined as \(T\#\mu(A) = \mu(T^{-1}(A))\) for all \(A\subset \mathbb{R}^d\). If the random variable \(x\) has law \(\mu\), the random variable \(T(x)\) has law \(T\#\mu\).</span></d-footnote>. Once the map \(T\) is learned, one can simply sample \(x\) from \(p_0\) and use \(T(x)\) as a generated sample from \(p_\mathrm{data}\).</p> <p>Two intertwined questions arise: what kind of map \(T\) to use, and how to learn it? A natural idea is to define \(T\) as a parametric map \(T_\theta\), typically a neural network, and to learn the optimal parameters \(\theta^*\) by maximizing the log-likelihood of the available samples<d-footnote>Note there also exist generative methods based on other principles, e.g. GANs, that are not covered in this blogpost.</d-footnote>:</p> \[\begin{equation}\label{eq:log_lik} \theta^* = \mathop{\mathrm{argmax}}_\theta \sum_{i=1}^n \log \left( (T_\theta \# p_0)(x^{(i)}) \right) \,. \end{equation}\] <p>Approximately, maximizing the log-likelihood in \eqref{eq:log_lik} corresponds to making \(p_{\mathrm{data}}\) and \(T_\theta\#p_0\) close in the sense of the Kullback-Leibler divergence<d-footnote><span> Indeed \(\begin{aligned} \mathop{\mathrm{KL}(p_{\mathrm{data}}, T_\theta\#p_0)} &amp; \overset{\mathrm{def}}{=} \int_x \log \left(\frac{p_{\mathrm{data}}(x)}{T_\theta\#p_0(x)}\right) p_{\mathrm{data}}(x) \, \mathrm{d}x \\ &amp; = \int_x \log (p_{\mathrm{data}}(x)) p_{\mathrm{data}}(x) \, \mathrm{d}x - \int_x \log (T_\theta\#p_0(x)) p_{\mathrm{data}}(x) \, \mathrm{d}x \ . \end{aligned}\) When minimizing with respect to \(\theta\) the first term of the right hand side is constant and can be ignored. The second term is simply \(-\mathbb{E}_{x \sim p_{\mathrm{data}}} [\log T_\theta\#p_0(x)]\): approximating it by an empirical mean using \(x^{(1)}, \ldots, x^{(n)}\) yields the objective in \eqref{eq:log_lik}.</span></d-footnote>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-conditional-flow-matching/T_theta_pushforward.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-conditional-flow-matching/T_theta_pushforward.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-conditional-flow-matching/T_theta_pushforward.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-conditional-flow-matching/T_theta_pushforward.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Modern generative modelling principle: trying to find a map \(T\) that sends the base distribution \(p_0\) as close as possible to the data distribution \(p_{\mathrm{data}}\).</figcaption> </figure> <h3 id="normalizing-flows">Normalizing Flows</h3> <p>In order to compute the log-likelihood objective function in \eqref{eq:log_lik}, if \(T_\theta\) is a diffeomorphism (and thus has a differentiable inverse \(T_\theta^{-1}\)), one can rely on the so-called <em>change-of-variable formula</em></p> \[\begin{equation}\label{eq:change_variable} \log p_1(x) = \log p_0(T_\theta^{-1}(x)) + \log |\det J_{T_\theta^{-1}}(x)| \end{equation}\] <p>where \(J_{T_\theta^{-1}}\in \mathbb{R}^{d\times d}\) is the Jacobian of \(T^{-1}_{\theta}\). Relying on this formula to evaluate the likelihood imposes two constraints on the network:</p> <ul> <li>\(T_\theta\) must be invertible; in addition \(T_{\theta}^{-1}\) should be easy to compute in order to evaluate the first right-hand side term in \eqref{eq:change_variable}</li> <li>\(T_\theta^{-1}\) must be differentiable, and the (log) determinant of the Jacobian of \(T_\theta^{-1}\) must not be too costly to compute in order to evaluate the second right-hand side term in \eqref{eq:change_variable}<d-footnote><span>Equivalently, both \(T^{-1}_\theta\) and the determinant of \(J_{T_\theta}\) must be easy to compute, since \(J_{T_\theta^{-1}}(x) = (J_{T_\theta}(T_\theta^{-1}(x)))^{-1}\) and \(\log|\det J_{T_\theta^{-1}}(x) | = - \log | \det J_{T_\theta}(T_\theta^{-1}(x))|\).</span></d-footnote>.</li> </ul> <p>The philosophy of Normalizing Flows (NFs) <d-cite key="tabak2013family,rezende2015variational,papamakarios2021normalizing"></d-cite> is to design special neural architectures satisfying these two requirements. Normalizing flows are maps \(T_\theta = \phi_K \circ \ldots \phi_1\), where each \(\phi_k\) is a simple transformation satisfying the two constraints – and hence so does \(T_\theta\). Let \(x_0 \sim p_0\) and \(x_{k} = \phi_k(x_{k-1})\) for \(k\in\{1, \ldots, K\}\), the chain rule yields the following formula for the log-likelihood </p> \[\begin{align*} \log p_1(x_K) &amp;= \log p_0(\phi_1^{-1} \circ \ldots \circ \phi_K^{-1} (x_K)) + \log |\det J_{\phi_1^{-1} \circ \ldots \circ \phi_K^{-1}}(x_K)| \\ &amp;= \log p_0(\phi_1^{-1} \circ \ldots \circ \phi_K^{-1} (x_K)) + \sum_{k=1}^{K} \log | \det J_{\phi^{-1}_k}(x_{k}) | \end{align*}\] <p>which is still easy to evaluate provided each \(\phi_k\) satisfies the two constraints.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-conditional-flow-matching/normalizing_flow_cross-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-conditional-flow-matching/normalizing_flow_cross-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-conditional-flow-matching/normalizing_flow_cross-1400.webp"/> <img src="/2025/assets/img/2025-04-28-conditional-flow-matching/normalizing_flow_cross.png" class="img-fluid invert" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Normalizing flow with \(K=4\), transforming an isotropic Gaussian (leftmost) to a cross shape target distribution (rightmost). Picture from <d-cite key="papamakarios2021normalizing"/></figcaption> </figure> <p>For example, one of the earliest instances of NF is the planar flow, which uses as building blocks</p> \[\phi_k(x) = x + \sigma(b_k^\top x + c) a_k\] <p>with \(a_k, b_k \in \mathbb{R}^d\), \(c \in \mathbb{R}\), and \(\sigma :\mathbb{R} \to \mathbb{R}\) a non linearity applied pointwise.<d-footnote>The Jacobian of a planar flow block is \(J_{\phi_k}(x)= \mathrm{Id}_d + \sigma'(b_k^\top x + c) a_k b_k^\top\) whose determinant can be computed in \(\mathcal{O}(d)\) through the matrix determinant lemma \(\det (\mathrm{Id} + x y^\top) = 1 + x^\top y\). However, \(\phi_k^{-1}\) does not admit an analytical expression, and one must resort to iterative algorithms such as Newton's method to approximate it. </d-footnote></p> <p>A more complex example of NF, that satisfies both constraints, is Real NVP <d-cite key="dinh2017density"></d-cite>.</p> <details> <summary>Click here for details about Real NVP</summary> <div> \[\begin{equation}\label{eq:realnvp} \begin{aligned} \phi(x)_{1:d'} &amp;= x_{1:d'}\\ \phi(x)_{d':d} &amp;= x_{d':d} \odot \exp(s(x_{1:d'})) + t(x_{1:d'}) \end{aligned} \end{equation}\] <p>where \(d' \leq d\) and the so-called <em>scale</em> \(s\) and <em>translation</em> \(t\) are functions from \(\mathbb{R}^{d'}\) to \(\mathbb{R}^{d-d'}\), parametrized by neural networks. This transformation is invertible in closed-form, and the determinant of its Jacobian is cheap to compute.</p> <p>The Jacobian of \(\phi\) defined in \eqref{eq:realnvp} is lower triangular:</p> \[J_{\phi}(x) = \begin{pmatrix} \mathrm{Id}_{d'} &amp; 0_{d',d -d'} \\ ... &amp; \mathrm{diag}(\exp(s(x_{1:d}))) \end{pmatrix}\] <p>hence its determinant can be computed at a low cost, and in particular without computing the Jacobians of \(s\) and \(t\). In addition, \(\phi\) is easily invertible:</p> <p>\(\begin{align*} \phi^{-1}(y)_{1:d'} &amp;= y_{1:d'} \\ \phi^{-1}(y)_{d':d} &amp;= (y_{d':d} - t(y_{1:d'})) \odot \exp(- s(y_{1:d'})) \end{align*}\)</p> </div> </details> <p><br/> It has met with a huge success in practice and a variety of alternative NFs have been proposed<d-cite key="tomczak2016improving,kingma2016improved,van2018sylvester"></d-cite>. Unfortunately, the architectural constraints on Normalizing Flows tends to hinder their expressivity<d-footnote>Alternative solutions exist, for example relying on invertible ResNets <d-cite key="behrmann2019invertible"></d-cite> or the recently proposed free form normalizing flows <d-cite key="draxler2024free"></d-cite>, that are out of scope for this blog post.</d-footnote>.</p> <h3 id="continuous-normalizing-flows">Continuous Normalizing Flows</h3> <p>A successful solution to this expressivity problem is based on an idea similar to that of ResNets, named Continuous Normalizing Flows (CNF) <d-cite key="chen2018neural"></d-cite>. If we return to the planar normalizing flow, by letting \(u_{k-1}(\cdot) \overset{\mathrm{def}}{=} K\sigma(b_k^\top \cdot + c) a_k\), we can rewrite the relationship between \(x_{k}\) and \(x_{k-1}\) as:</p> <figure class="sidebar" style="--w: 200; --h: 320;"> <iframe style="--h: 200" src="/2025/assets/html/2025-04-28-conditional-flow-matching/ot-flow-1d.html#loop1" frameborder="0" scrolling="no"></iframe> <figcaption class="caption">From a direct mapping, to finer and finer time discretizations, to a continuous-time mapping.</figcaption> </figure> \[\begin{align*} x_k &amp;= \phi_k(x_{k-1}) \\ &amp;= x_{k-1} + \sigma(b_k^\top x_{k-1} + c) a_k \\ &amp;= x_{k-1} + \frac{1}{K} u_{k-1}(x_{k-1}) \\ \end{align*}\] <p>which can be interpreted as a forward Euler discretization, with step \(1/K\), of the ODE</p> <p>\(\begin{equation}\label{eq:initial_value_pb} \begin{cases} x(0) = x_0 \\ \partial_t x(t) = u(x(t), t) \quad \forall t \in [0, 1] \end{cases} \end{equation}\) Note that the mapping defined by the ODE, \(T(x_0):= x(1)\) is inherently invertible because one can solve the <em>reverse-time</em> ODE (from \(t=1\) to \(0\)) with the initial condition \(x(1)=T(x_0)\).</p> <p>This ODE is called an <em>initial value problem</em>, controlled by the <strong>velocity field</strong> \(u: \mathbb{R}^{d} \times [0, 1] \to \mathbb{R}^d\). In addition to \(u\), it is related to two other fundamental objects:</p> <ul> <li>the <strong>flow</strong> \(f^u: \mathbb{R}^d \times[0, 1] \to \mathbb{R}^d\), with \(f^u(x, t)\) defined as the solution at time \(t\) to the initial value problem driven by \(u\) with initial condition \(x(0) = x\). </li> <li>the <strong>probability path</strong> \((p_t)_{t\in[0,1]}\), defined by \(p_t = f^u(\cdot, t)\# p_0\), i.e., \(p_t\) is the distribution of \(f^u(x, t)\) when \(x \sim p_0\).</li> </ul> <p>A fundamental equation linking \(p_t\) and \(u\) is the <em>continuity equation</em>, also called transport equation:</p> \[\begin{equation}\label{eq:continuity_eq} \partial_t p_t + \nabla\cdot u_t p_t = 0 \end{equation}\] <p>Under technical conditions and up to divergence-free vector fields, for a given \(p_t\) (resp. \(u\)) there exists a \(u\) (resp. \(p_t\)) such that the pair \((p_t, u)\) solves the continuity equation<d-footnote><span>If \((p_t)_t\) is absolutely continuous and \(∣\partial_t p_t∣ \in L^1([0,1])∣\) then there exists a vector field \(u_t\) of finite length such that \((p_t, u)\) satisfies the continuity equation (see <d-cite key="ambrosio2008gradient"></d-cite> Theorem 8.3.1). For a field \(u\) regular enough such that the initial value problem has a unique solution on \([0,1]\), given \(p_0\), then \((p_t :=f^u(t,⋅)\#p_0, u)\) is a solution to the continuity equation (see <d-cite key="ambrosio2008gradient"></d-cite> Lemma 8.1.6). Note however that the correspondance between \(p_t\) and \(u\) is unique only up to divergence free fields. </span></d-footnote>.</p> <figure> <img class="invert" style="height: 250px" src="/2025/assets/img/2025-04-28-conditional-flow-matching/trifecta.svg" frameborder="0" scrolling="no"/> <figcaption class="caption"> Link between the probability path, the velocity field and the flow. </figcaption> </figure> <p>Based on the ODE \eqref{eq:initial_value_pb}, Continuous Normalizing Flows work in the continuous-time domain, and directly model the continuous solution \((x(t))_{t \in [0, 1]}\) instead of a finite number of discretized steps \(x_1, \ldots, x_K\). They do so by learning the velocity field \(u\) as \(u_\theta: \mathbb{R}^d \times [0, 1] \to \mathbb{R}^d\). Sampling is then achieved by solving the initial value problem \eqref{eq:initial_value_pb} with \(x_0\) sampled from the base distribution \(p_0\).</p> <figure class="sidebar" style="--w: 200; --h: 420;"> <video class="invert" style="width: 100%; height: auto; object-fit: cover; border: none;" autoplay="" loop="" muted="" onclick="this.controls = true" src="/2025/assets/img/2025-04-28-conditional-flow-matching/traj.mp4"> </video> </figure> <p>CNFs are a particular case of Neural ODE networks<d-footnote><span>Neural ODE functions are also defined as the solution of an initial value problem like \eqref{eq:initial_value_pb}, but the framework is more general: the loss \(\mathcal{L}\) used for training is arbitrary, and, in order to train \(u_\theta\), the authors provide a way to compute \(\nabla_\theta \mathcal{L}\) by solving an augmented, reversed ODE</span></d-footnote>, with additional tricks to compute the likelihood in order to train them: if \(x(t)\) is the solution of the ODE \eqref{eq:initial_value_pb} with \(u = u_\theta\) , then its log-likelihood \(\log p_t(x(t))\) satisfies the so-called <em>instantaneous change of variable formula</em> <d-cite key="chen2018neural">, derived from the continuity equation:</d-cite></p> \[\begin{equation}\label{eq:ce_logptxt} \frac{\mathrm{d}}{\mathrm{d}t} \log p_t(x(t)) = - \mathrm{tr} J_{u_\theta(\cdot, t)} (x(t)) = - \nabla \cdot u_\theta(\cdot, t)(x(t)) \quad \forall t \in [0, 1] \end{equation}\] <details> <summary>Click here to unroll the proof</summary> <p>The proof relies on the identity</p> \[\nabla\cdot \left( p_t(x) u(t, x) \right) = \langle \nabla p_t(x) , u(x, t) \rangle + p_t(x) \nabla\cdot u(x, t) \, .\] <p>Starting from the continuity equation \eqref{eq:continuity_eq} at any \(t, x\) and dividing it by \(p_t(x)\), we get:</p> \[\begin{align} \frac{1}{p_t(x)} \frac{\mathrm{d} p_t}{\mathrm{d} t}(x) + \frac{1}{p_t(x)} \nabla\cdot(p_t(x) u(x, t)) &amp;= 0 \nonumber \\ \frac{\mathrm{d} \log p_t }{\mathrm{d} t} (x) + \frac{1}{p_t(x)} \langle \nabla p_t(x) , u(x, t) \rangle + \nabla\cdot u(x, t) &amp;= 0 \label{eq:inst_change_pf} \end{align}\] <p>Note that if we plug \(x = x(t)\), the left-hand side is the derivative with respect to \(t\) of \(\log p_t\), evaluated at \(x(t)\). This is different from the derivative with respect to \(t\) of \(t \mapsto \log p_t(x(t))\) – the so-called <em>total derivative</em>, which we now compute:</p> \[\begin{align*} \frac{\mathrm{d} \log p_t(x(t))}{\mathrm{d}t} &amp;= \frac{\mathrm{d} \log p_t }{\mathrm{d} t} (x(t)) + \langle \nabla \log p_t(x(t)), \frac{\mathrm{d}}{\mathrm{d}t} x(t) \rangle \\ &amp;= \frac{\mathrm{d} \log p_t }{\mathrm{d} t} (x(t)) + \langle \frac{1}{p_t(x_t)} \nabla p_t(x(t)), u_\theta(x(t), t) \rangle \\ &amp;= \frac{\mathrm{d} \log p_t }{\mathrm{d} t} (x(t)) + \langle \frac{1}{p_t(x_t)} \nabla p_t(x(t)), u_\theta(x(t), t) \rangle \\ &amp;= - \nabla\cdot u_\theta(x, t) \end{align*}\] <p>using \(\nabla \log p_t(x) = \frac{1}{p_t(x)} \nabla p_t(x)\) and \eqref{eq:inst_change_pf} successively. We conclude by observing that the divergence is equal to the trace of the Jacobian.</p> </details> <p>The ODE \eqref{eq:ce_logptxt} allows evaluating the log-likelihood objective in \eqref{eq:log_lik}, by finding the antecedent by the flow of the data point \(x^{(i)}\) as:</p> \[\begin{equation}\label{eq:inverting_cnf} x(0) = x^{(i)} + \int_1^0 u_\theta(x(t), t) \mathrm{dt} \end{equation}\] <p>(i.e., solving \eqref{eq:initial_value_pb} in reverse), and then<d-footnote><span>Actually, both \(x(0)\) and \(\log p_1(x^{(i)})\) can be computed in one go, by introducing the unknown \(F(t) = \begin{pmatrix} x(t) \\ \log p_t(x(t)) - \log p_1(x(1)) \end{pmatrix}\) and solving the augmented ODE </span> $$ \frac{\mathrm{d}}{\mathrm{d} t} F(t) = \begin{pmatrix} u_\theta(x(t), t) \\ - \nabla\cdot u_\theta(\cdot, t)(x(t)) \end{pmatrix} $$ <span> with initial condition \(F(1) = \begin{pmatrix} x^{(i)} \\ 0 \end{pmatrix}\). Evaluating the solution \(F\) at \(t=0\) gives both \(x(0)\) and \(\log p_0(x(0)) - \log p_1(x^{(i)})\); since \(p_0\) is available in closed form, this yields \(\log p_1(x^{(i)})\).</span></d-footnote> integrating \eqref{eq:ce_logptxt}:</p> \[\log p_1(x^{(i)}) = \log p_0(x(0)) - \int_0^1 \nabla \cdot u_\theta(\cdot, t)(x(t)) \mathrm{dt}\] <p>Finally, computing the gradient of the log-likelihood with respect to the parameters \(\theta\) in \(u_\theta\) is done by solving a reversed and augmented ODE, relying on the adjoint method as in the general Neural ODE framework <d-cite key="grathwohl2018ffjord"></d-cite>.</p> <p>The main benefits of continuous NF are:</p> <ul> <li>the constraints one needs to impose on \(u\) are much less stringent than in the discrete case: for the solution of the ODE to be unique, one only needs \(u\) to be Lipschitz continuous in \(x\) and continuous in \(t\)</li> <li>inverting the flow can be achieved by simply solving the ODE in reverse, starting from \(t=1\) with condition \(x(1) = x^{(i)}\) as in \eqref{eq:inverting_cnf}</li> <li>computing the likelihood does not require inverting the flow, nor to compute a log determinant; only the trace of the Jacobian is required, that can be approximated using the Hutchinson trick<d-footnote><span>The Hutchinson trick is \(\mathop{\mathrm{tr}} A = \mathbb{E}_\varepsilon[\varepsilon^t A \varepsilon]\) for any random variable \(\varepsilon \in \mathbb{R}^d\) having centered, independent components of variance 1. In practice, the expectation is approximated by Monte-Carlo sampling, usually using only one realization of \(\varepsilon\)</span></d-footnote>.</li> </ul> <p>However, training a neural ODE with log-likelihood does not scale well to high-dimensional spaces, and the process tends to be unstable, likely due to numerical approximations and to the (infinite) number of possible probability paths. In contrast, the flow-matching framework, which we now describe, explicitly targets a specific probability path during training. It is a likelihood-free approach, that does not require solving ODE – being hence coined a <em>simulation-free</em> method.</p> <h2 id="conditional-flow-matching">Conditional Flow Matching</h2> <p>This part presents <strong>Conditional Flow Matching</strong> (CFM). While the first part gives interesting background on normalizing flows, it is not a strict requirement to understand the principle of CFM.</p> <p>Before giving the goal and intuition of CFM, we summarize the main concepts and visual representation used in this blog post.</p> <figure> <div class="l-page" style="--ar: calc(218 / 161)"> <iframe style="aspect-ratio: 1; width: calc(100% / ( 1 + 2 * var(--ar)));" src="/2025/assets/html/2025-04-28-conditional-flow-matching/ot-flow-1d.html#loop9" frameborder="0" scrolling="no"></iframe> <img style="vertical-align: top; width: calc(100% * var(--ar) / ( 1 + 2 * var(--ar)));" src="/2025/assets/img/2025-04-28-conditional-flow-matching/pbackground.svg"/> <iframe style="aspect-ratio: var(--ar); margin: 0 -5px; width: calc(100% * var(--ar) / ( 1 + 2 * var(--ar)));" src="/2025/assets/html/2025-04-28-conditional-flow-matching/u-anim.html" frameborder="0" scrolling="no"></iframe> </div> <figcaption class="caption"> <p>Three types of visuals used in this blog post.</p> </figcaption> </figure> <div class="boxed"> <p>Core concepts and visuals manipulated in this post</p> <p><span class="ref-lastfig">Figure </span> illustrates the key background elements necessary to understand Flow Matching.</p> <ul> <li><strong>(left)</strong> A flow that maps a simple distribution \(p_0\) in blue (typically \(\mathcal{N}(0,1)\)) into the data distribution to be modelled \(\pdata\) in red. The probability path \(\p\) associates to each time \(t\), a distribution (dashed).</li> <li><strong>(center)</strong> The two distributions (in gray) together with a probability path \(\p\) shown as a heatmap. Such a sufficiently regular probability path is governed by a velocity field \(\u\).</li> <li><strong>(right)</strong> The velocity field \(\u\) (shown with arrows and colors) corresponding to the previous probability path. The animation shows samples from \(p_0\) that follow the velocity field. The distribution of these samples corresponds to \(\p\).</li> </ul> </div> <h3 id="intuition-of-conditional-flow-matching">Intuition of Conditional Flow Matching</h3> <figure class="sidebar is-right" style="--w: 200; --h: 300;"> <iframe style="--h: 147" src="/2025/assets/html/2025-04-28-conditional-flow-matching/u-anim.html" frameborder="0" scrolling="no"></iframe> <figcaption class="caption"> Generation of samples from \(p_1\) can be done by sampling from \(p_0\) and then following the velocity field \(\utheta\). </figcaption> </figure> <p><strong>Goal</strong>. <em>Similarly</em> to continuous normalizing flows, the goal of conditional flow matching is to find a velocity field \(\utheta\) that, when followed/integrated, transforms \(p_0\) into \(\pdata\). Once the velocity field is estimated, the data generation process of conditional flow matching and continuous normalizing flows are the same. It is illustrated in <span class="ref-lastfig">Figure </span>. The <em>particularity of CFM</em> is how the velocity field is learned, as we will detail below.</p> <figure class="sidebar" style="--w: 200; --h: 320;"> <iframe style="--h: 200" src="/2025/assets/html/2025-04-28-conditional-flow-matching/ot-flow-1d.html#loop3" frameborder="0" scrolling="no"></iframe> <figcaption class="caption"> An infinite number of probability paths \((p_t)_{t \in [0,1]}\) that transforms \(p_0\) in \(\pdata\). </figcaption> </figure> <p><strong>Intuition</strong>. In order to make the learning of the velocity field \(\utheta\) easier, one would like to get a supervision signal at each time step \(t \in [0,1]\) (and not only at time \(t=1\)). However, as illustrated in <span class="ref-lastfig">Figure </span>, there exists an infinite number of probability paths \(p_t\) (equivalently an infinite number of velocity fields \(\u\)) that tranform \(p_0\) in \(\pdata\). Thus, in order to get supervision for all \(t\), one must <strong>fully specify a probability path/velocity field</strong>.</p> <p><strong>Organization</strong>. In the <a href="#modelling-choices"> Modelling Choices Section </a> we provide details on how CFM fully specifies a probability path \(p_t\) that transforms \(p_0\) into \(\pdata\): this is not trivial since \(\pdata\) is unknown. Then in the <a href="#from-conditional-to-unconditional-velocity"> From Conditional to Unconditional Velocity Section </a> we provide new intuition on how to interpret the corresponding fully specified velocity field \(\u\). Finally, we recall how CFM learns the velocity field \(\u\) in a tractable fashion. </p> <h3 id="modelling-choices">Modelling Choices</h3> <div class="sidebar" style="--w: 200; --h: 150;"> <div class="caption"> <p>We consider \(t\) as a random variable and interchangeably write \(p(x \| t)\) and \(p_t(x)\).</p> </div> </div> <p><strong>How to fully specify a probability path \(p_t\)?</strong> For unknown target data distribution \(\pdata\) it is hard to choose a priori a probability path or velocity field. CFM core idea is to choose a conditioning variable \(z\) and a conditional probability path \(\pcond\) (examples below) such that (1) the induced global probability path \(\p\) transforms \(p_0\) into \(\pdata\), (2) the associated velocity field \(u^\mathrm{cond}\) has an analytic form.</p> <p><strong>Example 1: Linear interpolation <d-cite key="albergo_building_2023,liu_flow_2023"></d-cite></strong></p> <div class="left-lined"> <p>A first choice is to condition on the base points and the target points, i.e., \(z\) is a random variable defined as:</p> \[\begin{align*} z \overset{\mathrm{choice}}{=} (x_0, x_1) \sim p_0 \times p_\mathrm{data} \, . \end{align*}\] <p>Among all the possible probability paths, one can choose to use very concentrated Gaussian distributions and simply interpolate between \(x_0\) and \(x_1\) in straight line: for some fixed standard deviation \(\sigma\), it writes as</p> \[\begin{align*} p \big (x | t, z=(x_0, x_1) \big) \overset{\mathrm{choice}}{=} \mathcal{N}((1 - t) \cdot x_0 + t \cdot x_1, \sigma^2 \mathrm{Id}) \, . \end{align*}\] <p>To recover the correct distributions \(p_0\) at \(t= 0\) (resp. \(p_\mathrm{target}\) at \(t=1\)), one must enforce \(\sigma = 0\), finally leading to</p> \[\begin{align*} p \big (x | t, z=(x_0, x_1) \big) \overset{\mathrm{choice}}{=} \delta_{ (1-t) \cdot x_0 + t \cdot x_1 } (x) \, , \end{align*}\] <p>where \(\delta\) denotes the Dirac delta distribution.</p> <div class="sidebar" style="--w: 200; --h: 220;"> <div class="caption"> <p>\(t\) will always be a uniform random variable between \(0\) and \(1,\) hence \(p(t)=1\), and \(p(x \| t) = \frac{p(x, t)}{p(t)} = p(x, t)\). Similarly \(\pcond = p(x,t|z)\).</p> </div> </div> <figure class=""> <div style="--r: calc(1250 / (1250 + 400)); display: flex; align-items: end; margin-right: -15px; margin-left: -15px;"> <img style="width: calc(100% * var(--r))" src="/2025/assets/img/2025-04-28-conditional-flow-matching/a/a_pcondxt_representers.svg"/> <video style="width: calc(100% * (1 - var(--r)));border-left:1px solid black" autoplay="" loop="" onclick="this.controls = true" src="/2025/assets/img/2025-04-28-conditional-flow-matching/a/a_accumulate_pcond.mp4" frameborder="0" scrolling="no"></video> </div> <figcaption class="caption"> <p>Conditional probability paths as linear interpolation.<br/> <strong>(left)</strong> \(p(x|t, z=z^{(i)})\) for six samples \(z^{(i)}\) (each being a pair \((x_0,x_1)\)). <strong>(right)</strong> Visualizing the convergence of the empirical average towards \(p(x|t) = \Ebracket{z}{p(x|t,z)} \approx \frac{1}{N} \sum_{i=1}^N p(x|t,z=z^{(i)})\).</p> </figcaption> </figure> <p>Then, one can show that setting</p> \[\ucondcustom{x,t,z = (x_0,x_1)} = x_1 - x_0\] <p>satisfies the continuity equation with \(\pcond\)<d-footnote> In the sense of distributions, one has $$ \begin{align*} &amp; \partial_t p_t (x | t, z) + \nabla \cdot (\ucond p (x|t,z)) \\ =&amp; \langle x_1 - x_0 , \nabla \delta_{ (1-t) \cdot x_0 + t \cdot x_1 }(x) \rangle + \delta_{ (1-t) \cdot x_0 + t \cdot x_1 }(x) \nabla \cdot \ucond + \langle \ucond, \nabla \delta_{ (1-t) \cdot x_0 + t \cdot x_1 }(x) \rangle \end{align*} $$ One can easily identify that \(u(x,t,z) = x_1 - x_0\) which is constant with respect to \(x\) (hence, such that \(\nabla\cdot \ucond = 0 \)) is a suitable solution to the continuity equation.</d-footnote>. Hence, the two choices made – \(z\) and \(p(x |t,z)\) – result in a very easy-to-compute conditional velocity field \(\ucondcustom{x,t,z = (x_0,x_1)}\) which will be later used as a supervision signal to learn \(u_\theta (x,t)\).</p> </div> <p><strong>Example 2: Conical Gaussian paths</strong> <d-cite key="lipman_flow_2023"></d-cite></p> <div class="left-lined"> <p>One can make other choices for the conditioning variable, for instance</p> \[\begin{align*} z \overset{\mathrm{choice}}{=} x_1 \sim \pdata \, , \end{align*}\] <p>and the following choice for the conditional probability path: simply translate and progressively scale down the base normal distribution towards a Dirac delta in \(z\):</p> \[\begin{align*} p(x | t, z=x_1) \overset{\mathrm{choice}}{=} \mathcal{N}(tx_1, (1 - t)^2 \mathrm{Id}) \, . \end{align*}\] <figure class=""> <div style="--r: calc(1250 / (1250 + 400)); display: flex; align-items: end; margin-right: -15px; margin-left: -15px;"> <img style="width: calc(100% * var(--r))" src="/2025/assets/img/2025-04-28-conditional-flow-matching/l/l_pcondxt_representers.svg"/> <video style="width: calc(100% * (1 - var(--r)));border-left:1px solid black" autoplay="" loop="" onclick="this.controls = true" src="/2025/assets/img/2025-04-28-conditional-flow-matching/l/l_accumulate_pcond.mp4" frameborder="0" scrolling="no"></video> </div> <figcaption class="caption"> <p>Conditional probability paths as shrinking conical Gaussians.<br/> <strong>(left)</strong> \(p(x|t, z=z^{(i)})\) for six samples \(z^{(i)}\) (each being a value \(x_1\)). <strong>(right)</strong> Visualizing the convergence of the empirical average towards \(p(x|t) = \Ebracket{z}{p(x|t,z)} \approx \frac{1}{N} \sum_{i=1}^N p(x|t,z=z^{(i)})\).</p> </figcaption> </figure> <p>Then, one can show that setting \(\ucondcustom{x,t,z = x_1} = \frac{x - x_1}{1 - t}\) leads to a couple \((\ucond, \pcond)\) satisfying the continuity equation.</p> </div> <p><strong>General construction of conditional probability paths</strong></p> <div class="left-lined"> <p>To build a conditional probability path, the user must make two modelling choices:</p> <ul> <li>first, a <strong>conditioning variable</strong> \(z\) (independent of \(t\))</li> <li>then, <strong>conditional probability paths</strong><d-footnote> As Albergo and coauthors <d-cite key="albergo_building_2023"></d-cite>, one can construct the conditional probability path by first defining a conditional flow (also called stochastic interpolant) \(f_t^\mathrm{cond}(x, z)\). By pushing \(p_0\), these flows define random variables \(x \vert t, z = f^\mathrm{cond}(x, t, z)\), which have conditional distributions \(p(\cdot \vert z, t) = f^\mathrm{cond}(\cdot, t, z)\#p_0\).</d-footnote> \(\pcond\) that must satisfy the following constraint: marginalizing \(p(x \vert z, t=0)\) (resp. \(p(x \vert z, t=1)\)) over \(z\), yields \(p_0\) (resp. \(\pdata\)). In other words, \(\pcond\) must satisfy</li> </ul> \[\begin{align*} \forall x \enspace &amp; \Ebracket{z}{ p(x \vert z, t=0) } = p_0(x) \enspace, \\ \forall x \enspace &amp; \Ebracket{z}{ p(x \vert z, t=1) } = \pdata(x) \enspace. \end{align*}\] <details> <summary> Click here to check that the previous examples satisfy these constraints.</summary> <ul> <li>Choice 1:</li> </ul> \[\begin{align*} &amp;z = (x_0, x_1) \sim p_0 \times p_\mathrm{target} \\ &amp;\pcond = \delta_{(1 - t) x_0 + t x_1}(x) \end{align*}\] <p>One also easily checks that \(\int_z p(x \vert z, t=0) p(z) \mathrm{d}z = \int_z \delta_{x_0}(x) p(z) \mathrm{d}z = p_0(x)\) and \(\int_z p(x \vert z, t=1) p(z) \mathrm{d}z = \int_z \delta_{x_1}(x) p(z) \mathrm{d}z = p_\mathrm{target}(x)\)</p> <p>Note that the choice \(p \big (x | t, z=(x_0, x_1) \big) = \mathcal{N}((1 - t) \cdot x_0 + t \cdot x_1, \sigma^2)\), \(\sigma&gt; 0\) does not (exactly) respect the constraint \(\Ebracket{z}{ p(x \vert z, t=0)} = p_0(x)\), but it has been sometimes used in the literature.</p> <ul> <li>Choice 2 (valid for a Gaussian \(p_0\) only):</li> </ul> \[\begin{align*} &amp;z = x_1 \sim p_\mathrm{target} \\ &amp;\pcond = \mathcal{N}(t z, (1 - t)^2 \mathrm{Id}) \end{align*}\] <p>One easily checks that \(\int_z p(x \vert z, t=0) p(z) \mathrm{d}z = \int_z p_0(x) p(z) \mathrm{d}z = p_0(x)\) and \(\int_z p(x \vert z, t=1) p(z) \mathrm{d}z = \int_z \delta_z(x) p(z) \mathrm{d}z = p_\mathrm{target}(x)\), so those are admissible conditional paths.</p> </details> </div> <h3 id="from-conditional-to-unconditional-velocity">From Conditional to Unconditional Velocity</h3> <div class="left-lined"> <p>The previous section provided examples on how to choose a conditioning variable \(z\) and a simple conditional probability path \(\pcond\). The marginalization of \(\pcond\) directly yields a (intractable) closed-form formula for the probability path: \(\p = \Ebracket{z}{\pcond}\).</p> </div> <figure class="l"> <div class="grid-custom1" style=""> <span class="" style="grid-column: 1 / span 3">$$p(x,t|z=z^{(i)})$$</span> <span style="grid-column: 5">$$p(x,t)$$</span> <img src="/2025/assets/img/2025-04-28-conditional-flow-matching/a/a_pcond_0.svg"/> <img src="/2025/assets/img/2025-04-28-conditional-flow-matching/a/a_pcond_1.svg"/> <img src="/2025/assets/img/2025-04-28-conditional-flow-matching/a/a_pcond_2.svg"/> <div style="font-size: 3em;">⇒</div> <img src="/2025/assets/img/2025-04-28-conditional-flow-matching/a/a_ptot.svg"/> <span class="" style="grid-column: 1 / span 3">$$\ucondzi$$</span> <span style="grid-column: 5"></span> <img src="/2025/assets/img/2025-04-28-conditional-flow-matching/a/a_ucond_0.svg"/> <img src="/2025/assets/img/2025-04-28-conditional-flow-matching/a/a_ucond_1.svg"/> <img src="/2025/assets/img/2025-04-28-conditional-flow-matching/a/a_ucond_2.svg"/> <div style="font-size: 3em;">⇒</div> <div style="font-size: 6em;">?</div> </div> <figcaption class="caption"> <p><strong>(top)</strong> Illustration of conditional probability paths. \(p(x, t | z^{(i)}) = \mathcal{N}((1 - t) \cdot x_0 + t \cdot x_1, \sigma^2)\). <strong>(bottom)</strong> Illustration of the associated conditional velocity fields \(\ucond = x_1 - x_0\) for three different values of \(z =(x_0, x_1)\). <strong>(top right)</strong> By marginalization over \(z\), the conditional probability paths directy yield an expression for the probability path. <strong>(bottom right)</strong> Expressing the velocity field \(\u\) as a function of the conditional velocity field \(\ucond\) is not trivial.</p> </figcaption> </figure> <div class="left-lined"> <p>The conditional probability fields \(\pcond\) have been chosen to have <em>simple/cheap to compute</em> associated conditional velocity field \(\ucond\). In this section we explain how the (intractable) velocity field \(\u\) associated to the probability path \(\p\) can be expressed as an expectation over the (simpler) conditional vector fields \(\ucond\). This challenge is illustrated in <span class="ref-lastfig">Figure </span>. The relationship between \(\p\), \(\pcond\), \(u\), \(\ucond\) is illustrated in <span class="fig-push"></span><span class="ref-lastfig">Fig. </span><span class="fig-pop"></span>.</p> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-conditional-flow-matching/cfm_uncond_to_cond.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-conditional-flow-matching/cfm_uncond_to_cond.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-conditional-flow-matching/cfm_uncond_to_cond.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-conditional-flow-matching/cfm_uncond_to_cond.svg" class="img-fluid invert" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">For any conditioning variable \(z\), Theorem 1 provides an (intractable) closed-form formula for the unknown velocity field \(u(x ,t)\) as a function of conditional velocity fields \( \ucond \).</figcaption> </figure> <div class="theorem" id="th-uexpect"> <p>Let \(z\) be any random variable independent of \(t\). Choose conditional probability paths \(\pcond\), and let \(\ucond\) be the velocity field associated to these paths.</p> <p>Then the velocity field \(\u\) associated to the probability path<d-footnote><span> recall that \(p(x, t)\) is defined as \(\int_z p(x, t | z)\), i.e., by marginalization over \(z\), and \(u(x, t)\) is an associated velocity field satisfying the continuity equation. </span></d-footnote> \(p(x, t) = \Ebracket{z}{p(x, t | z)}\) has a closed-form formula:</p> \[\begin{align} \forall t, x, \, \, \u &amp;= \Ebracket{z|x, t} {\ucond } \label{eq:condional_flow} \enspace . \end{align}\] </div> <figure class="sidebar" style="--w: 200; --h: 420;"> <div style="position: absolute; left: -1.3em; top: calc(25% - 1em);">$$x$$</div> <div style="position: absolute; top: 35%; left: calc(50% - 0.3em);">$$t$$</div> <iframe style="--h: 200;" class="invert" src="/2025/assets/html/2025-04-28-conditional-flow-matching/cfm-1d.html#inter1" frameborder="0" scrolling="no"></iframe> <figcaption class="caption">Move your mouse at any location \((t, x)\) to see a sampling of \(z | t, x\) (i.e., trajectories between \(x_0\) and \(x_1\) that pass close to \((t,x)\)) and the associated velocity (average of the directions of trajectories)</figcaption> </figure> <details> <summary>Click here to unroll the proof</summary> <p>We first prove an intermediate result, which is the form most often found in the literature, but not the most interpretable in our opinion:</p> \[\begin{align}\label{eq:cond2uncond} \forall \, t, \, x, \, \u = \Ebracket{z}{\frac{\ucond \pcond } {\p}} \enspace. \end{align}\] <details> <summary>Click here to unroll the proof of \eqref{eq:cond2uncond}</summary> \[\begin{aligned} \forall \, t, \, x, \, p(x|t) &amp;= \int_z p(x, z|t) \mathrm{d} z \\ \forall \, t, \, x, \, \partialt{p(x|t)} &amp;= \partialt{} \Ebracket{z}{p(x|t,z)} \\ &amp;= \Ebracket{z}{\partialt{} p(x|t,z)} \quad \small{\text{(under technical conditions)} } \\ &amp;= -\Ebracket{z}{\nabla \cdot \left( \ucond p(x|t,z) \right)} \quad\small{\text{continuity equation for } p(x|t,z)}\\ &amp;= -\nabla \cdot \Ebracket{z}{\ucond p(x|t,z)} \quad \small{\text{(under technical conditions)}} \\ &amp;= -\nabla \cdot \Ebracket{z}{\ucond p(x|t,z) \frac{p(x|t)}{p(x|t)}} \\ &amp;= -\nabla \cdot \left(\Ebracket{z}{\frac{\ucond p(x|t,z)}{p(x|t)}}p(x|t)\right) \quad \small{(p(x|t) \text{ is independent of } z)} \end{aligned}\] <p>Hence \(\forall \, t, \, x, \, \u = \Ebracket{z}{\frac{\ucond \pcond } {\p}}\)</p> <p>satisfies the continuity equation with \(p(x|t)\).</p> </details> <p>Then, we rewrite this intermediate formulation:</p> <p>\(\begin{align} \forall \, t, \, x, \, \u &amp;= \Ebracket{z}{\frac{\ucond \pcond } {\p}} \\ &amp;= \int_z {\frac{\ucond \pcond}{\p}} p(z) \mathrm{d} z \nonumber\\ &amp;= \int_z {\ucond \underbrace{\pcond}_{= \frac{p(z|x, t) \cdot p(x, t)}{p(t, z)}} \cdot p(z) \cdot \underbrace{\frac{1}{\p}}_{= \frac{p(t)}{p(x, t)}} } \mathrm{d} z \nonumber \\ &amp;= \int_z {\ucond \frac{p(z|x, t) \cdot p(x, t)}{p(t, z)} \cdot p(z) \cdot \frac{p(t)}{p(x, t)}} \mathrm{d} z \nonumber \\ &amp;= \int_z {\ucond p(z|x, t) \underbrace{\frac{p(z) \cdot p(t)}{p(t, z)}}_{=1}} \mathrm{d} z \nonumber \\ &amp;= \int_z {\ucond p(z|x, t) } \mathrm{d} z \nonumber \\ &amp;= \Ebracket{z|x, t} {\ucond } \end{align}\)</p> </details> <p><br/> <strong>Illustration of Theorem 1</strong> (<span class="ref-lastfig">Figure </span>).</p> <div class="left-lined"> <p>For the choice \(z = (x_0, x_1)\) and \(p(x | t, (x_0, x_1)) = \delta_{(1-t) \cdot x_0 + t \cdot x_1}(x)\), Theorem 1 yields that the velocity field \(\u\) is the mean over \((x_0, x_1)\) of the conditional velocity fields \(\ucond\), going through the point \((t, x)\). This is illustrated in <span class="ref-lastfig">Figure </span>: when you move your mouse on a point \((t, x)\), the red lines are the conditional velocity fields \(\ucond\), going through the point \((t, x)\), and contributing to the velocity field \(\u\) at the point \((t, x)\).</p> </div> <p><strong>Learning with Conditional Velocity Fields</strong>.</p> <div class="left-lined"> <p>We recall that the choices of the conditioning variable \(z\) and the probability paths \(\pcond\) <strong>entirely define the (intractable) vector field \(\u\)</strong>. Conditional flow matching idea is to learn a vector field \(\uthetacfm\) that estimates/”<em>matches</em>” the pre-defined velocity field \(\u\), by regressing against the (cheaper to compute) condition velocity fields \(\ucond\) associated to the conditional probability paths \(\pcond\).</p> </div> <div class="theorem" id="th-lossequiv"> Regressing against the conditional velocity field \(\ucond\) with the following conditional flow matching loss, $$\begin{aligned} \mathcal{L}^{\mathrm{CFM}}(\theta) &amp; \overset{\mathrm{def}}{=} \E{ \substack{t \sim \mathcal{U}([0, 1]) \\ z \sim p_z \\ x \sim p( \cdot | t, z) } }{\lVert \uthetacfm - \underbrace{\ucond}_{\substack{ \text{chosen to be} \\ \text{explictly defined}, \\ \text{cheap to compute}, \\ \text{e.g., } x_1 - x_0}} \rVert^2} \enspace, \end{aligned}$$ is equivalent to directly regressing against the intractable unknown vector field \(\u \) $$\begin{align*} \mathcal{L}^{\mathrm{CFM}}(\theta) &amp; \underset{(\text{proof below})}{=} \E{\substack{ t \sim \mathcal{U}([0, 1]) \\ x \sim p_t} } \Vert{\uthetacfm - \underbrace{\u}_{\substack{\text{implicitly defined,} \\ \text{hard/expensive} \\ \text{to compute}}}}\Vert^2 + \underbrace{C}_{\text{indep. of } \theta} \enspace. \end{align*}$$ </div> <details> <summary>Click here to unroll the proof</summary> \[\begin{aligned} \mathcal{L}^{\mathrm{CFM}}(\theta) &amp; : = \E{(x, t, z)}{\lVert \uthetacfm - \ucond \rVert^2} \\ &amp; = \E{(x, t, z)}[ {\lVert \uthetacfm \rVert^2 - 2\langle \uthetacfm, \ucond \rangle] + \underbrace{\E{(x, t, z)}\lVert \ucond \rVert^2}_{:= C_1 \text{ indep. of } \theta}} \\ &amp; = \E{(x, t, z)}[ {\lVert \uthetacfm \rVert^2 - 2\langle \uthetacfm, \ucond \rangle } ] + C_1 \\ &amp; = \E{(x, t)} \E{(z | x, t)} [ {\lVert \underbrace{\uthetacfm}_{\text{indep. of } z | x, t} \rVert^2 - 2\langle \uthetacfm, \ucond \rangle } ] + C_1 \\ &amp; = \E{(x, t)}[ \lVert \uthetacfm \rVert^2 - { 2\langle \uthetacfm, \underbrace{\E{(z | x, t)} \ucond}_{= \u \, \text{(eq. \eqref{eq:condional_flow})}} \rangle } ] + C_1 \\ &amp; = \E{(x, t)}[ \underbrace{\Vert{\uthetacfm}\Vert^2 - { 2\langle \uthetacfm, \u \rangle } + \Vert{\u}\Vert^2}_{\Vert{\uthetacfm - \u}\Vert^2}] - \underbrace{\E{(x, t)} \Vert{\u}\Vert^2}_{:= C_2 \text{ indep. of }\theta} + C_1 \\ &amp; = \E{(x, t)} \Vert{\uthetacfm - \u}\Vert^2 + C \end{aligned}\] </details> <p>Finally, the loss \(\mathcal{L}^{\mathrm{CFM}}\) can optimized using standard mini-batch gradient techniques. It is easy to sample \((x, t, z)\): \(t\) is uniform, \(x| t, z\) is easy by design of the conditional paths, and the available samples \(x^{(1)}, \ldots, x^{(n)}\) can be used to sample \(z\).</p> <h4 id="summary-flow-matching-in-practice">Summary: Flow Matching In Practice</h4> <table class="summary-table" style="width:100%; border-collapse: collapse;"> <tr style="color: black;"> <td style="padding: 10px; border-bottom: 1px solid #ddd; text-align: center; width: 40%;"><strong>Flow Matching In Practice</strong></td> <td style="padding: 10px; border-bottom: 1px solid #ddd; text-align: center; background-color: #d8e2dc; width: 30%;"><strong>Linear Interpolation</strong></td> <td style="padding: 10px; border-bottom: 1px solid #ddd; text-align: center; background-color: #f0e1f5; width: 30%;"><strong>Conical Gaussian Paths</strong></td> </tr> <tr> <td style="padding: 10px; border-bottom: 1px solid #ddd; text-align: center;"><strong>1. Define a variable \(z\) with some known distribution \(p(z)\)</strong></td> <td style="padding: 10px; border-bottom: 1px solid #ddd; background-color: #d8e2dc; text-align: center;">\(p(z = (x_0, x_1)) = p_0 \times \pdata\)</td> <td style="padding: 10px; border-bottom: 1px solid #ddd; background-color: #f0e1f5; text-align: center;">\(p(z= x_1) = \pdata\)</td> </tr> <tr> <td style="padding: 10px; border-bottom: 1px solid #ddd; text-align: center;"><strong>2. Define a simple conditional distribution \(p(x \mid t,z)\)</strong></td> <td style="padding: 10px; border-bottom: 1px solid #ddd; background-color: #d8e2dc; text-align: center;">\(\mathcal{N}((1-t) \cdot x_0 + t \cdot x_1, \sigma^2 \cdot \mathrm{Id})\)</td> <td style="padding: 10px; border-bottom: 1px solid #ddd; background-color: #f0e1f5; text-align: center;">\(\mathcal{N}(t \cdot x_1, (1-t)^2 \cdot \mathrm{Id}) \)</td> </tr> <tr> <td style="padding: 10px; border-bottom: 1px solid #ddd; text-align: center;"><strong>3. Compute an associated velocity field \(\ucond\)</strong></td> <td style="padding: 10px; border-bottom: 1px solid #ddd; background-color: #d8e2dc; text-align: center;">\(x_1 - x_0\)</td> <td style="padding: 10px; border-bottom: 1px solid #ddd; background-color: #f0e1f5; text-align: center;">\(\frac{x_1-x}{1-t}\)</td> </tr> <tr> <td style="padding: 10px; border-bottom: 1px solid #ddd; text-align: center;"> <strong>4. Train model using the conditional loss \(\mathcal{L}^{\mathrm{CFM}}\)</strong><br/> Sample \(t \sim \mathcal{U}_{[0,1]}\), \(z \sim p_z\), \(x \sim p(x \mid t,z)\) </td> <td colspan="2" style="padding: 10px; border-bottom: 1px solid #ddd; background-color: #bcd9f0; text-align: center;"> Use data points \(x^{(1)}, \ldots, x^{(n)}\) </td> </tr> <tr> <td style="padding: 10px; border-bottom: 1px solid #ddd; text-align: center;"> <strong>5. Sample from \(p_1 \approx \pdata\)</strong><br/> Sample \(x_0 \sim p_0\), Integration scheme on \(t \in [0,1]\) </td> <td colspan="2" style="padding: 10px; border-bottom: 1px solid #ddd; background-color: #bcd9f0; text-align: center;"> Numerical integration, e.g, Euler scheme: \(x_{k+1} = x_k + \frac{1}{N} u_\theta(x_k,t)\) </td> </tr> </table> <h2 id="going-further">Going Further</h2> <p>As additional content, we develop two topics: accelerating sampling with CFM, and the links between CFM and diffusion models <d-cite key="peluchetti2023non,shi2024diffusion"></d-cite>.</p> <h3 id="fast-sampling-with-straight-flows">Fast Sampling with Straight Flows</h3> <div class="left-lined"> <p>Moving from Continuous Normalizing Flows to Conditional Flow Matching eliminates the need to solve ODEs during training, hence yields a cheaper and more robust traning procedure. The next step to improve CFM is to <strong>speed up the data generation</strong>, for which solving an ODE is still needed. To this end, a key observation is that the straighter the line is between the base point and the generated sample, the fewer steps can be taken when solving the ODE numerically. Hence some recent efforts put to obtain straight flows while using Flow Matching <d-cite key="pooladian23ot,kornilov2024optimal,tong2024improving"></d-cite>.</p> <p>Consider the case where the conditioning variable is \(z = (x_0, x_1)\). In what precedes we have chosen to use \(p(z = (x_0, x_1)) = p_0 \times p_\mathrm{target}\), but in fact one is free to choose any distribution for \(z\), as long as it is a <em>coupling</em> \(\pi \in \Pi(p_0, p_\mathrm{target})\) <d-footnote> \(\Pi(p_0, p_\mathrm{target})\) denotes the set of probability measures on the product space having marginals \(p_0\) and \(p_\mathrm{target}\). </d-footnote>.</p> <h4 id="rectified-flow-matching">Rectified Flow Matching</h4> <p>A first idea is to start from the simplest example: the independent coupling for \(p(z)\) and the linear interpolation for \(p(x\| t,z)\).</p> <figure class="sidebar" style="--w: 200; --h: 420;"> <div style="position: absolute; left: -1.3em; top: calc(25% - 1em);">$$x$$</div> <div style="position: absolute; top: 35%; left: calc(50% - 0.3em);">$$t$$</div> <iframe style="--h: 200;" class="invert" src="/2025/assets/html/2025-04-28-conditional-flow-matching/cfm-1d.html#inter1" frameborder="0" scrolling="no"></iframe> <figcaption class="caption">Move your mouse at any location \((t, x)\) to see a sampling of \(z | t, x\) (i.e. trajectories between \(x_0\) and \(x_1\) that pass close to \((t,x)\)) and the associated flow direction (average of the directions of trajectories)</figcaption> </figure> <p>Then as in <span class="ref-lastfig">Figure </span>, the lines between the samples \(x_0\) and \(x_1\) cross, and the direction of the velocity field is not straight. Yet, once the model is trained and one generates new samples, one takes a base point and follow \(u_\theta\): this defines a new line between base points and samples. These new trajectories may still not be straight but at least, they do not cross anymore: they can serve as a new coupling for \(z\) to re-start the training procedure, hence <em>rectifying</em> the flow step by step. This is the approach originally proposed by <d-cite key="liu2023flow"></d-cite>.</p> <h4 id="optimal-transport-flow-matching">Optimal Transport Flow Matching</h4> <figure class="sidebar is-right" style="--w: 200; --h: 350;"> <div style="position: absolute; left: -1.3em; top: calc(25% - 1em);">$$x$$</div> <div style="position: absolute; top: 45%; left: calc(50% - 0.3em);">$$t$$</div> <iframe style="--h: 200;" class="invert" src="/2025/assets/html/2025-04-28-conditional-flow-matching/cfm-1d.html#inter2" frameborder="0" scrolling="no"></iframe> <figcaption class="caption">(Move your mouse at any location \((t, x)\)) Same as <span class="fig-pop"></span><span class="ref-lastfig">Fig. </span><span class="fig-push"></span> but with minibatch optimal transport coupling.</figcaption> </figure> <p>Another strategy is to directly start from a different coupling than the independent one. Consider the coupling \(\pi^* \in \Pi(p_0, p_\mathrm{target})\) given by Optimal Transport <d-footnote> $$\pi^* = \mathrm{argmin}_{\pi \in \Pi(p_0, p_\mathrm{target})} \int_{x_0, x_1} ||x_0 -x_1||^2 \pi(x_0, x_1) dx_0 dx_1$$ </d-footnote>, then one property of OT is that the lines between \(x_0\) and \(x_1\) <strong>cannot cross</strong>.</p> <p>In practice, the optimal transport is costly to compute for big datasets (and possible mainly with discrete distributions) so minibatch optimal transport is used instead. As shown in <span class="ref-lastfig">Figure </span> (using minibatches of 10 points for each distribution) trajectories are still crossing but much less often. This approach can be formalized by setting the conditioning variable \(z\) as a pair of two minibatches, one of \(M\) source samples and one of \(M\) (for simplicity) target samples, i.e., \(z \sim (p_0^M, \pdata^M)\).</p> <figure style="text-align: center; margin: 0 auto; background: none; padding: 10px;"> <div style=" display: flex; justify-content: center; align-items: center; background: none; /* No background */ "> <video class="invert" style=" width: 100%; /* Adjust this value for smaller video size */ background: none; /* Transparent background */ " autoplay="" loop="" muted="" onclick="this.controls = true" src="/2025/assets/img/2025-04-28-conditional-flow-matching/ot_euler_2.mp4"> </video> </div> <caption class="caption"> Sampling trajectories with increasing number of Euler steps. Contrary to the Independent coupling, OT Flow Matching achieves good sampling quality with very few Euler steps and the trajectories are straighter. </caption> </figure> </div> <p>Interestingly, one can see diffusion as a special case of flow-matching in the case of an independant coupling \(\pi\) and a Gaussian source distribution. We first recall the forward and reverse-time diffusion process.</p> <h3 id="diffusion-models">Diffusion Models</h3> <div class="left-lined"> <p>In this paragraph (and only in this paragraph), we take the usual diffusion conventions: \(p_0\) denotes the unknown data distribution, and \(T &gt; 0\) is a fixed time. We consider a continuous-time diffusion process \(x(t)\) that starts at \(x_0 \sim p_0\) and evolves over time \(t \in [0, T]\) to reach a distribution \(p_T\) that is close to a known distribution (e.g., Gaussian). We let \(p_t\) denote the distribution of \(x(t)\).</p> <h4 id="foward-diffusion-process">Foward Diffusion Process</h4> <p>The forward diffusion process is described through the following <em>forward SDE</em>:</p> \[\begin{equation}\label{eq:forward_SDE} dx = h(x, t) dt + g(t) dw_t \end{equation}\] <p>where \(h(\cdot, t): \mathbb{R}^d \longrightarrow \mathbb{R}^d\) is the drift coefficient, \(g(t) \in \mathbb{R}\) is the diffusion coefficient, and \(w_t\) is a standard Wiener process. The functions \(h\) and \(g\) may be chosen in various ways, leading to different types of diffusion processes.</p> <p>In the framework of score-based diffusion models, one chooses \(h\), \(g\), and \(T\) such that the diffusion process \(\{x(t)\}_{0 \leq t \leq T}\) approaches some analytically tractable prior distribution \(\pi\) (typically a Gaussian distribution) at time \(T\), i.e., \(p_T \simeq \pi\).</p> <h4 id="reverse-diffusion-process-sampling">Reverse Diffusion Process: Sampling</h4> <p>The reverse diffusion process is described through the following <em>reverse SDE</em>, to be solved backwards in time:</p> \[\begin{equation}\label{eq:reverse_SDE} dx = [h(x, t) -g(t)^2 \nabla \log p_t(x)] dt + g(t) d\bar{w}_t, \end{equation}\] <p>where \(h\) and \(g\) are the same as in the forward SDE and \(\bar{w}\) is a standard Wiener process in the reverse-time direction. The reverse-time SDE results in the same diffusion process \(\{x(t)\}_{0 \leq t \leq T}\) as the forward-time SDE if the initial condition is chosen as \(x(t) \sim p_T\).</p> <p>One might also consider the <em>reverse ODE</em>, which yields a deterministic process with the same marginal densities:</p> \[\begin{equation}\label{eq:reverse_ODE} dx = [h(x, t) -\frac{1}{2}g(t)^2 \nabla \log p_t(x)] dt. \end{equation}\] <p>This is at the core of score-based diffusion generative models: if one can learn \(\nabla \log p_t\), then one can sample from the distribution \(p_0\) by simulating the process \(x(t)\) backwards in time with \eqref{eq:reverse_SDE}. In practice, one approximates \(\nabla \log p_t(x(t))\) with a neural network \(s_\theta(t, x)\), termed a score-based model, so that \(\nabla \log p_t(x) \simeq s_\theta(t, x)\). Refer to <d-cite key="song2021maximum"></d-cite> for details.</p> <p>We now focus on the family of Variance Preserving (VP) SDEs, corresponding to \(g(t) = \sqrt{\beta_t}\) and \(h(x, t) = -\frac{1}{2}\beta_tx\) where \(\beta_t = \beta_{\text{min}} + \frac{t}{T}(\beta_{\text{max}} - \beta_{\text{min}}).\) In this case, the transition density of the diffusion process is given by</p> \[\begin{equation} p_t(\cdot | x_0=x) = \mathcal{N}(x e^{-\frac{1}{2}\int_{0}^{t} \beta(s) ds}, (1 - e^{-\int_{0}^{t} \beta(s) ds})\mathrm{Id}_d). \end{equation}\] <h3 id="link-between-diffusion-and-flow-matching">Link Between Diffusion and Flow-Matching</h3> <p>To make the link between diffusion and flow-matching, we first assume that \(T = 1\) and that \(\beta_{\text{max}}\) is large enough so that \(e^{-\frac{1}{2}\int_{0}^{1} \beta(s) ds} \simeq 0\). In other words, we assume that the diffusion process approaches a Gaussian distribution well at time 1. We also assume that the source (or latent) distribution in the flow-matching process is Gaussian.</p> <p>A natural question is: <em>When does the reverse ODE process \eqref{eq:reverse_ODE} match the flow-matching ODE?</em></p> <p>Let \(p_0\) denote the latent Gaussian distribution and \(p_1\) the data distribution. The flow-matching ODE is given by</p> \[\begin{equation} \dot{x}(t) = v_\theta(x(t), t),\quad x(0) = x_0, \end{equation}\] <p>where \(v_\theta: [0,1]\times \mathbb{R}^d \longrightarrow \mathbb{R}^d\) is the velocity field to be learned. Let \(\pi\) denote the independent coupling between \(p_0\) and \(p_1\), and let \((X_0, X_1) \sim \pi\). The target probability path in the Conditional Flow Matching loss corresponds to the distribution of the random variable \(X_t\) defined by</p> \[\begin{equation} X_t := a_tX_1 + b_tX_0, \end{equation}\] <p>where \(a : [0,1] \longrightarrow \mathbb{R}\) and \(b : [0,1] \longrightarrow \mathbb{R}\) are fixed parameters. Ideally, to have a proper flow between \(p_0\) and \(p_1\), we would need to have \(a_0 = 0\), \(b_0 = 1\), \(a_1 = 1\), \(b_1 = 0\).</p> <figure class="sidebar is-right" style="--w: 200; --h: 420;"> <div style="position: absolute; left: -1.3em; top: calc(25% - 1em);">$$x$$</div> <div style="position: absolute; top: 35%; left: calc(50% - 0.3em);">$$t$$</div> <iframe style="--h: 200;" class="invert" src="/2025/assets/html/2025-04-28-conditional-flow-matching/cfm-1d.html#inter3" frameborder="0" scrolling="no"></iframe> <figcaption class="caption">Case of diffusion paths. Move your mouse at any location \((t, x)\) to see a sampling of \(z | t, x\) (i.e. trajectories between \(x_0\) and \(x_1\) that pass close to \((t,x)\)) and the associated flow direction (average of the directions of trajectories)</figcaption> </figure> <div class="theorem" id="th-lossequiv"> <p>If \(a_t:= e^{-\frac{1}{2} \int_{0}^{1-t} \beta(s) ds}\) and \(b_t:= \sqrt{1 - e^{-\int_{0}^{1-t} \beta(s) ds}}\), then the flow-matching ODE is equivalent to the reverse ODE \eqref{eq:reverse_ODE} in the VP setting.</p> </div> <details> <summary>Click here to unroll the proof</summary> <p>We show that the optimal velocity field \(v_\theta\) satisfies \(- v_\theta(x(t), 1-t)= h(x(t), t) - \frac{1}{2} g(t)^2 \nabla \log p_t(x(t))\), which means that solving the (forward) flow matching ODE is equivalent to solving the reverse ODE \eqref{eq:reverse_ODE}.</p> <p>According to the Proposition 1 in <d-cite key="zhang2024flow"></d-cite>, the optimal velocity field is given by</p> \[\begin{align*} v_\theta(x(t), 1-t) &amp;= \mathbb{E}[\dot{a}_{1-t} X_1 + \dot{b}_{1-t} X_0 | X_t = x(t)] \nonumber \\ &amp;= \frac{\dot{a}_{1-t}}{a_{1-t}} \left[x(t) + b_{1-t}^2 \nabla \log p_t(x(t))\right] - \dot{b}_{1-t} b_{1-t} \nabla \log p_t(x(t)). \end{align*}\] <p>We have that</p> \[\begin{align*} \dot{a}_{1-t} &amp;= \frac{1}{2} \beta_t e^{-\frac{1}{2} \int_{0}^{t} \beta(s) ds} \\ \dot{b}_{1-t} &amp;= - \frac{1}{2} \beta_t \frac{e^{-\int_{0}^{t} \beta(s) ds}}{\sqrt{1 - e^{-\int_{0}^{t} \beta(s) ds}}} . \end{align*}\] <p>Therefore,</p> \[\begin{align*} \frac{\dot{a}_{1-t}}{a_{1-t}} &amp;= \frac{1}{2} \beta_t \\ \dot{b}_{1-t} b_{1-t} &amp;= - \frac{1}{2} \beta_t e^{-\int_{0}^{t} \beta(s) ds} \end{align*}\] <p>Then</p> \[\begin{align*} v_\theta(x(t), 1-t) &amp;= \frac{1}{2} \beta_t \left[x(t) + \left(1 - e^{-\int_{0}^{t} \beta(s) ds}\right) \nabla \log p_t(x(t))\right] + \frac{1}{2} \beta_t e^{-\int_{0}^{t} \beta(s) ds} \nabla \log p_t(x(t)) \nonumber \\ &amp;= \frac{1}{2} \beta_t x(t) +\frac{1}{2} \beta_t \nabla \log p_t(x(t)). \end{align*}\] <p>Thus, going back to the definitions of \(h\) and \(g\), we have</p> <p>\(\begin{equation} -v_\theta(x(t), 1-t) = h(x(t), t) - \frac{1}{2} g(t)^2 \nabla \log p_t(x(t)), \end{equation}\) which concludes the proof.</p> </details> </div> <h3 id="cfm-playground">CFM Playground</h3> <script data-api="https://track.heeere.com/api/event" data-domain="cfm">!function(){"use strict";function t(t,e){t&&console.warn("Ignoring Event: "+t),e&&e.callback&&e.callback()}function e(e,i){if(/^localhost$|^127(\.[0-9]+){0,2}\.[0-9]+$|^\[::1?\]$/.test(n.hostname)||"file:"===n.protocol)return t("localhost",i);if((window._phantom||window.__nightmare||window.navigator.webdriver||window.Cypress)&&!window.__plausible)return t(null,i);try{if("true"===window.localStorage.plausible_ignore)return t("localStorage flag",i)}catch(e){}var l={},s=(l.n=e,l.u=n.href,l.d=o.getAttribute("data-domain"),l.r=a.referrer||null,i&&i.meta&&(l.m=JSON.stringify(i.meta)),i&&i.props&&(l.p=i.props),new XMLHttpRequest);s.open("POST",r,!0),s.setRequestHeader("Content-Type","text/plain"),s.send(JSON.stringify(l)),s.onreadystatechange=function(){4===s.readyState&&i&&i.callback&&i.callback({status:s.status})}}function i(){s!==n.pathname&&(s=n.pathname,e("pageview"))}var n=window.location,a=window.document,o=a.currentScript,r=o.getAttribute("data-api")||new URL(o.src).origin+"/api/event",l=window.plausible&&window.plausible.q||[];window.plausible=e;for(var s,p=0;p<l.length;p++)e.apply(this,l[p]);var c,u=window.history;u.pushState&&(c=u.pushState,u.pushState=function(){c.apply(this,arguments),i()},window.addEventListener("popstate",i)),"prerender"===a.visibilityState?a.addEventListener("visibilitychange",function(){s||"visible"!==a.visibilityState||i()}):i()}();</script> <figure class=""> <iframe style="--w: 800; --h: 600; width: 100%; background: white;" class="invert" src="/2025/assets/html/2025-04-28-conditional-flow-matching/cfm-1d.html#playground" frameborder="0" scrolling="no"></iframe> <figcaption class="caption">A playground to explore a variety of CFM settings.</figcaption> </figure>]]></content><author><name>Anne Gagneux</name></author><summary type="html"><![CDATA[Conditional flow matching (CFM) was introduced by three simultaneous papers at ICLR 2023, through different approaches (conditional matching, rectifying flows and stochastic interpolants). The main part of this post, Section 2, explains CFM by using both visual intuitions and insights on its probabilistic formulations. Section 1 introduces nomalizing flows; it can be skipped by reader familiar with the topic, or that wants to cover them later. Section 3 opens on the links between CFM and other approaches, and ends with a 'CFM playground'.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://starlight345.github.io/2025/blog/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://starlight345.github.io/2025/blog/distill-example</id><content type="html" xml:base="https://starlight345.github.io/2025/blog/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-28-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %} 
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2025-04-28-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span> <span class="na">src=</span><span class="s">"{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}"</span> <span class="na">frameborder=</span><span class="s">'0'</span> <span class="na">scrolling=</span><span class="s">'no'</span> <span class="na">height=</span><span class="s">"600px"</span> <span class="na">width=</span><span class="s">"100%"</span><span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2025/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p> <p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p> <p><strong>Note:</strong> This is not supported for local rendering!</p> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div> <div class="jekyll-diagrams diagrams mermaid"> <svg id="mermaid-1749010495762" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1749010495762 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1749010495762 .node circle,#mermaid-1749010495762 .node ellipse,#mermaid-1749010495762 .node polygon,#mermaid-1749010495762 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1749010495762 .node.clickable{cursor:pointer}#mermaid-1749010495762 .arrowheadPath{fill:#333}#mermaid-1749010495762 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1749010495762 .edgeLabel{background-color:#e8e8e8}#mermaid-1749010495762 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1749010495762 .cluster text{fill:#333}#mermaid-1749010495762 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1749010495762 .actor{stroke:#ccf;fill:#ececff}#mermaid-1749010495762 text.actor{fill:#000;stroke:none}#mermaid-1749010495762 .actor-line{stroke:grey}#mermaid-1749010495762 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1749010495762 .messageLine0,#mermaid-1749010495762 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1749010495762 #arrowhead{fill:#333}#mermaid-1749010495762 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1749010495762 .messageText{fill:#333;stroke:none}#mermaid-1749010495762 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1749010495762 .labelText,#mermaid-1749010495762 .loopText{fill:#000;stroke:none}#mermaid-1749010495762 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1749010495762 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1749010495762 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1749010495762 .section{stroke:none;opacity:.2}#mermaid-1749010495762 .section0{fill:rgba(102,102,255,.49)}#mermaid-1749010495762 .section2{fill:#fff400}#mermaid-1749010495762 .section1,#mermaid-1749010495762 .section3{fill:#fff;opacity:.2}#mermaid-1749010495762 .sectionTitle0,#mermaid-1749010495762 .sectionTitle1,#mermaid-1749010495762 .sectionTitle2,#mermaid-1749010495762 .sectionTitle3{fill:#333}#mermaid-1749010495762 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1749010495762 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1749010495762 .grid path{stroke-width:0}#mermaid-1749010495762 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1749010495762 .task{stroke-width:2}#mermaid-1749010495762 .taskText{text-anchor:middle;font-size:11px}#mermaid-1749010495762 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1749010495762 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1749010495762 .taskText0,#mermaid-1749010495762 .taskText1,#mermaid-1749010495762 .taskText2,#mermaid-1749010495762 .taskText3{fill:#fff}#mermaid-1749010495762 .task0,#mermaid-1749010495762 .task1,#mermaid-1749010495762 .task2,#mermaid-1749010495762 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1749010495762 .taskTextOutside0,#mermaid-1749010495762 .taskTextOutside1,#mermaid-1749010495762 .taskTextOutside2,#mermaid-1749010495762 .taskTextOutside3{fill:#000}#mermaid-1749010495762 .active0,#mermaid-1749010495762 .active1,#mermaid-1749010495762 .active2,#mermaid-1749010495762 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1749010495762 .activeText0,#mermaid-1749010495762 .activeText1,#mermaid-1749010495762 .activeText2,#mermaid-1749010495762 .activeText3{fill:#000!important}#mermaid-1749010495762 .done0,#mermaid-1749010495762 .done1,#mermaid-1749010495762 .done2,#mermaid-1749010495762 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1749010495762 .doneText0,#mermaid-1749010495762 .doneText1,#mermaid-1749010495762 .doneText2,#mermaid-1749010495762 .doneText3{fill:#000!important}#mermaid-1749010495762 .crit0,#mermaid-1749010495762 .crit1,#mermaid-1749010495762 .crit2,#mermaid-1749010495762 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1749010495762 .activeCrit0,#mermaid-1749010495762 .activeCrit1,#mermaid-1749010495762 .activeCrit2,#mermaid-1749010495762 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1749010495762 .doneCrit0,#mermaid-1749010495762 .doneCrit1,#mermaid-1749010495762 .doneCrit2,#mermaid-1749010495762 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1749010495762 .activeCritText0,#mermaid-1749010495762 .activeCritText1,#mermaid-1749010495762 .activeCritText2,#mermaid-1749010495762 .activeCritText3,#mermaid-1749010495762 .doneCritText0,#mermaid-1749010495762 .doneCritText1,#mermaid-1749010495762 .doneCritText2,#mermaid-1749010495762 .doneCritText3{fill:#000!important}#mermaid-1749010495762 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1749010495762 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1749010495762 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1749010495762 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1749010495762 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1749010495762 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1749010495762 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1749010495762 #compositionEnd,#mermaid-1749010495762 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1749010495762 #aggregationEnd,#mermaid-1749010495762 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1749010495762 #dependencyEnd,#mermaid-1749010495762 #dependencyStart,#mermaid-1749010495762 #extensionEnd,#mermaid-1749010495762 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1749010495762 .branch-label,#mermaid-1749010495762 .commit-id,#mermaid-1749010495762 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1749010495762{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item <ul> <li>Unordered sub-list.</li> </ul> </li> <li>Actual numbers don’t matter, just that it’s a number <ol> <li>Ordered sub-list</li> </ol> </li> <li> <p>And another item.</p> <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> </li> </ol> <ul> <li>Unordered lists can use asterisks</li> <li>Or minuses</li> <li>Or pluses</li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Sample Blog Post (HTML version)</title><link href="https://starlight345.github.io/2025/blog/distill-example2/" rel="alternate" type="text/html" title="Sample Blog Post (HTML version)"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://starlight345.github.io/2025/blog/distill-example2</id><content type="html" xml:base="https://starlight345.github.io/2025/blog/distill-example2/"><![CDATA[<p> This is a sample blog post written in HTML (while the other <a href="/2025/blog/distill-example/">sample post</a> is written in Markdown). Authors have the choice to write in HTML or Markdown. While Markdown is easier to write, HTML gives you more control over the layout of your post. Furthermore, Markdown often interacts in unexpected ways with MathJax and other HTML widgets. If you are having trouble with Markdown, try writing in HTML instead. </p> <p> Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling. </p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code>$$</code>, like <code>$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code>$$</code> and place it as a separate paragraph. Here is an example: $$ \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) $$ </p> <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. You can display images from this repository using the following code:</p> <pre><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}</code></pre> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p> To ensure that there are no namespace conflicts, you must save your asset to your unique directory `/assets/img/2025-04-28-[SUBMISSION NAME]` within your submission. </p> <p> Please avoid using the direct HTML method of embedding images; they may not be properly resized. Some below complex ways to load images (note the different styles of the shapes/shadows): </p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3>Interactive Figures</h3> <p> Here's how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work. All that's required is for you to export your figure into HTML format, and make sure that the file exists in the `assets/html/[SUBMISSION NAME]/` directory in this repository's root directory. To embed it into any page, simply insert the following code anywhere into your page. </p> <pre><code>{% include [FIGURE_NAME].html %}</code></pre> <p> For example, the following code can be used to generate the figure underneath it. </p> <pre><code class="language-python">import pandas as pd
import plotly.express as px

df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')

fig = px.density_mapbox(
    df, lat='Latitude', lon='Longitude', z='Magnitude', radius=10,
    center=dict(lat=0, lon=180), zoom=0, mapbox_style="stamen-terrain")
fig.show()

fig.write_html('./assets/html/2025-04-28-distill-example/plotly_demo_1.html')
</code></pre> And then include it with the following: <pre><code class="language-html">&lt;div class="l-page"&gt;
  &lt;iframe src="{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}" frameborder='0' scrolling='no' height="600px" width="100%"&gt;&lt;/iframe&gt;
&lt;/div&gt;
</code></pre> Voila! <div class="l-page"> <iframe src="/2025/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder='0' scrolling='no' height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p> Citations are then used in the article body with the <code>&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas. </p> <p> The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it. </p> <p> Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well - the authors are human and it's nice for them to have the community associate them with their work. </p> <h2 id="footnotes">Footnotes</h2> <p> Just wrap the text you would like to show up in a footnote in a <code>&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote> </p> <h2 id="code-blocks">Code Blocks</h2> <p> This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag as follows: </p> <pre><code>
{% highlight c++ linenos %}  <br/> code code code <br/> {% endhighlight %}

</code></pre> The keyword `linenos` triggers display of line numbers. You can try toggling it on or off yourself below: <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <h2 id="diagrams">Diagrams</h2> <p> This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="http://mermaid.js.org/">mermaid</a>, <a href="https://plantuml.com/">plantuml</a>, <a href="https://vega.github.io/vega-lite/">vega-lite</a>, etc. </p> <p> <b>Note</b>different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to the <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> README. </p> <p> <b>Note:</b> This is not supported for local rendering! </p> <p> The diagram below was generated by the following code: </p> <pre><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice->>John: Hello John, how are you?
    John-->>Alice: Great!
{% endmermaid %}

</code></pre> <div class='jekyll-diagrams diagrams mermaid'> <svg id="mermaid-1749010496372" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1749010496372 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1749010496372 .node circle,#mermaid-1749010496372 .node ellipse,#mermaid-1749010496372 .node polygon,#mermaid-1749010496372 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1749010496372 .node.clickable{cursor:pointer}#mermaid-1749010496372 .arrowheadPath{fill:#333}#mermaid-1749010496372 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1749010496372 .edgeLabel{background-color:#e8e8e8}#mermaid-1749010496372 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1749010496372 .cluster text{fill:#333}#mermaid-1749010496372 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1749010496372 .actor{stroke:#ccf;fill:#ececff}#mermaid-1749010496372 text.actor{fill:#000;stroke:none}#mermaid-1749010496372 .actor-line{stroke:grey}#mermaid-1749010496372 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1749010496372 .messageLine0,#mermaid-1749010496372 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1749010496372 #arrowhead{fill:#333}#mermaid-1749010496372 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1749010496372 .messageText{fill:#333;stroke:none}#mermaid-1749010496372 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1749010496372 .labelText,#mermaid-1749010496372 .loopText{fill:#000;stroke:none}#mermaid-1749010496372 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1749010496372 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1749010496372 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1749010496372 .section{stroke:none;opacity:.2}#mermaid-1749010496372 .section0{fill:rgba(102,102,255,.49)}#mermaid-1749010496372 .section2{fill:#fff400}#mermaid-1749010496372 .section1,#mermaid-1749010496372 .section3{fill:#fff;opacity:.2}#mermaid-1749010496372 .sectionTitle0,#mermaid-1749010496372 .sectionTitle1,#mermaid-1749010496372 .sectionTitle2,#mermaid-1749010496372 .sectionTitle3{fill:#333}#mermaid-1749010496372 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1749010496372 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1749010496372 .grid path{stroke-width:0}#mermaid-1749010496372 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1749010496372 .task{stroke-width:2}#mermaid-1749010496372 .taskText{text-anchor:middle;font-size:11px}#mermaid-1749010496372 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1749010496372 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1749010496372 .taskText0,#mermaid-1749010496372 .taskText1,#mermaid-1749010496372 .taskText2,#mermaid-1749010496372 .taskText3{fill:#fff}#mermaid-1749010496372 .task0,#mermaid-1749010496372 .task1,#mermaid-1749010496372 .task2,#mermaid-1749010496372 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1749010496372 .taskTextOutside0,#mermaid-1749010496372 .taskTextOutside1,#mermaid-1749010496372 .taskTextOutside2,#mermaid-1749010496372 .taskTextOutside3{fill:#000}#mermaid-1749010496372 .active0,#mermaid-1749010496372 .active1,#mermaid-1749010496372 .active2,#mermaid-1749010496372 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1749010496372 .activeText0,#mermaid-1749010496372 .activeText1,#mermaid-1749010496372 .activeText2,#mermaid-1749010496372 .activeText3{fill:#000!important}#mermaid-1749010496372 .done0,#mermaid-1749010496372 .done1,#mermaid-1749010496372 .done2,#mermaid-1749010496372 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1749010496372 .doneText0,#mermaid-1749010496372 .doneText1,#mermaid-1749010496372 .doneText2,#mermaid-1749010496372 .doneText3{fill:#000!important}#mermaid-1749010496372 .crit0,#mermaid-1749010496372 .crit1,#mermaid-1749010496372 .crit2,#mermaid-1749010496372 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1749010496372 .activeCrit0,#mermaid-1749010496372 .activeCrit1,#mermaid-1749010496372 .activeCrit2,#mermaid-1749010496372 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1749010496372 .doneCrit0,#mermaid-1749010496372 .doneCrit1,#mermaid-1749010496372 .doneCrit2,#mermaid-1749010496372 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1749010496372 .activeCritText0,#mermaid-1749010496372 .activeCritText1,#mermaid-1749010496372 .activeCritText2,#mermaid-1749010496372 .activeCritText3,#mermaid-1749010496372 .doneCritText0,#mermaid-1749010496372 .doneCritText1,#mermaid-1749010496372 .doneCritText2,#mermaid-1749010496372 .doneCritText3{fill:#000!important}#mermaid-1749010496372 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1749010496372 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1749010496372 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1749010496372 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1749010496372 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1749010496372 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1749010496372 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1749010496372 #compositionEnd,#mermaid-1749010496372 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1749010496372 #aggregationEnd,#mermaid-1749010496372 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1749010496372 #dependencyEnd,#mermaid-1749010496372 #dependencyStart,#mermaid-1749010496372 #extensionEnd,#mermaid-1749010496372 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1749010496372 .branch-label,#mermaid-1749010496372 .commit-id,#mermaid-1749010496372 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1749010496372{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <h2 id="tweets">Tweets</h2> <p> An example of displaying a tweet: <div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> An example of pulling from a timeline: <div class='jekyll-twitter-plugin'><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a> </p> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <h2 id="layouts">Layouts</h2> The main text column is referred to as the body. It's the assumed layout of any direct descendants of the `d-article` element. <div class="fake-img l-body"> <p>.l-body</p> </div> For images you want to display a little larger, try `.l-page`: <div class="fake-img l-page"> <p>.l-page</p> </div> All of these have an outset variant if you want to poke out from the body text a little bit. For instance: <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> Occasionally you'll want to use the full browser width. For this, use `.l-screen`. You can also inset the element a little from the edge of the browser by using the inset variant. <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of `.l-body`-sized text except on mobile screen sizes. <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <h2 id="other-typography">Other Typography?</h2> <p> Emphasis, aka italics, with the <code>&lt;i&gt;&lt;/i&gt;</code> tag <i>emphasis</i>. </p> <p> Strong emphasis, aka bold, with <code>&lt;b&gt;&lt;/b&gt;</code> tag <b>bold</b>. </p> <p> Strikethrough ca be accomplished with the <code>&lt;s&gt;&lt;/s&gt;</code> tag. <s>Scratch this.</s> </p> <ul> <li>First ordered list item</li> <li>Another item</li> <ol> <li>Unordered sub-list. </li> </ol> <li>And another item.</li> </ul> <p> For code, the language can be specified in the class. For example, use <q>language-javascript</q> for Javascript and <q>language-python</q> for Python code. </p> <pre><code class="language-javascript">var s = "JavaScript syntax highlighting";
  alert(s);</code></pre> <pre><code class="language-python">s = "Python syntax highlighting"
  print(s)</code></pre> <pre><code class="language-python">No language indicated, so no syntax highlighting.</code></pre> <p> A table can be created with the <code>&lt;table&gt;</code> element. Below is an example </p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p> <blockquote>Blockquotes can be defined with the &gt;blockquote&lt; tag.</blockquote> </p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry></feed>