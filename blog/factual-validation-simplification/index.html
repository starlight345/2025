<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,a=e=>{let t=e.split(" "),a=t.slice(0,-1).join(" ");return[t.at(-1),a]},n=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(a),i=n[0][0],o=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"April 28, 2025"),r="Factual Context Validation and Simplification: A Scalable Method to Enhance GPT Trustworthiness and Efficiency",l="As the deployment of Large Language Models (LLMs) like GPT expands across domains, mitigating their susceptibility to factual inaccuracies or hallucinations becomes crucial for ensuring reliable performance. This blog post introduces two novel frameworks that enhance retrieval-augmented generation (RAG): one uses summarization to achieve a maximum of 57.7% storage reduction, while the other preserves critical information through statement-level extraction. Leveraging DBSCAN clustering, vectorized fact storage, and LLM-driven fact-checking, the pipelines deliver higher overall performance across benchmarks such as PubMedQA, SQuAD, and HotpotQA. By optimizing efficiency and accuracy, these frameworks advance trustworthy AI for impactful real-world applications.";{let e=n.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(i+"2025"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${r}},\n  abstract = {${l}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${o}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=n.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${r}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Factual Context Validation and Simplification: A Scalable Method to Enhance GPT Trustworthiness and Efficiency | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="As the deployment of Large Language Models (LLMs) like GPT expands across domains, mitigating their susceptibility to factual inaccuracies or hallucinations becomes crucial for ensuring reliable performance. This blog post introduces two novel frameworks that enhance retrieval-augmented generation (RAG): one uses summarization to achieve a maximum of 57.7% storage reduction, while the other preserves critical information through statement-level extraction. Leveraging DBSCAN clustering, vectorized fact storage, and LLM-driven fact-checking, the pipelines deliver higher overall performance across benchmarks such as PubMedQA, SQuAD, and HotpotQA. By optimizing efficiency and accuracy, these frameworks advance trustworthy AI for impactful real-world applications."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/2025/assets/css/main.css"> <link rel="canonical" href="https://starlight345.github.io/2025/blog/factual-validation-simplification/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/2025/assets/js/theme.js"></script> <script src="/2025/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2025/assets/js/distillpub/template.v2.js"></script> <script src="/2025/assets/js/distillpub/transforms.v2.js"></script> <script src="/2025/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}.box-important{background:linear-gradient(135deg,#f8f9fa,#e6f7f9);border:1px solid #76b7c7;padding:20px;margin:20px 0;box-shadow:0 4px 6px rgba(0,0,0,0.1);font-family:'Arial',sans-serif;font-size:16px;color:#2a3e4b;line-height:1.7;border-radius:0}.box-two{border-left:5px solid #4a90e2;background:rgba(74,144,226,0.1);padding:15px;margin:20px 0;font-family:"Roboto",Arial,sans-serif;font-size:16px;line-height:1.6;border-radius:5px}.matrix-box{border:1px solid #d3d3d3;border-radius:10px;padding:15px;margin:20px auto;background-color:#fefefe;box-shadow:0 4px 8px rgba(0,0,0,0.1);display:inline-block}.box-future{border:2px solid #007bff;background:#f8faff;padding:25px;margin:20px 0;font-family:"Roboto",Arial,sans-serif;font-size:16px;line-height:1.6;border-radius:8px;box-shadow:0 3px 8px rgba(0,0,0,0.08)}summary-math{text-align:center;color:black}[data-theme="dark"] summary-math{color:white}d-article{overflow-x:visible}pre{background-color:#f5f5f5;border:1px solid #ddd;border-radius:6px;padding:12px;overflow-x:auto;box-shadow:0 1px 3px rgba(0,0,0,0.1)}</style> <d-front-matter> <script async type="text/json">{
      "title": "Factual Context Validation and Simplification: A Scalable Method to Enhance GPT Trustworthiness and Efficiency",
      "description": "As the deployment of Large Language Models (LLMs) like GPT expands across domains, mitigating their susceptibility to factual inaccuracies or hallucinations becomes crucial for ensuring reliable performance. This blog post introduces two novel frameworks that enhance retrieval-augmented generation (RAG): one uses summarization to achieve a maximum of 57.7% storage reduction, while the other preserves critical information through statement-level extraction. Leveraging DBSCAN clustering, vectorized fact storage, and LLM-driven fact-checking, the pipelines deliver higher overall performance across benchmarks such as PubMedQA, SQuAD, and HotpotQA. By optimizing efficiency and accuracy, these frameworks advance trustworthy AI for impactful real-world applications.",
      "published": "April 28, 2025",
      "authors": [
        {
          "author": "Tianyi Huang",
          "authorURL": "https://www.linkedin.com/in/tianyi-huang-36ba49280/",
          "affiliations": [
            {
              "name": "App Inventor Foundation",
              "url": "https://www.appinventorfoundation.org/"
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/reviewing/">reviewing</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Factual Context Validation and Simplification: A Scalable Method to Enhance GPT Trustworthiness and Efficiency</h1> <p>As the deployment of Large Language Models (LLMs) like GPT expands across domains, mitigating their susceptibility to factual inaccuracies or hallucinations becomes crucial for ensuring reliable performance. This blog post introduces two novel frameworks that enhance retrieval-augmented generation (RAG): one uses summarization to achieve a maximum of 57.7% storage reduction, while the other preserves critical information through statement-level extraction. Leveraging DBSCAN clustering, vectorized fact storage, and LLM-driven fact-checking, the pipelines deliver higher overall performance across benchmarks such as PubMedQA, SQuAD, and HotpotQA. By optimizing efficiency and accuracy, these frameworks advance trustworthy AI for impactful real-world applications.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#the-challenge-of-factual-accuracy-in-ai">The Challenge of Factual Accuracy in AI</a></div> <ul> <li><a href="#1-dynamic-and-unstructured-contexts">1. Dynamic and Unstructured Contexts</a></li> <li><a href="#2-compounded-errors-in-reasoning">2. Compounded Errors in Reasoning</a></li> </ul> <div><a href="#limitations-of-existing-approaches">Limitations of Existing Approaches</a></div> <ul> <li><a href="#fine-tuning">Fine-Tuning</a></li> <li><a href="#retrieval-augmented-generation-rag">Retrieval-Augmented Generation (RAG)</a></li> <li><a href="#post-hoc-validation">Post-Hoc Validation</a></li> </ul> <div><a href="#theoretical-foundations">Theoretical Foundations</a></div> <ul> <li><a href="#factual-validation-core-principles">Factual Validation: Core Principles</a></li> <li><a href="#the-role-of-vectorization">The Role of Vectorization</a></li> <li><a href="#combining-granularity-and-vectorization">Combining Granularity and Vectorization</a></li> </ul> <div><a href="#our-proposal-fact-based-validation-for-gpt-systems">Our Proposal: Fact-Based Validation for GPT Systems</a></div> <div><a href="#implementation-details">Implementation Details</a></div> <ul> <li><a href="#new-data-input-system">New Data Input System</a></li> <li><a href="#llm-validity-judger">LLM Validity Judger</a></li> </ul> <div><a href="#experimentation-and-results">Experimentation and Results</a></div> <ul> <li><a href="#factual-accuracy">Factual Accuracy</a></li> <li><a href="#rag-effectiveness">RAG Effectiveness</a></li> <li><a href="#storage-efficiency">Storage Efficiency</a></li> <li><a href="#validity-judgment-with-chain-of-thought">Validity Judgment with Chain of Thought</a></li> </ul> <div><a href="#alternative-solution-statement-extraction">Alternative Solution: Statement Extraction</a></div> <ul> <li><a href="#extended-benchmarks">Extended Benchmarks</a></li> <li><a href="#realistic-usage">Realistic Usage</a></li> </ul> <div><a href="#broader-implications">Broader Implications</a></div> <div><a href="#limitations-and-future-work">Limitations and Future Work</a></div> <div><a href="#reproducibility">Reproducibility</a></div> <div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Large Language Models (LLMs), such as GPT, have advanced natural language processing by offering incredible fluency and adaptability <d-cite key="brown2020"></d-cite>. Yet, these models are susceptible to <strong>“hallucinations”</strong>—outputs that are linguistically coherent but factually incorrect <d-cite key="ji2022, maynez2020"></d-cite>. This issue arises because LLMs optimize for contextual fluency rather than accuracy, a trade-off that becomes increasingly problematic in applications where precision or reliability is necessary <d-cite key="holtzman2019, esteva2019, ullah2024"></d-cite>.</p> <figure style="text-align: center;"> <img src="/2025/assets/img/2025-04-28-factual-validation-simplification/hallucination.png" style="width: 85%;"> <figcaption style="font-size: 15px; text-align: left; margin: 0 auto; max-width: 800px;"> <strong>Figure 1.</strong> An example of hallucination in ChatGPT. This figure demonstrates a fabricated response confidently generated by ChatGPT, showing how large language models (LLMs) can produce coherent but factually incorrect outputs. The highlighted sections indicate fictional and misleading content, emphasizing the challenges in ensuring factual accuracy in LLM-generated responses. </figcaption> </figure> <p>Existing solutions, including fine-tuning, retrieval-augmented generation (RAG), and post-hoc validation, address specific aspects of this problem but often at the cost of scalability or computational efficiency <d-cite key="guu2020"></d-cite>. These methods frequently fail to account for the growing demand to process unstructured data efficiently while maintaining factual accuracy.</p> <p>In this blog post, we propose an enhancement to RAG pipelines that integrates LLM summarization, clustering with DBSCAN, and vectorized fact storage to tackle the challenges of managing unstructured data <d-cite key="ester1996"></d-cite>. Our framework further innovates a fact-verification system that splits LLM responses into granular components for individual fact-level validation. By processing expansive datasets into concise, verifiable formats, this approach achieves significant improvements in storage efficiency without sacrificing factual accuracy. Evaluated on the PubMedQA dataset, the proposed method demonstrates how preprocessing and factual validation can enhance RAG workflows, improve scalability, and maintain retrieval precision, offering new possibilities for real-world applications <d-cite key="jin2019"></d-cite>.</p> <h2 id="the-challenge-of-factual-accuracy-in-ai">The Challenge of Factual Accuracy in AI</h2> <p><span style="font-size: 20px; font-weight: bold;">Hallucinations and Trust Deficit</span></p> <p>Hallucinations in LLMs arise from their probabilistic design, which prioritizes predicting plausible sequences over verifying factual accuracy <d-cite key="radford2019"></d-cite>. This limitation is magnified by two interconnected factors:</p> <p><span style="font-size: 20px; font-weight: bold;">1. Dynamic and Unstructured Contexts</span></p> <p>LLMs are trained on vast corpora of text that often lack semantic organization. Moreover, the facts embedded within these datasets are static—reflecting the state of the world at the time of training. However, real-world knowledge evolves dynamically, leaving models unable to adapt without post-training updates <d-cite key="mousavi2024"></d-cite>.</p> <h4 id="formalizing-the-knowledge-divergence">Formalizing the Knowledge Divergence</h4> <p>Let the model’s knowledge at time \(t\) be denoted as \(K(t)\). We can express the knowledge state as: \(K(t) = \textcolor{#5DADEC}{K(t_0)} + \textcolor{#E67E22}{\Delta K(t)},\) where:</p> <ul> <li>\(\textcolor{#5DADEC}{K(t_0)}\) represents the knowledge at the time of training.</li> <li>\(\textcolor{#E67E22}{\Delta K(t)}\) captures incremental updates post-deployment.</li> </ul> <p>In most deployed LLMs: \(\textcolor{#E67E22}{\Delta K(t)} = 0,\) indicating that the knowledge remains frozen at \(\textcolor{#5DADEC}{K(t_0)}\). Over time, as \(t \to \infty\), the divergence between \(K(t)\) and real-world knowledge widens, resulting in increasingly outdated or inaccurate outputs.</p> <div class="box-important"> <h4 id="real-world-implications">Real-World Implications</h4> <p>This static nature is especially problematic in domains such as:</p> <ul> <li>Medicine, where evolving treatment protocols and research are critical <d-cite key="ji2022"></d-cite>.</li> <li>Law, where changes in regulations or precedents can render older knowledge obsolete.</li> </ul> <p>Without mechanisms to dynamically update \(\Delta K(t)\), LLMs fail to provide accurate, timely information in these high-stakes environments.</p> </div> <p><span style="font-size: 20px; font-weight: bold;">2. Compounded Errors in Reasoning</span></p> <p>In multi-step reasoning, LLMs encounter a <strong>cascading error problem</strong>, where inaccuracies at each step propagate and amplify in subsequent steps <d-cite key="bender2021"></d-cite>.</p> <h4 id="quantifying-error-propagation">Quantifying Error Propagation</h4> <p>Let \(P(x_n)\) denote the probability of correctness at reasoning step \(n\). The overall probability of correctness across \(N\) steps is given by: \(P_{\text{total}} = \prod_{n=1}^N \textcolor{#0f941b}{P(x_n)}.\) Since \(\textcolor{#0f941b}{P(x_n)} &lt; 1\) in most real-world scenarios, \(P_{\text{total}}\) decays exponentially as \(N\) increases. This exponential decay illustrates how longer reasoning chains magnify errors, making LLMs less reliable for complex tasks <d-cite key="wei2022"></d-cite>.</p> <h4 id="modeling-error-interactions">Modeling Error Interactions</h4> <p>Error propagation can also be modeled using second-order interactions through the Hessian matrix <d-cite key="bishop1992"></d-cite>: \(H_{ij} = \frac{\partial^2 \textcolor{#E74C3C}{E}}{\partial x_i \partial x_j}.\)</p> <p>To represent this interaction in matrix form, the Hessian matrix can be visualized as:</p> <div class="matrix-box"> $$ H = \begin{bmatrix} \frac{\partial^2 \textcolor{#E74C3C}{E}}{\partial x_1^2} &amp; \frac{\partial^2 \textcolor{#E74C3C}{E}}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 \textcolor{#E74C3C}{E}}{\partial x_1 \partial x_n} \\ \frac{\partial^2 \textcolor{#E74C3C}{E}}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 \textcolor{#E74C3C}{E}}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 \textcolor{#E74C3C}{E}}{\partial x_2 \partial x_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial^2 \textcolor{#E74C3C}{E}}{\partial x_n \partial x_1} &amp; \frac{\partial^2 \textcolor{#E74C3C}{E}}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2 \textcolor{#E74C3C}{E}}{\partial x_n^2} \end{bmatrix} $$ </div> <p>where:</p> <ul> <li>\(H_{ij}\) quantifies how an error in dimension \(x_i\) interacts with errors in dimension \(x_j\).</li> <li>\(\textcolor{#E74C3C}{E}\) is the error function dependent on reasoning dimensions \(x_i\).</li> </ul> <p>This matrix representation highlights the symmetry of the Hessian (\(H_{ij} = H_{ji}\)), demonstrating how second-order effects capture complex interactions between dimensions. By modeling these interactions, we can better understand how errors propagate and amplify through reasoning pathways. This indicates that inaccuracies are not isolated; instead, they cascade through interconnected reasoning pathways, compounding the overall error and making multi-step reasoning increasingly unreliable in LLMs.</p> <h2 id="limitations-of-existing-approaches">Limitations of Existing Approaches</h2> <p><span style="font-size: 20px; font-weight: bold;">Fine-Tuning</span></p> <figure style="text-align: center;"> <img src="/2025/assets/img/2025-04-28-factual-validation-simplification/finetuning.png"> <figcaption style="font-size: 15px; text-align: left; margin: 0 auto; max-width: 800px;"> <strong>Figure 2.</strong> Workflow showing LLM fine-tuning with domain-specific data, integrating fine-tuned datasets to produce specialized responses. </figcaption> </figure> <p>Fine-tuning involves retraining LLMs on domain-specific datasets, improving their factual accuracy within specific contexts <d-cite key="howard2018"></d-cite>. However, it has inherent trade-offs:</p> <ul> <li>High resource requirements make it impractical for many organizations and individuals <d-cite key="zhang2022"></d-cite>.</li> <li>Static knowledge freezes models in time, necessitating constant retraining to stay relevant <d-cite key="bommasani2021"></d-cite>.</li> <li>Specialization often comes at the expense of general-purpose utility, limiting versatility <d-cite key="ruder2019"></d-cite>.</li> </ul> <p><span style="font-size: 20px; font-weight: bold;">Retrieval-Augmented Generation (RAG)</span></p> <figure style="text-align: center;"> <img src="/2025/assets/img/2025-04-28-factual-validation-simplification/rag.png"> <figcaption style="font-size: 15px; text-align: left; margin: 0 auto; max-width: 800px;"> <strong>Figure 3.</strong> Workflow showing external data retrieval and integration with the LLM to generate grounded responses. </figcaption> </figure> <p>RAG pipelines integrate external knowledge retrieval into the LLM workflow, providing factual grounding by leveraging curated databases or search engines <d-cite key="ibm2023"></d-cite>. While effective in many cases, RAG systems face challenges:</p> <ul> <li>Retrieved information may be incomplete, outdated, or biased, directly affecting output reliability <d-cite key="lewis2020"></d-cite>.</li> <li>RAG systems retrieve information but do not inherently validate it, leaving unresolved conflicts between retrieved facts and the LLM’s internal predictions <d-cite key="karpukhin2020"></d-cite>.</li> <li>As query complexity increases, retrieved information can become overly sparse, and the increasing contextual length can diminish its enhancement for the LLM, ultimately restricting scalability <d-cite key="laban2024"></d-cite>.</li> </ul> <p><span style="font-size: 20px; font-weight: bold;">Post-Hoc Validation</span></p> <figure style="text-align: center;"> <img src="/2025/assets/img/2025-04-28-factual-validation-simplification/posthoc.png"> <figcaption style="font-size: 15px; text-align: left; margin: 0 auto; max-width: 800px;"> <strong>Figure 4.</strong> Workflow showing validation of LLM-generated responses against external datasets, with flagged issues reprocessed for accuracy. </figcaption> </figure> <p>Post-hoc validation attempts to correct inaccuracies after generation by cross-referencing outputs with external datasets or models <d-cite key="zhong2024"></d-cite>. While valuable, it suffers from inefficiencies:</p> <ul> <li>It introduces latency, making it unsuitable for real-time applications.</li> <li>Validating every output, even when unnecessary, wastes computational resources <d-cite key="guu2020"></d-cite>.</li> <li>It provides limited feedback for refining the underlying generative process, addressing symptoms but not root causes <d-cite key="li2024"></d-cite>.</li> </ul> <h2 id="theoretical-foundations">Theoretical Foundations</h2> <p><span style="font-size: 20px; font-weight: bold;">Factual Validation: Core Principles</span></p> <p>Factual validation ensures that LLM outputs are verifiable and contextually accurate. This approach is guided by two main principles:</p> <ul> <li> <p><strong>Granularity</strong>: Responses are decomposed into discrete factual units for precise validation, reducing the likelihood of undetected inaccuracies <d-cite key="maynez2020"></d-cite>.</p> <p>Mathematically, a generated response \(R\) is represented as: \(R = \{\textcolor{#1f77b4}{F_1}, \textcolor{#ff7f0e}{F_2}, \dots, \textcolor{#2ca02c}{F_n}\},\) where each \(F_i\) represents a discrete factual unit, such as a statement or claim.</p> <h4 id="example">Example:</h4> \[R = \{\text{"Paris is the capital of France"}, \text{"The Eiffel Tower is in Paris"}\}.\] <p>Validation is performed by comparing each \(F_i\) against a knowledge base \(K\), which contains validated information:</p> \[K = \{\text{"Paris is the capital of France"}, \text{"The Eiffel Tower is in Paris"}\}.\] <p>A factual unit \(F_i\) is considered valid if it exists in \(K\): \(V(F_i) = \begin{cases} 1 &amp; \text{if } F_i \in K, \\ 0 &amp; \text{otherwise.} \end{cases}\)</p> <p>The overall validation score for the response is calculated as: \(V(R) = \frac{\sum_{i=1}^{n} V(F_i)}{n},\) where \(V(R) \in [0, 1]\) quantifies the proportion of factual units in \(R\) that are verified by \(K\).</p> </li> <li> <p><strong>Scalability</strong>: Validation systems leverage vectorized representations to efficiently compare outputs against large, structured knowledge bases <d-cite key="karpukhin2020"></d-cite>. Instead of exact string matching, factual units \(F_i\) are mapped into a high-dimensional vector space for efficient comparisons.</p> </li> </ul> <p><span style="font-size: 20px; font-weight: bold;">The Role of Vectorization</span></p> <p>Vectorization encodes atomic facts into high-dimensional representations (<strong>embeddings</strong>) that facilitate efficient storage, retrieval, and comparison <d-cite key="johnson2017"></d-cite>.</p> <p>Each factual unit \(F_i\) is transformed into an embedding: \(\mathbf{v}_i = \textcolor{#1f77b4}{\text{Encode}(F_i)},\) where \(\text{Encode}(\cdot)\) maps textual data to numerical vectors.</p> <h4 id="example-1">Example:</h4> <p>\(F_i = \text{"Paris is the capital of France"}\) \(\mathbf{v}_i = [0.12, 0.45, 0.67, \dots, 0.89]\)</p> <p>The knowledge base \(K\) is also represented as a set of embeddings: \(K = \{\mathbf{k}_1, \mathbf{k}_2, \dots, \mathbf{k}_m\}.\)</p> <p>To validate \(F_i\), the similarity between \(\mathbf{v}_i\) and \(\mathbf{k}_j\) is measured using cosine similarity: \(\text{Sim}(\mathbf{v}_i, \mathbf{k}_j) = \frac{\mathbf{v}_i \cdot \mathbf{k}_j}{\|\mathbf{v}_i\| \|\mathbf{k}_j\|}.\)</p> <p><strong>Cosine Similarity</strong>: Determines how closely aligned two vectors are in high-dimensional space (range: -1 to 1):</p> <ul> <li>\(1\): Identical vectors.</li> <li>\(-1\): Completely opposite vectors.</li> </ul> <p>A factual unit \(F_i\) is considered valid if: \(\max_{j} \text{Sim}(\mathbf{v}_i, \mathbf{k}_j) \geq \tau,\) where \(\tau\) is a predefined similarity threshold <d-cite key="reimers2019"></d-cite>. This ensures that validation considers <strong>semantic similarity</strong> rather than exact matches.</p> <p><span style="font-size: 20px; font-weight: bold;">Combining Granularity and Vectorization</span></p> <p>Factual validation combines the precision of granularity with the efficiency of vectorization to determine the validity of responses. The process involves:</p> <ol> <li> <p><strong>Decompose Response</strong>: Break \(R\) into: \(R = \{\textcolor{#1f77b4}{F_1}, \textcolor{#ff7f0e}{F_2}, \dots, \textcolor{#2ca02c}{F_n}\}.\)</p> </li> <li> <p><strong>Encode Units</strong>: Transform each \(F_i\) into its high-dimensional embedding: \(\mathbf{v}_i = \text{Encode}(F_i).\)</p> </li> <li> <p><strong>Compare with Knowledge Base</strong>: Use cosine similarity to validate \(\mathbf{v}_i\) against \(K\): \(\max_{j} \text{Sim}(\mathbf{v}_i, \mathbf{k}_j) \geq \tau.\)</p> </li> </ol> <p>The overall validation score is computed as: \(V(R) = \frac{\sum_{i=1}^{n} \mathbb{1} \left( \max_{j} \text{Sim}(\mathbf{v}_i, \mathbf{k}_j) \geq \tau \right)}{n},\) where \(\mathbb{1}(\cdot)\) is the indicator function that outputs 1 if the condition is satisfied and 0 otherwise.</p> <p><strong>This approach ensures</strong>:</p> <ol> <li> <strong>Precision</strong> at the <strong>component level</strong> (granularity).</li> <li> <strong>Scalability</strong> across <strong>large knowledge bases</strong> (vectorization) <d-cite key="guu2020"></d-cite>.</li> </ol> <h2 id="our-proposal-fact-based-validation-for-gpt-systems">Our Proposal: Fact-Based Validation for GPT Systems</h2> <p>To address the challenges of ensuring factual accuracy in GPT systems, we propose a framework that preprocesses data into structured and verifiable units for use in fact verification with the aim to enhance the reliability of LLM-generated responses while optimizing storage and retrieval efficiency.</p> <figure style="text-align: center;"> <img src="/2025/assets/img/2025-04-28-factual-validation-simplification/methodology.png" style="width: 100%;"> <figcaption style="font-size: 15px; text-align: left; margin: 0 auto; max-width: 800px;"> <strong>Figure 5.</strong> This diagram illustrates the workflow of the LLM Validity Judger. The system fetches relevant facts from a vector database, generates responses based on this context, and simplifies outputs into factual statements. It validates these statements by comparing them with the vector database, generating an overall validity score and explanation. The process repeats with feedback until a valid result is achieved. The diagram also includes the New Data Input System, which independently adds summarized and verified facts into the vector database, ensuring its relevance and accuracy. For clarity, the fetching system mirrors standard vector database retrieval processes. </figcaption> </figure> <p><span style="font-size: 20px; font-weight: bold;">System Architecture</span></p> <ul> <li> <strong>New Data Input System</strong>: Fact Extraction and Storage <ul> <li> <strong>Input</strong>: Multiple text chunks containing factual information.</li> <li> <strong>Output</strong>: Extracted facts embedded as vector representations, stored in a vector database.</li> </ul> </li> <li> <strong>LLM Validity Judger</strong>: Prompt and Response Validation <ul> <li> <strong>Input</strong>: A GPT prompt and its corresponding response.</li> <li> <strong>Output</strong>: An overall validity rating between 0 and 1 indicating the accuracy of the prompt/response pair, along with optional reasoning.</li> </ul> </li> </ul> <h2 id="implementation-details">Implementation Details</h2> <p><span style="font-size: 25px; font-weight: bold;">New Data Input System</span></p> <p><strong>Fetching Contexts</strong>: All contexts are fetched from the PubMedQA dataset to serve as the input data for preprocessing <d-cite key="jin2019"></d-cite>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">qiaojin/PubMedQA</span><span class="sh">"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">pqa_labeled</span><span class="sh">"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">select_columns</span><span class="p">([</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">context</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">pubid</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div> <p><strong>Summarization and Compression</strong>: Each context is summarized using <code class="language-plaintext highlighter-rouge">GPT-4o-mini</code>, compressing the content into concise paragraphs. This step ensures efficient downstream processing and storage <d-cite key="gpt4omini"></d-cite>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">new_contexts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nf">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="k">await</span> <span class="nf">async_gpt_calls</span><span class="p">(</span>
      <span class="p">[</span><span class="nf">get_summarize_prompt</span><span class="p">(</span><span class="n">context</span><span class="p">)</span> <span class="k">for</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">raw_contexts</span><span class="p">]</span>
    <span class="p">)</span>
  <span class="p">]</span>
</code></pre></div></div> <p><strong>Embedding with OpenAI Models</strong>: The summarized contexts are embedded into high-dimensional vector representations using OpenAI’s <code class="language-plaintext highlighter-rouge">text-embedding-3-large</code> model <d-cite key="openaidocs"></d-cite>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="nb">input</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">text-embedding-3-large</span><span class="sh">"</span><span class="p">,</span>
  <span class="p">)</span>
  <span class="k">return</span> <span class="nc">EmbeddingResponse</span><span class="p">(</span>
      <span class="n">result</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">embedding</span><span class="p">,</span>
      <span class="n">result</span><span class="p">.</span><span class="n">model</span><span class="p">,</span>
      <span class="n">result</span><span class="p">.</span><span class="n">usage</span><span class="p">,</span>
  <span class="p">)</span>
</code></pre></div></div> <p><strong>Clustering with DBSCAN</strong>: The embeddings are grouped into clusters using DBSCAN, which identifies similar contexts without requiring predefined categories. This step is critical for organizing related data efficiently <d-cite key="ester1996"></d-cite>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">cluster</span><span class="p">(</span><span class="n">vectors</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Vector</span><span class="p">],</span> <span class="n">eps</span><span class="o">=</span><span class="mf">45.0</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">().</span><span class="nf">fit_transform</span><span class="p">(</span>
      <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">vector</span><span class="p">[</span><span class="sh">"</span><span class="s">vector</span><span class="sh">"</span><span class="p">]</span> <span class="k">for</span> <span class="n">vector</span> <span class="ow">in</span> <span class="n">vectors</span><span class="p">])</span>
    <span class="p">)</span>
    <span class="n">db</span> <span class="o">=</span> <span class="nc">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="n">min_samples</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="sh">"</span><span class="s">euclidean</span><span class="sh">"</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">clusters</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">item</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">db</span><span class="p">.</span><span class="n">labels_</span><span class="p">):</span>
      <span class="n">clusters</span><span class="p">.</span><span class="nf">setdefault</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="p">[]).</span><span class="nf">append</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">vectors</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">vectors</span> <span class="ow">in</span> <span class="n">clusters</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">label</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div> <p><strong>Context Combination and Summarization</strong>: Context summaries within each cluster are combined and re-summarized into unified representations using <code class="language-plaintext highlighter-rouge">GPT-4o-mini</code>, further reducing redundancy and optimizing coherence <d-cite key="gpt4omini"></d-cite>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">combine_and_summarize</span><span class="p">(</span><span class="n">cluster</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
    <span class="n">combined_context</span> <span class="o">=</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>
    <span class="k">return</span> <span class="nf">summarize_context</span><span class="p">(</span><span class="n">combined_context</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Final Embedding and Storage</strong>: The new summaries are re-embedded with <code class="language-plaintext highlighter-rouge">text-embedding-3-large</code> <d-cite key="openaidocs"></d-cite>. These embeddings, along with any unclustered individual summaries, are stored in PineconeDB for retrieval <d-cite key="pinecone"></d-cite>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">vectors</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">:</span> <span class="nb">id</span><span class="p">,</span> <span class="sh">"</span><span class="s">values</span><span class="sh">"</span><span class="p">:</span> <span class="n">embedding</span><span class="p">.</span><span class="n">vector</span><span class="p">,</span> <span class="sh">"</span><span class="s">metadata</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">content</span><span class="p">}}</span>
    <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">content</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">contexts</span><span class="p">)</span>
  <span class="p">]</span>
  <span class="n">index</span><span class="p">.</span><span class="nf">upsert</span><span class="p">(</span><span class="n">vectors</span><span class="o">=</span><span class="n">vectors</span><span class="p">,</span> <span class="n">namespace</span><span class="o">=</span><span class="sh">"</span><span class="s">pubmed_summarized</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><span style="font-size: 25px; font-weight: bold;">LLM Validity Judger</span></p> <p><strong>Decomposing Responses</strong>: GPT processes the prompt and its corresponding response, breaking them into discrete factual statements. Each statement is assigned an importance score (0–1), indicating its relevance to the prompt.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">decompose_response</span><span class="p">(</span><span class="n">response</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]:</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[{</span><span class="sh">"</span><span class="s">statement</span><span class="sh">"</span><span class="p">:</span> <span class="n">s</span><span class="p">.</span><span class="nf">strip</span><span class="p">(),</span> <span class="sh">"</span><span class="s">importance</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span> <span class="k">if</span> <span class="n">s</span><span class="p">.</span><span class="nf">strip</span><span class="p">()]</span>
</code></pre></div></div> <p><strong>Proximity Search</strong>: For each statement, the system retrieves nearby facts from the vector database through a semantic proximity search <d-cite key="johnson2017"></d-cite>. These facts serve as the context for validation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">proximity_search</span><span class="p">(</span><span class="n">statement_embedding</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">vector</span><span class="o">=</span><span class="n">statement_embedding</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span><span class="p">.</span><span class="n">matches</span>
</code></pre></div></div> <p><strong>Validity Scoring</strong>: Each statement is evaluated against its contextual facts using GPT, which assigns a validity score between 0 and 1 based on alignment. This evaluation determines how well the statement aligns with retrieved facts <d-cite key="kryscinski2019"></d-cite>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">evaluate_validity</span><span class="p">(</span><span class="n">statement</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Evaluate: </span><span class="sh">'</span><span class="si">{</span><span class="n">statement</span><span class="si">}</span><span class="sh">'</span><span class="s"> based on: </span><span class="sh">'</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="sh">'</span><span class="s">. Assign a validity score (0-1).</span><span class="sh">"</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o-mini</span><span class="sh">"</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="nf">float</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span><span class="p">.</span><span class="nf">strip</span><span class="p">())</span>
</code></pre></div></div> <p><strong>Weighted Average Calculation</strong>: The validity scores are aggregated into a weighted average, where the weights are derived from the importance scores of the statements. This weighted score represents the overall factual accuracy of the response.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">calculate_weighted_average</span><span class="p">(</span><span class="n">scores</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">weights</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]):</span>
    <span class="k">return</span> <span class="nf">sum</span><span class="p">(</span><span class="n">s</span> <span class="o">*</span> <span class="n">w</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span> <span class="o">/</span> <span class="nf">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Explainability</strong>: If required, GPT generates a rationale for the validity rating, outlining which facts were used, how alignment was determined, and any potential conflicts. This reasoning enhances interpretability and transparency.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">generate_explanation</span><span class="p">(</span><span class="n">statement</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Explain validity of: </span><span class="sh">'</span><span class="si">{</span><span class="n">statement</span><span class="si">}</span><span class="sh">'</span><span class="s"> in context: </span><span class="sh">'</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="sh">'"</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o-mini</span><span class="sh">"</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span>
</code></pre></div></div> <div class="box-two"> <h4 id="advantages-of-this-system">Advantages of This System</h4> <p>The proposed system introduces several features that enhance the reliability and adaptability of existing RAG pipelines:</p> <ul> <li> <p><strong>Innovative Design</strong>: By integrating techniques like LLM summarization, DBSCAN clustering, and vectorized fact storage, the system introduces a new framework for precise fact validation and efficient data storage reduction.</p> </li> <li> <p><strong>Streamlined Architecture</strong>: The system’s design supports simple integration with existing RAG systems while reducing storage needs and retrieval token usage. This adaptability ensures suitability and long-term applicability for large-scale applications across various domains.</p> </li> <li> <p><strong>Granular Fact Verification</strong>: By decomposing responses into discrete factual units and validating each against a vectorized knowledge base, the system achieves significantly higher precision compared to traditional document- or paragraph-level validation methods, reducing inaccuracies and improving trustworthiness.</p> </li> </ul> </div> <p><span style="font-size: 20px; font-weight: bold;">Why Not KNN for Clustering?</span></p> <p>K-nearest neighbor (KNN) was considered for fact clustering but was ultimately not used due to its reliance on predefined parameters, such as the number of clusters or neighbors—constraints that are impractical for real-world datasets with high diversity <d-cite key="taunk2019"></d-cite>. In contrast, DBSCAN identifies clusters based on density, eliminating the need for prior knowledge of the dataset’s structure <d-cite key="ester1996"></d-cite>. This makes DBSCAN more effective for processing unstructured or unknown data distributions. Additionally, KNN’s approach risks significant data abstraction or loss when attempting to condense diverse facts into singular representations, worsening the system’s ability to maintain factual granularity <d-cite key="cunningham2020"></d-cite>.</p> <p><span style="font-size: 20px; font-weight: bold;">Additional Note</span></p> <p>If the dataset is preprocessed to isolate one specific fact per entry, we recommend bypassing that step. Instead, the data can directly enter our model, which dynamically groups related facts. This approach preserves the richness of the dataset and streamlines the overall workflow.</p> <h2 id="experimentation-and-results">Experimentation and Results</h2> <p>We evaluated our proposed pipeline by benchmarking it against a <strong>Traditional Retrieval-Augmented Generation (RAG) Pipeline</strong> using the PubMedQA dataset <d-cite key="jin2019"></d-cite>. The evaluation focused on three key metrics: <strong>factual accuracy, RAG effectiveness,</strong> and <strong>storage efficiency,</strong> collectively measuring response quality, retrieval precision, and data storage optimization.</p> <p><span style="font-size: 20px; font-weight: bold;">Traditional RAG Pipeline</span></p> <p>The traditional RAG pipeline was tested under ideal conditions, embedding and retrieving the labeled contexts directly from the PubMedQA dataset <d-cite key="jin2019"></d-cite>. This setup provided perfect access to the correct answer contexts, offering a significant advantage. Despite this, our proposed pipeline—which applies summarization and compression—demonstrates performance comparable to this baseline, emphasizing its effectiveness.</p> <p>The traditional pipeline workflow includes the following steps:</p> <ul> <li>Embed each context using OpenAI’s <code class="language-plaintext highlighter-rouge">text-embedding-3-large</code> model.</li> <li>Store the embeddings in PineconeDB for retrieval.</li> </ul> <p><span style="font-size: 20px; font-weight: bold;">Comparative Metrics</span></p> <table> <thead> <tr> <th><strong>Metric</strong></th> <th><strong>Traditional Pipeline</strong></th> <th><strong>Proposed Pipeline</strong></th> <th><strong>Difference</strong></th> </tr> </thead> <tbody> <tr> <td>Factual Accuracy</td> <td><strong>71.7%</strong></td> <td>71.2%</td> <td>-0.5%</td> </tr> <tr> <td>RAG Effectiveness</td> <td><strong>99.2%</strong></td> <td>98.9%</td> <td>-0.3%</td> </tr> <tr> <td>Storage Efficiency</td> <td>1,351 KB</td> <td><strong>571 KB</strong></td> <td>-57.7% (Reduction)</td> </tr> </tbody> </table> <p><strong>Table 1.</strong> Comparison of factual accuracy, RAG effectiveness, and storage efficiency between the traditional pipeline and the proposed pipeline. The proposed pipeline achieves comparable performance in accuracy and effectiveness while significantly reducing storage requirements by 57.7%.</p> <p><span style="font-size: 20px; font-weight: bold;">Factual Accuracy</span></p> <p>We tested factual accuracy to measure how well the system addressed prompts by integrating all processes, including context retrieval, summarization, and LLM response generation. Using the PubMedQA dataset with 1,000 labeled context-question-answer groups, we queried each question and deemed responses correct if they matched the dataset’s answer (yes, no, or maybe) <d-cite key="jin2019"></d-cite>. The traditional pipeline achieved 71.7% accuracy, while our pipeline achieved 71.2%, a negligible difference of 0.5%. This suggests that summarizing contexts did not hinder the LLM’s ability to generate correct answers. Further improvements in prompt engineering or summary generation could potentially surpass the traditional pipeline’s accuracy.</p> <p><span style="font-size: 20px; font-weight: bold;">RAG Effectiveness</span></p> <p>RAG effectiveness was evaluated to determine how well each pipeline retrieved the most relevant context for a given query. The PubMedQA dataset contexts were queried using labeled questions, and a retrieval was marked correct if the top result matched the labeled correct context <d-cite key="jin2019"></d-cite>. The traditional pipeline achieved 99.2% RAG effectiveness, while our pipeline achieved 98.9%, a minor 0.3% difference. The minimal reduction shows that summarization and clustering do not compromise retrieval quality. Additionally, granting the proposed pipeline access to original embeddings instead of summaries for labeling could eliminate this gap entirely, further reinforcing its effectiveness.</p> <p><span style="font-size: 20px; font-weight: bold;">Storage Efficiency</span></p> <p>We measured storage efficiency by calculating the total size of stored contexts in the vector database (excluding vector embeddings). The traditional pipeline required 1,351 KB, whereas our pipeline used only 571 KB, a reduction of 57.7%. This demonstrates the significant compression achieved through summarization and clustering. The benefits are particularly pronounced in unstructured datasets, where redundant data can be consolidated. While PubMedQA is a structured dataset, in more diverse real-world datasets, the proposed pipeline would likely achieve even greater storage savings. This reduced footprint allows for the storage of larger datasets and faster query times, providing scalability without sacrificing usability.</p> <p><span style="font-size: 20px; font-weight: bold;">Validity Judgment with Chain of Thought</span></p> <p>In addition to the metrics discussed above, we evaluated the integration of a validity judgment mechanism in both the traditional and proposed pipelines. The PubMedQA dataset, designed for direct question-answer accuracy, presents inherent limitations for chain-of-thought reasoning <d-cite key="jin2019"></d-cite>. Despite this, the validity judger paired with the traditional pipeline achieved a factual accuracy of 72.4%, demonstrating its effectiveness in assessing and verifying LLM responses.</p> <p>When integrated into the proposed pipeline, the accuracy decreased to 68.9%, showcasing areas for refinement in our system rather than inherent shortcomings in the validity judger. This drop is attributed to two primary reasons:</p> <ul> <li> <strong>Skewed Scoring Distribution</strong>: Validity and importance scores were overly concentrated near 1, limiting their ability to distinguish subtle differences in factual accuracy.</li> <li> <strong>Excessive Context Retrieval</strong>: The RAG system retrieved more context than necessary, introducing noise to the validation process.</li> </ul> <p>While the PubMedQA dataset is not well-suited for chain-of-thought reasoning, the validity judger still demonstrated its capability to decompose LLM responses into granular statements and retrieve relevant contexts for validation. These findings demonstrate its potential as a strong component for factual validation, particularly on tasks requiring multi-step reasoning. Addressing scoring and retrieval challenges, along with testing on more reasoning-intensive datasets, will likely see significant improvements in both accuracy and efficiency.</p> <h2 id="alternative-solution-statement-extraction">Alternative Solution: Statement Extraction</h2> <p>One of the main limitations of the proposed framework (summarization-based) is the potential loss of important contextual details—particularly concerning in high-stakes domains such as finance, law, or medicine. To mitigate this risk and retain comprehensive coverage of critical information, we introduce an Alternative Pipeline that avoids excessive compression by converting the input data into standalone, verifiable statements. When an input passage can’t safely be condensed, the system simply reproduces statements identical to the source text, thus ensuring no crucial content is omitted. Rather than aiming for a prescribed compression metric, the pipeline focuses on statement-level extraction, preserving essential facts while eliminating redundancies and inaccuracies.</p> <figure style="text-align: center;"> <img src="/2025/assets/img/2025-04-28-factual-validation-simplification/alternative.png" style="width: 75%;"> <figcaption style="font-size: 15px; text-align: left; margin: 0 auto; max-width: 800px;"> <strong>Figure 6.</strong> This diagram illustrates the workflow of the Statement Extraction pipeline, which adds extracted and compressed statements into a vector database. When data is added to the system, the text is converted into statements, clustered and combined based on similarity, embedded, and entered into a vector database. When data is fetched from the system, it fetches the closest embeddings and statements to the query embedding and determines their sufficiency in answering the prompt. If the context is insufficient, it creates new queries based on the information it needs and repeats the process until it has enough information. Then, it returns the statements that it fetched. The boxes highlighted with blue dashed borders indicate modifications to the summarization-based pipeline. </figcaption> </figure> <h2 id="extended-benchmarks">Extended Benchmarks</h2> <p>To evaluate this alternative pipeline, experiments were conducted on two widely used QA datasets: Stanford Question Answering Dataset (SQuAD) <d-cite key="rajpurkar2016"></d-cite> and HotpotQA <d-cite key="yang2018"></d-cite>. We compare factual accuracy and storage size between this Alternative Pipeline and the Traditional Pipeline that stores and retrieves paragraph-level contexts under ideal conditions with RAG.</p> <h4 id="squad">SQuAD</h4> <table> <thead> <tr> <th><strong>Metric</strong></th> <th><strong>Traditional Pipeline</strong></th> <th><strong>Statement Extraction</strong></th> <th><strong>Difference</strong></th> </tr> </thead> <tbody> <tr> <td>Factual Accuracy</td> <td>87.3%</td> <td><strong>89.7%</strong></td> <td>+2.4%</td> </tr> <tr> <td>Storage Size</td> <td>1.4 MB</td> <td><strong>1.1 MB</strong></td> <td>-21.43%</td> </tr> </tbody> </table> <p><strong>Table 2.</strong> On the SQuAD dataset, statement extraction yields a 2.4% increase in factual accuracy while reducing storage size by 21.43%.</p> <h4 id="hotpotqa">HotpotQA</h4> <table> <thead> <tr> <th><strong>Metric</strong></th> <th><strong>Traditional Pipeline</strong></th> <th><strong>Statement Extraction</strong></th> <th><strong>Difference</strong></th> </tr> </thead> <tbody> <tr> <td>Factual Accuracy</td> <td>92.0%</td> <td><strong>93.3%</strong></td> <td>+1.3%</td> </tr> <tr> <td>Storage Size</td> <td>763 KB</td> <td><strong>701 KB</strong></td> <td>-8.12%</td> </tr> </tbody> </table> <p><strong>Table 3.</strong> On the HotpotQA dataset, the statement extraction pipeline achieves a 1.3% higher accuracy while reducing storage by 8.12%. Though less significant than SQuAD, the improvement demonstrates that statement-level coverage can enhance reliability even in multi-hop question answering tasks.</p> <p>Notably, in comparison to our proposed pipeline, the storage savings here are not as large. However, this Alternative Pipeline ensures no critical details are inadvertently removed. Just as with the proposed approach, it integrates easily into existing RAG setups, requiring minimal computational overhead. DBSCAN and cosine similarity are relatively lightweight, and the LLM calls typically cost only around $0.01 per API call.</p> <h2 id="realistic-usage">Realistic Usage</h2> <p>Most real-world data comes from unstructured sources, including blog posts, news reports, or technical articles. Below, we demonstrate a small-scale test with two AI-related news articles—one from Google DeepMind <d-cite key="deepmind"></d-cite> and another from Wired <d-cite key="wired"></d-cite>. Both discuss the Gemini 2.0 model releases, showcasing how statement extraction saves storage while retaining necessary context.</p> <table> <thead> <tr> <th><strong>Metric</strong></th> <th><strong>Google Deepmind</strong></th> <th><strong>Wired Article</strong></th> <th><strong>Combination</strong></th> </tr> </thead> <tbody> <tr> <td>Raw Size</td> <td>3.0 KB</td> <td>5.4 KB</td> <td>8.4 KB</td> </tr> <tr> <td>Statement Extraction Size</td> <td><strong>2.2 KB</strong></td> <td><strong>2.3 KB</strong></td> <td><strong>3.3 KB</strong></td> </tr> <tr> <td>Percentage Reduction</td> <td>26.67%</td> <td>57.41%</td> <td>60.71%</td> </tr> </tbody> </table> <p><strong>Table 4.</strong> Storage sizes before and after statement extraction. Combining both articles after statement extraction yields a 3.3 KB storage size, significantly smaller than the raw 8.4 KB.</p> <p>Through DBSCAN clustering, near-duplicate statements—such as multiple introductions of “Gemini 2.0” from different perspectives—are merged into a single statement without risking the loss of important data. In high-stakes settings, this pipeline can similarly maintain critical details (e.g., disclaimers, side effects, or legal clauses). Consequently, while it may not compress as aggressively as the proposed pipeline, the Alternative Pipeline provides stronger guarantees of context sensitivity and factual completeness.</p> <h2 id="broader-implications">Broader Implications</h2> <p>The results of these two pipelines highlight their potential to improve how RAG systems handle unstructured and large-scale datasets. By either compressing data via summarization or preserving complete details through statement extraction, it expands the capacity of vector databases, allowing systems to manage larger and more diverse datasets without sacrificing query performance. Such scalability is critical for real-world applications and for increasing context windows, enabling more complex multi-step reasoning tasks and better contextual understanding <d-cite key="yao2024"></d-cite>.</p> <p>Furthermore, by leveraging modular components like clustering and vectorized storage, it enables future systems to integrate real-time updates, ensuring that models stay relevant as knowledge evolves without requiring full retraining. The design also ensures seamless integration with existing RAG and GPT systems, supporting easy implementation while allowing for domain-specific customization. As a result, organizations and researchers can tailor these methods to build specialized QA systems, bias detection tools, or other applications where data quality is essential.</p> <h2 id="limitations-and-future-work">Limitations and Future Work</h2> <ol> <li> <strong>Suboptimal Fit</strong>: <ul> <li>The data retrieved may not always align perfectly with the prompt, leading to potential mismatches in utility.</li> <li> <strong>Mitigation</strong>: Introduce additional retrieval steps that identify and remove irrelevant facts, guided by LLM evaluations.</li> </ul> </li> <li> <strong>Context Relevancy</strong>: <ul> <li>Statements sometimes refer to their original context, making them unhelpful when used by themselves.</li> <li> <strong>Mitigation</strong>: Better prompt engineering for statement generation or additional agents to ensure statements are standalone before entry into the database.</li> </ul> </li> <li> <strong>Potential Source Bias</strong>: <ul> <li>Bias in the original source documents can propagate into the system, affecting the impartiality of responses.</li> <li> <strong>Mitigation</strong>: Increase the number of trusted source documents to prevent bias by having a higher quantity of unbiased statements overrule the lower quantity of biased statements.</li> </ul> </li> </ol> <div class="box-future"> <h4 id="future-direction-concept-based-fact-representation">Future Direction: Concept-Based Fact Representation</h4> <p>Building on the strengths of the current system, a future enhancement could involve representing facts as structured relationships between concepts (e.g., <code class="language-plaintext highlighter-rouge">CONCEPT 1, CONNECTION, CONCEPT 2</code>). By storing concepts as Word2Vec embeddings and dynamically mapping their relationships in a database, this approach could enable more granular validation and reasoning <d-cite key="mikolov2013"></d-cite>. This would extend our existing pipeline by not only validating individual facts but also examining their interconnectedness, facilitating a richer understanding of context and relationships.</p> <p>The potential benefits of this approach include:</p> <ul> <li> <strong>Improved Contextual Validation</strong>: Exploring relationships between concepts could strengthen the system’s ability to handle complex queries, reducing the risk of context misalignment and improving factual accuracy.</li> <li> <strong>Enhanced Scalability</strong>: Dynamically storing concepts and their connections would allow the system to adapt to new knowledge over time, enhancing long-term usability and robustness.</li> <li> <strong>Greater Explainability</strong>: Converting retrieved connections into human-readable formats could improve transparency, offering clearer and more interpretable rationales for decisions made by the system.</li> </ul> <p>While this concept represents a forward-looking direction, it builds naturally on the goals of the current system. By enhancing scalability, reliability, and transparency, it offers a pathway for advancing the framework in future iterations.</p> </div> <h2 id="reproducibility">Reproducibility</h2> <p>We have open-sourced all code for our framework at <a href="https://github.com/Tonyhrule/Factual-Validation-Simplification" rel="external nofollow noopener noopener noreferrer" target="_blank">Tonyhrule/Factual-Validation-Simplification</a>. The repository contains complete scripts and instructions for replicating the results from this work. We hope this open-source release facilitates public auditing, broader adoption of our methods, encourages community-driven improvements, and enables new research on trustworthy LLMs.</p> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, we explored two approaches to improving the scalability and reliability of RAG pipelines by addressing challenges in storage efficiency and factual accuracy. The first method uses summarization, clustering, and vectorized fact storage to optimize data size while maintaining strong retrieval performance. The second method—the Alternative Pipeline—avoids summarization and instead extracts standalone statements, offering higher fidelity in high-stakes domains where preserving every nuance of context is critical.</p> <p>Through experimentation with PubMedQA, SQuAD, HotpotQA, the pipelines demonstrated competitive performance relative to traditional methods. The summarization-based pipeline excels in reducing storage space, while statement extraction preserves sensitive details, improving factual accuracy and removing redundant information. Looking ahead, by grounding outputs in verified information and ensuring greater transparency in decision-making, this framework takes a step forward in building trustworthy and explainable AI systems.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2025/assets/bibliography/2025-04-28-factual-validation-simplification.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>