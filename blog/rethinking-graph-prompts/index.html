<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,a=e=>{let t=e.split(" "),a=t.slice(0,-1).join(" ");return[t.at(-1),a]},n=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(a),i=n[0][0],r=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"April 28, 2025"),o="Rethinking Graph Prompts: Unraveling the Power of Data Manipulation in Graph Neural Networks",l="Graph Neural Networks (GNNs) have transformed graph learning but face challenges like distribution shifts, data anomalies, and adversarial vulnerabilities. Graph prompt emerges as a novel solution, enabling data transformation to align graph data with pre-trained models without altering model parameters. This paradigm addresses negative transfer, enhances adaptability, and bridges modality gaps. Unlike traditional fine-tuning, graph prompts rewrite graph structures and features through components like prompt tokens and insertion patterns, improving flexibility and efficiency. Applications in IoT, drug discovery, fraud detection, and personalized learning demonstrate their potential to dynamically adapt graph data. While promising, challenges such as optimal design, benchmarks, and gradient issues persist. Addressing these will unlock  full potential of graph prompt to advance GNNs for complex real-world tasks.";{let e=n.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(i+"2025"+o.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${o}},\n  abstract = {${l}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${r}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=n.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${o}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Rethinking Graph Prompts: Unraveling the Power of Data Manipulation in Graph Neural Networks | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Graph Neural Networks (GNNs) have transformed graph learning but face challenges like distribution shifts, data anomalies, and adversarial vulnerabilities. Graph prompt emerges as a novel solution, enabling data transformation to align graph data with pre-trained models without altering model parameters. This paradigm addresses negative transfer, enhances adaptability, and bridges modality gaps. Unlike traditional fine-tuning, graph prompts rewrite graph structures and features through components like prompt tokens and insertion patterns, improving flexibility and efficiency. Applications in IoT, drug discovery, fraud detection, and personalized learning demonstrate their potential to dynamically adapt graph data. While promising, challenges such as optimal design, benchmarks, and gradient issues persist. Addressing these will unlock full potential of graph prompt to advance GNNs for complex real-world tasks."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/2025/assets/css/main.css"> <link rel="canonical" href="https://starlight345.github.io/2025/blog/rethinking-graph-prompts/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/2025/assets/js/theme.js"></script> <script src="/2025/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2025/assets/js/distillpub/template.v2.js"></script> <script src="/2025/assets/js/distillpub/transforms.v2.js"></script> <script src="/2025/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <d-front-matter> <script async type="text/json">{
      "title": "Rethinking Graph Prompts: Unraveling the Power of Data Manipulation in Graph Neural Networks",
      "description": "Graph Neural Networks (GNNs) have transformed graph learning but face challenges like distribution shifts, data anomalies, and adversarial vulnerabilities. Graph prompt emerges as a novel solution, enabling data transformation to align graph data with pre-trained models without altering model parameters. This paradigm addresses negative transfer, enhances adaptability, and bridges modality gaps. Unlike traditional fine-tuning, graph prompts rewrite graph structures and features through components like prompt tokens and insertion patterns, improving flexibility and efficiency. Applications in IoT, drug discovery, fraud detection, and personalized learning demonstrate their potential to dynamically adapt graph data. While promising, challenges such as optimal design, benchmarks, and gradient issues persist. Addressing these will unlock  full potential of graph prompt to advance GNNs for complex real-world tasks.",
      "published": "April 28, 2025",
      "authors": [
        {
          "author": "Chenyi Zi",
          "authorURL": "https://barristen.github.io/",
          "affiliations": [
            {
              "name": "Hong Kong University of Science and Technology (Guangzhou)",
              "url": ""
            }
          ]
        },
        {
          "author": "Bowen LIU",
          "authorURL": "https://adastraabyssoque.github.io/",
          "affiliations": [
            {
              "name": "Hong Kong University of Science and Technology (Guangzhou)",
              "url": ""
            }
          ]
        },
        {
          "author": "Xiangguo Sun",
          "authorURL": "https://xgsun.mysxl.cn/",
          "affiliations": [
            {
              "name": "The Chinese University of Hong Kong",
              "url": ""
            }
          ]
        },
        {
          "author": "Hong Cheng",
          "authorURL": "https://www.se.cuhk.edu.hk/people/academic-staff/prof-cheng-hong/",
          "affiliations": [
            {
              "name": "The Chinese University of Hong Kong",
              "url": ""
            }
          ]
        },
        {
          "author": "Jia Li",
          "authorURL": "https://sites.google.com/view/lijia",
          "affiliations": [
            {
              "name": "Hong Kong University of Science and Technology (Guangzhou)",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/reviewing/">reviewing</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Rethinking Graph Prompts: Unraveling the Power of Data Manipulation in Graph Neural Networks</h1> <p>Graph Neural Networks (GNNs) have transformed graph learning but face challenges like distribution shifts, data anomalies, and adversarial vulnerabilities. Graph prompt emerges as a novel solution, enabling data transformation to align graph data with pre-trained models without altering model parameters. This paradigm addresses negative transfer, enhances adaptability, and bridges modality gaps. Unlike traditional fine-tuning, graph prompts rewrite graph structures and features through components like prompt tokens and insertion patterns, improving flexibility and efficiency. Applications in IoT, drug discovery, fraud detection, and personalized learning demonstrate their potential to dynamically adapt graph data. While promising, challenges such as optimal design, benchmarks, and gradient issues persist. Addressing these will unlock full potential of graph prompt to advance GNNs for complex real-world tasks.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#origin-of-graph-prompts">Origin of Graph Prompts</a></div> <div><a href="#preliminaries-and-definitions">Preliminaries and Definitions</a></div> <div><a href="#the-true-nature-of-graph-prompts">The True Nature of Graph Prompts</a></div> <div><a href="#understanding-how-graph-prompt-manipulate-data">Understanding How Graph Prompt Manipulate Data</a></div> <div><a href="#why-data-manipulation-is-crucial-in-graph-learning">Why Data Manipulation Is Crucial in Graph Learning</a></div> <ul> <li><a href="#bridging-the-expressiveness-flexibility-trade-off">Bridging the Expressiveness-Flexibility Trade-off</a></li> <li><a href="#efficiency-in-model-adaptation">Efficiency in Model Adaptation</a></li> <li><a href="#incorporating-multi-modal-and-cross-domain-knowledge">Incorporating Multi-modal and Cross-domain Knowledge</a></li> </ul> <div><a href="#graph-prompts-vs-nlp-prompts-a-fundamental-difference">Graph Prompts vs. NLP Prompts: A Fundamental Difference</a></div> <ul> <li><a href="#gpf-like-graph-prompt-and-soft-prompt-in-nlp">GPF-like Graph Prompt and Soft Prompt in NLP</a></li> <li><a href="#all-in-one-like-graph-prompt-and-hard-prompt-in-nlp">All-in-One-like Graph Prompt and Hard Prompt in NLP</a></li> </ul> <div><a href="#current-challenges-in-graph-prompt-research">Current Challenges in Graph Prompt Research</a></div> <ul> <li><a href="#limited-understanding-of-prompt-design">Limited Understanding of Prompt Design</a></li> <li><a href="#limited-standardized-benchmarks">Limited Standardized Benchmarks</a></li> <li><a href="#vanishing-gradient-in-the-graph-prompt">Vanishing Gradient in the graph prompt</a></li> </ul> <div><a href="#applications-and-future-directions">Applications and Future Directions</a></div> <ul> <li><a href="#real-time-network">Real-Time Network</a></li> <li><a href="#drug-discovery">Drug Discovery</a></li> <li><a href="#financial-networks">Financial Networks</a></li> <li><a href="#knowledge-graphs">Knowledge Graphs</a></li> </ul> <div><a href="#conclusion">Conclusion</a></div> <div><a href="#references">References</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-rethinking-graph-prompts/figure2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-rethinking-graph-prompts/figure2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-rethinking-graph-prompts/figure2-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-rethinking-graph-prompts/figure2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p align="center" style="color:gray;"> <em>Figure 1: An Iceberg towards Graph AGI</em> </p> <p>Graph Neural Networks (GNNs) have played a crucial role in advancing Graph Artificial General Intelligence (AGI) in the past decade, unlocking new possibilities in understanding and processing complex, interconnected data. As powerful tools for representation learning on graphs, GNNs have revolutionized a wide range of applications from social analysis to knowledge graph. However, despite their success, GNNs still face significant challenges that hinder their full potential towards Graph AGI:</p> <ul> <li> <strong>Distribution Shifts</strong>: Graph data often vary significantly in structure and features across tasks or domains, making it difficult to generalize pre-trained models.</li> <li> <strong>Data Anomalies</strong>: Missing, noisy, or incomplete graph data can severely degrade model performance, limiting the reliability of downstream applications.</li> <li> <strong>Adversarial Vulnerabilities</strong>: Graphs are susceptible to adversarial attacks that manipulate their structure or features, exposing weaknesses in model robustness.</li> </ul> <p>When we envision Graph AGI as an iceberg, the part above the surface represents the visible components such as model architectures, training tricks, fine-tuning methods, and computational capabilities. These are the “model-level” operations—the more apparent and well-studied aspects of AI. However, these visible elements are merely the tip of the iceberg. Below the surface lies the true foundation that supports the progress of Graph AGI: vast amounts of data. Historically, handling data has been a manual, labor-intensive process. Tasks like data cleaning, selection, and curation have relied heavily on human effort, and even today, much of this work remains unsophisticated. While we’ve developed countless techniques and “tricks” at the model level, our operations at the data level seem stuck in a primitive era.</p> <p>In response to these challenges, graph prompt have recently garnered renewed attention. Traditionally viewed as an extension of the prompting paradigm in NLP, their potential for graph data extends far beyond fine-tuning. <strong>In this blog, we argue that the true strength of graph prompts lies in their capacity to effectively manipulate and adapt graph data.</strong></p> <p>By adopting graph prompts as a data manipulation framework, the graph learning community can address long-standing limitations of GNNs, paving the way for deeper exploration and innovation. This approach holds the promise of enabling a new generation of robust, adaptable, and efficient solutions, ready to tackle the complexities of real-world graph data.</p> <h2 id="origin-of-graph-prompts">Origin of Graph Prompts</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-rethinking-graph-prompts/figure3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-rethinking-graph-prompts/figure3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-rethinking-graph-prompts/figure3-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-rethinking-graph-prompts/figure3.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p align="center" style="color:gray;"> <em>Figure 2: Comparison between `Pre-train &amp; Fine-tune` and `Pre-train &amp; Prompting` paradigms</em> </p> <p>While the <code class="language-plaintext highlighter-rouge">Pre-train &amp; Fine-tune</code> paradigm has achieved remarkable success in graph learning like GraphMAE and GraphCL, it faces significant challenges, including large-scale parameter tuning and time-intensive optimization. Moreover, <strong>negative transfer</strong> remains a persistent issue. Unlike images, words, or sentences that often share extensive underlying semantics across datasets, graph data tend to be more abstract, making it harder to identify low-level common semantics. This abstraction often leads to negative transfer in graph learning.</p> <p>To overcome these limitations, recent studies have proposed the <code class="language-plaintext highlighter-rouge">Pre-train &amp; Prompting</code> paradigm. This innovative approach manipulates downstream data by inserting a small prompt graph or learnable vector, and reformulating the downstream task to align with the pre-trained GNN model, all without altering the model’s pre-trained parameters. This method has demonstrated impressive results across various domains, including drug design, protein prediction, social network analysis, and dynamic graph processing.</p> <h2 id="preliminaries-and-definitions">Preliminaries and Definitions</h2> <p>Let $\mathcal{G}$ be a graph dataset and each graph $G=(\mathcal{V}, \mathcal{E}, \mathbf{X}, \mathbf{A})\in \mathcal{G}$ where $\mathcal{V}=\left{v_1, v_2, \ldots, v_N\right}$ denotes the node set with a node feature matrix $\mathbf{X}=\left{x_1, x_2, \ldots, x_N\right} \in \mathbb{R}^{N \times F} ; \mathcal{E}$ denotes the edge set and the connection of nodes can be further indicated by the adjacent matrix $\mathbf{A} \in{0,1}^{N \times N}$. Concretely, the purpose of graph prompt is to manipulate downstream graph data to align them to upstream tasks, guaranteeing the pre-trained knowledge transfer. The smallest manipulation units of a graph prompt module can be divided into three components:</p> <ul> <li> <strong>Prompt tokens</strong>: Typically refer to the additional (learnable) vectors or features that are appended to the original graph data (e.g., added to node features or representing specific information relevant to the downstream task)ss. These tokens are the key unit that determines the basic manipulations.</li> <li> <strong>Token structure</strong>: Describes how multiple prompt tokens are organized and interrelated. It aims to introduce additional relational patterns to the original graphs based on the prompt tokens.</li> <li> <strong>Insert patterns</strong>: Define how the token structure should be integrated into the original graph. This aims to determine which parts of the graph require the inclusion of additional relational patterns from the token structures.</li> </ul> <p>Let $\mathcal{P}_{\omega}$ denote a parameterized graph prompt function with learnable parameters $\omega$. The basic expression for this function is given by:</p> \[\mathcal{P}_{\omega}=\psi(\mathcal{G}, \mathcal{G_p}) \tag{1}\] <p>where $\psi$ denotes the insert pattern, and $\mathcal{G_p}$ represents the prompt graph, which consists of prompt tokens organized according to a specific token structure. These three components work together organically to lead to different graph prompts, which can be broadly classified into two types: prompt as tokens (focusing more on the design of the prompt tokens themselves) and prompt as graphs (focusing more on the organic integration of the three components).</p> <p>Let’s see the simplest graph prompt <code class="language-plaintext highlighter-rouge">GPF</code><d-cite key="gpfplus2023"></d-cite> , which adds a learnable prompt vector to each node’s feature vectors. Let $\mathcal{\omega}=p$, $p \in \mathbb{R}^{F \times 1}$ then the updated node features are:</p> \[\mathbf{X}_\omega=\left\{x_1+p, x_2+p \ldots, x_N+p\right\} \tag{2}\] <p>Then the original graph $G=(\mathbf{X}, \mathbf{A})$ becomes the prompt-enhanced graph $G_\omega=\left(\mathbf{X}_\omega, \mathbf{A}\right)$.</p> <p> Another typical graph prompt `All-in-One`<d-cite key="sun2023all"></d-cite> , which integrates entire prompt subgraphs into the original graph. Let <span>$\mathbf{P} \in \mathbb{R}^{k \times F}$</span> represent <span>$K$</span> learnable prompt token vectors, and <span>$\mathbf{A}_{\text {in }} \in\{0,1\}^{k \times k}$</span> denote the internal adjacency among prompt tokens. The connections between prompt tokens and original nodes are defined by a cross adjacency matrix <span>$\mathbf{A}_{\text {cro }} \in\{0,1\}^{k \times N}$</span>. Then the prompt-enhanced graph is: </p> <p> <span> $$ G_\omega=\left(\mathbf{X}, \mathbf{A},\mathbf{P}, \mathbf{A}_{\text {in }} , \mathbf{A}_{\text {cro }})\right. \tag{3} $$ </span> </p> <p> `All-in-One` approach optimizes the prompt tokens and their connections to adapt a graph-level pre-trained model for downstream tasks while keeping the pre-trained model parameters unchanged. </p> <p> The learnable parameters <span>$\omega$</span> are adjusted to minimize the loss <span>$\mathcal{L}_{T_{\mathrm{dow}}}$</span> for the downstream task <span>$T_{\text{dow}}$</span>, which is measured by the pre-trained model <span>$F_{\theta^*}$</span> acting on the prompt-enhanced graphs <span>$\mathcal{P}_{\omega}(G)$</span>: </p> <p> <span> $$ \omega^*=\arg \min _\omega \sum_{G \in \mathcal{G}} \mathcal{L}_{T_{\mathrm{dow}}}\left(F_{\theta^*}\left(\mathcal{P}_{\omega}(G))\right)\right) \tag{4} $$ </span> </p> <p> This optimization ensures that the graph prompts are effectively tailored to the downstream tasks without altering the parameters of the pre-trained model. </p> <h2 id="the-true-nature-of-graph-prompts">The True Nature of Graph Prompts</h2> <p>Graph prompt learning inherently focuses on learning effective transformation operations for graphs or their representations, enabling the reformulation of downstream tasks to align seamlessly with pre-training tasks. From the perspective of data manipulation, this approach leverages graph-level transformations to bridge the gap between diverse graph structures and the fixed capabilities of pre-trained graph models. Let $t(⋅)$ represent any graph-level transformation, such as modifying node features or augmenting graph structures. It has been established by Wang<d-cite key="wang2024does"></d-cite> et al. that an appropriate prompt module $\mathcal{P}$ can be learned to satisfy the following relationship:</p> \[F_{\theta^*}(\mathcal{P}_{\omega}(G)) = F_{\theta^*}(t(\mathbf{X}, \mathbf{A})) + O_{\mathcal{P}F} \tag{5}\] <p>The prompt module $\mathcal{P}$ integrates the input graph with the graph prompt, producing a transformed graph. This transformation mimics the effects of any desired graph manipulation $t(\mathbf{X}, \mathbf{A})$ quantifies the error bound $O_{\mathcal{P}F}$ between the representations of the manipulated graph and the graph modified using the prompt.</p> <p>This formulation highlights the <strong>flexibility and adaptability</strong> of graph prompt, showing that they can emulate a wide range of graph data manipulations. By effectively learning and applying appropriate transformations, graph prompts serve as a powerful tool to bridge data distribution shifts, mitigate anomalies, and enhance robustness, positioning them as a cornerstone for advancing graph-based learning systems.</p> <h2 id="understanding-how-graph-prompt-manipulate-data">Understanding How Graph Prompt Manipulate Data</h2> <h4 id="example-1-adjusting-node-features">Example 1: Adjusting Node Features</h4> <p>Suppose we have a graph where some node features are missing or noisy. A class <code class="language-plaintext highlighter-rouge">FeatureAdjustmentPrompt</code> can learn to adjust these features, the prompt vector <code class="language-plaintext highlighter-rouge">global_emb</code> is initialized as a learnable parameter, it starts as a tensor with the same dimensionality as the input features (<code class="language-plaintext highlighter-rouge">in_channels</code>). It is initialized using the Glorot initialization method (also known as Xavier initialization).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeatureAdjustmentPrompt</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">FeatureAdjustmentPrompt</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt_vector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">))</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">prompt_vector</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Adjust node features
</span>        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">prompt_vector</span>

    <span class="k">def</span> <span class="nf">multiplicate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">global_emb</span>

    <span class="k">def</span> <span class="nf">concatenate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Repeat the global_emb along the batch dimension to match x's shape
</span>        <span class="n">global_emb_expanded</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">global_emb</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">global_emb_expanded</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>In this example, the prompt learns a vector that, when added, multiplied or concatenated to the node features, improves the data quality or aligns it with the pre-trained model’s expectations.</p> <h4 id="example-2-integrating-subgraphs">Example 2: Integrating Subgraphs</h4> <p>A more complex manipulation involves adding a prompt subgraph to enrich the original graph’s structure:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SubGraphPrompt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">token_dim</span><span class="p">,</span> <span class="n">inner_prune</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">cross_prune</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        :param token_dim: Dimension of each token.
        :param inner_prune: Threshold for pruning inner token connections.
        :param cross_prune: Threshold for pruning cross connections (optional).
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">UnifiedPrompt</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">inner_prune</span> <span class="o">=</span> <span class="n">inner_prune</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cross_prune</span> <span class="o">=</span> <span class="n">cross_prune</span>

        <span class="n">self</span><span class="p">.</span><span class="n">token_list</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ParameterList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">token_dim</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
        <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="nf">token_init</span><span class="p">(</span><span class="n">init_method</span><span class="o">=</span><span class="sh">"</span><span class="s">kaiming_uniform</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">token_init</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="sh">"</span><span class="s">kaiming_uniform</span><span class="sh">"</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">init_method</span> <span class="o">==</span> <span class="sh">"</span><span class="s">kaiming_uniform</span><span class="sh">"</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token_list</span><span class="p">:</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">kaiming_uniform_</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="sh">'</span><span class="s">leaky_relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">fan_in</span><span class="sh">'</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Only kaiming_uniform init is supported.</span><span class="sh">"</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">update_token_structure</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">pg_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tokens</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token_list</span><span class="p">):</span>
            <span class="n">token_dot</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tokens</span><span class="p">.</span><span class="nf">t</span><span class="p">())</span>
            <span class="n">token_sim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">token_dot</span><span class="p">)</span>
            <span class="n">inner_adj</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">token_sim</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">inner_prune</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">token_sim</span><span class="p">)</span>
            <span class="n">edge_index</span> <span class="o">=</span> <span class="n">inner_adj</span><span class="p">.</span><span class="nf">nonzero</span><span class="p">(</span><span class="n">as_tuple</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">t</span><span class="p">().</span><span class="nf">contiguous</span><span class="p">()</span>

            <span class="n">pg_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nc">Data</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">tokens</span><span class="p">,</span> <span class="n">edge_index</span><span class="o">=</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">i</span><span class="p">]).</span><span class="nf">long</span><span class="p">()))</span>

        <span class="n">pg_batch</span> <span class="o">=</span> <span class="n">Batch</span><span class="p">.</span><span class="nf">from_data_list</span><span class="p">(</span><span class="n">pg_list</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pg_batch</span>

    <span class="k">def</span> <span class="nf">insert</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">graph_batch</span><span class="p">:</span> <span class="n">Batch</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Enhances input graphs with prompt graphs and cross connections.
        </span><span class="sh">"""</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">inner_structure_update</span><span class="p">()</span>
        <span class="n">inner_edge_index</span> <span class="o">=</span> <span class="n">pg</span><span class="p">.</span><span class="n">edge_index</span>
        <span class="n">token_num</span> <span class="o">=</span> <span class="n">pg</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">re_graph_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">Batch</span><span class="p">.</span><span class="nf">to_data_list</span><span class="p">(</span><span class="n">graph_batch</span><span class="p">):</span>
            <span class="n">g_edge_index</span> <span class="o">=</span> <span class="n">g</span><span class="p">.</span><span class="n">edge_index</span> <span class="o">+</span> <span class="n">token_num</span>

            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">cross_prune</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">cross_dot</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">pg</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">g</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="nf">t</span><span class="p">())</span>
                <span class="n">cross_sim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">cross_dot</span><span class="p">)</span>
                <span class="n">cross_adj</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">cross_sim</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">cross_prune</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">cross_sim</span><span class="p">)</span>
                <span class="n">cross_edge_index</span> <span class="o">=</span> <span class="n">cross_adj</span><span class="p">.</span><span class="nf">nonzero</span><span class="p">(</span><span class="n">as_tuple</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">t</span><span class="p">().</span><span class="nf">contiguous</span><span class="p">()</span>
                <span class="n">cross_edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">token_num</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">cross_edge_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">pg</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">g</span><span class="p">.</span><span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">g</span><span class="p">.</span><span class="n">y</span>
            <span class="n">edge_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">inner_edge_index</span><span class="p">,</span> <span class="n">g_edge_index</span><span class="p">,</span> <span class="n">cross_edge_index</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">data</span> <span class="o">=</span> <span class="nc">Data</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="o">=</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
            <span class="n">re_graph_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="n">graphp_batch</span> <span class="o">=</span> <span class="n">Batch</span><span class="p">.</span><span class="nf">from_data_list</span><span class="p">(</span><span class="n">re_graph_list</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">graphp_batch</span>
</code></pre></div></div> <p>This prompt learns both node features and structural relationships within the prompt subgraph, which, when integrated, can enhance the original graph’s representation.</p> <h4 id="example-3-tuning-graph-prompt">Example 3. Tuning Graph Prompt</h4> <p>In case of <code class="language-plaintext highlighter-rouge">FeatureAdjustmentPrompt</code> which add a trainable vector to the node, during the forward pass, the input graph data is modified by the soft prompt (global_emb). The modified node features are passed through the GNNs, which processes the graph structure and computes output predictions. The loss is then computed based on the model’s prediction and the ground truth.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">FeatureAdjustmentPromptTrain</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
   <span class="n">prompt</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
   <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
   <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
   <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
       <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
       <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
       <span class="n">batch</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">x</span><span class="p">)</span><span class="c1"># Add the prompt to the node features
</span>       <span class="n">out</span> <span class="o">=</span> <span class="nf">gnn</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">batch</span><span class="p">,</span> <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">prompt_type</span> <span class="o">=</span> <span class="sh">'</span><span class="s">GPF</span><span class="sh">'</span><span class="p">)</span>
       <span class="n">out</span> <span class="o">=</span> <span class="nf">answering</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">y</span><span class="p">)</span>   <span class="c1"># Calculate loss
</span>       <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>  <span class="c1"># Backpropagate gradients
</span>       <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>  <span class="c1"># Update model parameters
</span>       <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
   <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
</code></pre></div></div> <p>During the backpropagation step, gradients are computed for all parameters, including the learnable soft prompt. The goal is to adjust the model (including the prompt) such that the loss function is minimized over time.</p> <p><code class="language-plaintext highlighter-rouge">FeatureAdjustmentPrompt</code> works similarly to how soft prompts are used in natural language processing with transformers. The key difference is that <code class="language-plaintext highlighter-rouge">FeatureAdjustmentPrompt</code> is specifically designed to work with graph-structured data.</p> <h4 id="example-4-inference-and-get-results">Example 4. Inference and get results</h4> <p>In the provided example, <code class="language-plaintext highlighter-rouge">FeatureAdjustmentPrompt</code> is applied to Graph Classification. The prompt is a trainable embedding (<code class="language-plaintext highlighter-rouge">global_emb</code>), which is added to the node features of the graph to help guide the GNN in its learning process. The model uses the modified node features (<code class="language-plaintext highlighter-rouge">batch.x = prompt.add(batch.x)</code>) to improve the graph’s representation for classification.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">FeatureAdjustmentPromptEva</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">gnn</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">answering</span><span class="p">,</span> <span class="n">num_class</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">prompt</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">answering</span><span class="p">:</span>
        <span class="n">answering</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">torchmetrics</span><span class="p">.</span><span class="n">classification</span><span class="p">.</span><span class="nc">Accuracy</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">multiclass</span><span class="sh">"</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_class</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">accuracy</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch_id</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">batch</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="nf">gnn</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">batch</span><span class="p">)</span> <span class="c1"># Forward pass through GNN
</span>            <span class="n">out</span> <span class="o">=</span> <span class="nf">answering</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># Get predicted class
</span>            <span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="c1"># Calculate accuracy
</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">.</span><span class="nf">compute</span><span class="p">()</span>  <span class="c1"># Compute final accuracy
</span>    <span class="k">return</span> <span class="n">acc</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
</code></pre></div></div> <h2 id="why-data-manipulation-is-crucial-in-graph-learning">Why Data Manipulation Is Crucial in Graph Learning</h2> <h3 id="bridging-the-expressiveness-flexibility-trade-off">Bridging the Expressiveness-Flexibility Trade-off</h3> <p>Traditional GNNs like GCN and GAT, while expressive, rely on rigid structures and require extensive task-specific fine-tuning. This limits their ability to generalize across diverse tasks without retraining. Shallow embedding approaches like DeepWalk which treats node representations as free parameters are flexible but lack the ability to encode complex graph structures or adapt effectively to downstream tasks with diverse objectives. While the <code class="language-plaintext highlighter-rouge">Pre-train &amp; Fine-tune</code> mechanism can also facilitate domain or task adaptation of pre-trained graph models, enabling the expressiveness of a new task, it often necessitates a considerable amount of labeled information and requires exhaustive retraining of the pre-trained model and loss the flexibility. In comparison, <strong>graph prompt mechanism offers a higher degree of flexibility and efficiency.</strong> It allows the insertion of learnable tokens into a graph’s structure, effectively acting as dynamic extensions. These prompts enhance the graph’s ability to encode task-specific or domain-specific features, balancing expressiveness (leveraging the GNN’s pre-trained capacity) and flexibility (lightweight and adaptable tokens).</p> <h3 id="efficiency-in-model-adaptation">Efficiency in Model Adaptation</h3> <p>Fine-tuning requires retraining large portions of the model for each task, consuming significant computational resources and energy. Moreover, it struggles with zero-shot or few-shot learning scenarios where labeled data is scarce. In contrast, prompt-based methods focus on lightweight tuning by inserting task-specific information into pre-trained models without modifying their core parameters. This efficiency becomes increasingly critical as graph models grow larger and are applied to more diverse datasets.</p> <p>To highlight the advantages of graph prompt learning, we compare the tunable parameters in graph prompt methods during training with those in classical <code class="language-plaintext highlighter-rouge">Pre-train &amp; Fine-tune</code> approaches across various backbones. Consider a graph $G=(\mathcal{V}, \mathcal{E}, \mathbf{X}, \mathbf{A})$ where $\mathcal{V}=\left{v_1, v_2, \ldots, v_N\right}$, with $N$ nodes, $M$ edges, and $d$ features. For a graph model with $L$ layers and a maximum layer dimension $D$, traditional backbones (e.g., GCN) have a learnable parameter complexity of $O(dD + D^2(L-1))$. More complex backbones, such as graph transformers, involve even larger parameter counts. However, in the graph prompt learning framework, tuning the prompt parameters with a frozen pre-trained backbone enables faster convergence during training. For instance, in the relatively complex <code class="language-plaintext highlighter-rouge">All-in-One</code> method, with $K$ tokens and $m$ edges, the learnable parameter complexity is $O(Kd)$. This is significantly lower than that of GCN, especially when the layer number $L$ is large. Other graph prompt methods also achieve similar tunable parameter efficiencies, with specific parameter sizes determined by their design. For example, <code class="language-plaintext highlighter-rouge">Gprompt</code> <d-cite key="liu2023graphprompt"></d-cite> integrates a learnable vector into the graph pooling step via element-wise multiplication, further reducing the number of tunable parameters.</p> <h3 id="incorporating-multi-modal-and-cross-domain-knowledge">Incorporating Multi-modal and Cross-domain Knowledge</h3> <p>Graphs often integrate information from other modalities, such as text (e.g., attributed graphs in social networks) or images (e.g., scene graphs in computer vision). Effective use of cross-modal information demands a method to bridge structured graph data with linear modalities like text or images. Prompts act as adapters that introduce modality-specific tokens or structures, enabling seamless integration and representation of multi-modal information. Similarly, for cross-domain knowledge, graph prompts serve as intermediaries, encoding transferable features that help align distinct graph domains by reconstruct graph data.</p> <p>In this blog, we argue that the prompting mechanism offers a promising solution to address the limitations of existing graph representation learning methods, effectively balancing flexibility and expressiveness and saving training cost .</p> <p>Gradually, we are realizing that the original purpose of Graph Prompt—mimicking LLM prompts to bridge the gap between pretext and downstream tasks—is evolving. Unlike in LLMs, where prompts primarily serve to guide the model’s attention and improve task alignment, the essence of Graph Prompting lies more in data operations. This opens up a much larger space for potential manipulations and optimizations at the data level.</p> <p>Most past research has focused on using Graph Prompting to address the limitations of the traditional <code class="language-plaintext highlighter-rouge">pre-train &amp; fine tune</code> paradigm, which often struggled with transferring knowledge to new tasks efficiently. However, we are beginning to recognize that Graph Prompting holds even greater potential—not just as a tool to reformulate tasks, but as a powerful mechanism to simulate various graph transformations. By leveraging these data operations, we can create more flexible and dynamic models that go beyond simple task adaptation, pushing the boundaries of how we interact with and manipulate graph-structured data.</p> <h2 id="graph-prompts-vs-nlp-prompts-a-fundamental-difference">Graph Prompts vs. NLP Prompts: A Fundamental Difference</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-rethinking-graph-prompts/figure1-n.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-rethinking-graph-prompts/figure1-n.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-rethinking-graph-prompts/figure1-n.gif-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-rethinking-graph-prompts/figure1-n.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p align="center" style="color:gray;"> <em>Figure 3: GIF - Graph Prompts vs. NLP Prompts</em> </p> <h3 id="gpf-like-graph-prompt-and-soft-prompt-in-nlp">GPF-like Graph Prompt and Soft Prompt in NLP</h3> <p>Soft prompt in NLP involve adding learned, continuous embeddings directly into the input without explicit hard-coded tokens. These embeddings are optimized during training and are typically added to the input embeddings of each token, acting as “soft hints” for the model. Similarly, <code class="language-plaintext highlighter-rouge">GPF</code> adds continuous token vectors to the feature representations of each node in the graph. It doesn’t introduce new nodes or edges; it simply modifies the node feature matrix by integrating the prompt vector, thus making the node representations richer. This approach is “soft” in the sense that it subtly integrates prompt information directly into the existing structure without altering the core structure. In both cases, the prompt information is “blended” into the existing features, relying on embedding vectors rather than discrete or structured prompts.</p> <h3 id="all-in-one-like-graph-prompt-and-hard-prompt-in-nlp">All-in-One-like Graph Prompt and Hard Prompt in NLP</h3> <p>Hard prompt in NLP refer to explicit, often human-readable tokens added to the input, such as specific keywords or phrases designed to steer the model’s output. They’re “hard-coded” into the input sequence, giving more direct and structured guidance to the model. <code class="language-plaintext highlighter-rouge">All-in-One</code> similarly introduces new, explicit graph tokens(like nodes and edges) to the original graph, which serve as a structured prompt. This additional subgraph provides clear and explicit information in the form of new connections and nodes, rather than just altering existing node features. <code class="language-plaintext highlighter-rouge">All-in-One</code> acts as a fully-formed, standalone structure that connects to the original graph, similar to how hard prompts are added as discrete, noticeable tokens in an NLP input sequence. This approach is “hard” because it directly changes the structure of the graph by introducing an entirely new subgraph, making the prompt information an explicit part of the graph’s topology.</p> <p>Interestingly, from information theory perspective, Graph Prompt and its counterpart in LLMs can be viewed as two distinct processes concerning entropy. Graph Prompt tends to increase the entropy of the system. By introducing additional tokens or subgraphs to the original input graph, Graph Prompt enriches the input data, expanding its feature space and increasing the diversity and complexity of the information. This entropy increase allows the GNN to capture a broader range of graph transformations, thus enhancing its flexibility and generalization across downstream tasks. Essentially, Graph Prompt adds more potential configurations, creating a more dynamic and adaptable input, which translates into higher entropy within the system.</p> <p>In contrast, the use of prompts in LLMs typically functions as an entropy-reducing mechanism. Prompts in LLMs provide a guiding context or specific task instruction that narrows the possible outputs by focusing the model’s attention on a constrained area of the information space. By reducing uncertainty and ambiguity, the prompt decreases the number of plausible outcomes the model can generate. This process, in essence, limits the variability of the model’s response, channeling it toward a more predictable and specific set of outputs, thus leading to a reduction in entropy.</p> <h2 id="current-challenges-in-graph-prompt-research">Current Challenges in Graph Prompt Research</h2> <p>Despite the promising potential of graph prompt as data manipulation tools, several challenges are impeding their advancement. Recognizing and addressing these issues is essential for the field to progress.</p> <h3 id="limited-understanding-of-prompt-design">Limited Understanding of Prompt Design</h3> <p>Although we can theoretically prove that there exists a graph prompt that can approximate or even equal the optimal solution for a downstream task from <strong>Equation 5</strong>, our practical ability to construct such a prompt remains limited. The exact form of an optimal graph prompt is often difficult to find, and this gap in understanding affects subsequent prompt design. Without clear insights into the optimal prompt structure, designing effective prompts often relies on trial-and-error or heuristic-based methods. Existing prompts may not fully leverage the theoretical potential of graph prompts, leading to less efficient or less effective performance in practical applications. The absence of a clear framework for designing graph prompts makes it challenging to scale them to more complex or diverse graph data. This limitation emphasizes the need for more research into theoretical frameworks that connect graph structure and task objectives to prompt design and methods for learning or discovering optimal prompts directly from data.</p> <h3 id="limited-standardized-benchmarks">Limited Standardized Benchmarks</h3> <p>So far , we only find a benchmark about graph prompt. From ProgBench <d-cite key="zi2024prog"></d-cite> , graph prompt methods demonstrate significant effectiveness in knowledge transfer, outperforming both supervised and classical “Pre-train &amp; Fine-tune” methods in few-shot node and graph classification tasks across diverse datasets. They exhibit consistent advantages by reducing negative transfer and enhancing performance. Methods like GPF-plus excel in node-level tasks due to their ability to adapt node features, while All-in-One achieves superior results in graph-level tasks by leveraging learnable subgraphs. The alignment between pre-training and downstream tasks is crucial, with node-level pre-training benefiting node tasks and graph-level pre-training enhancing graph tasks. Moreover, graph prompt mitigate negative transfer effectively without negative transfer rates in challenging scenarios, which is unattainable with classical methods. Overall, graph prompt offer a powerful, adaptive, and reliable solution for maximizing knowledge transfer and addressing the limitations of traditional graph learning approaches.</p> <h3 id="vanishing-gradient-in-the-graph-prompt">Vanishing Gradient in the graph prompt</h3> <p>One potential challenge with graph prompt, particularly when integrated into deep GNNs, is the <strong>vanishing gradient problem</strong>. Since graph prompts are introduced directly into the graph’s structure (such as adding prompt tokens to the node features or structure), they must rely on the gradient flow from the downstream task to update their learnable parameters. However, when the GNNs contains many layers, the gradient signal from the loss function can degrade as it propagates backward through the network layers. In deep GNNs, the gradient flowing back to the earlier layers, where the prompt is embedded, can become so small that it effectively “disappears.” This means that the prompt parameters might not receive sufficient updates to optimize effectively, leading to poor adaptation of the graph prompt. In traditional GNNs architectures, this issue is exacerbated due to the nature of message passing across multiple layers. As the graph information is aggregated over layers, nodes receive information from an exponentially larger number of neighbors, potentially diluting the gradient signal and making it harder to train the prompt module.</p> <h2 id="applications-and-future-directions">Applications and Future Directions</h2> <p>Despite these challenges, graph prompts hold great promise for advancing graph learning. By addressing the current obstacles, we can unlock and explore potential applications and enhance existing ones.</p> <h3 id="real-time-network">Real-Time Network</h3> <p>For IoT and sensor networks, resilient and adaptable connectivity is paramount. Graph prompting can simulate network adjustments by rewriting parts of the network graph. For example, if a sensor node goes offline or a connection degrades, graph prompts can introduce virtual nodes or reinforce connections to maintain network stability. This capacity for real-time, data-driven network reconfiguration enables IoT systems to adapt to changing conditions dynamically, providing robust performance without the need for complete network reconfiguration.</p> <h3 id="drug-discovery">Drug Discovery</h3> <p>In drug discovery, predicting protein structures and interactions with new compounds is essential. Graph prompts provide a data-rewriting approach by introducing prompts that simulate specific molecular interactions within biochemical graphs. Instead of extensive retraining, researchers can modify pre-trained models to explore new compounds by dynamically adjusting protein interaction graphs. This rapid, flexible adaptation supports faster drug screening processes, accelerating the discovery of compounds with potential efficacy against targeted diseases.</p> <h3 id="financial-networks">Financial Networks</h3> <p>Financial transaction networks evolve constantly, with new transaction types and fraud patterns emerging frequently. Graph Prompt enhance fraud detection systems by allowing for prompt-based modifications that simulate emerging fraud patterns within transaction graphs. By dynamically rewriting graph data to incorporate these evolving patterns, fraud detection systems can remain agile and responsive, helping financial institutions detect fraud more quickly and cost-effectively without the need for frequent model retraining.</p> <h3 id="knowledge-graphs">Knowledge Graphs</h3> <p>In educational technology, personalized learning pathways are becoming increasingly important. Knowledge graphs represent concepts, skills, and learning objectives, but each student’s path through this network is unique. Graph prompts allow for personalized data rewriting within these graphs by introducing tokens that emphasize prerequisite skills or areas aligned with the learner’s goals. This data-driven adaptation supports the creation of customized learning pathways, helping students progress based on their individual needs and strengths.</p> <p>In summary, graph prompt, as a tool for data-driven graph rewriting, is revealing promising applications across diverse fields. By allowing for dynamic reshaping of graph data, it offers a flexible solution for tasks ranging from personalized recommendations and robust IoT networks to adaptive learning pathways. The potential to revolutionize graph-based applications is only beginning to unfold.</p> <h2 id="conclusion">Conclusion</h2> <p>Graph prompt, when viewed as strategies for data manipulation, offer a powerful approach to overcoming the unique challenges of graph learning. By focusing on transforming the input data rather than adjusting the model, we can leverage pre-trained GNNs more effectively, enhancing flexibility, robustness, and efficiency.</p> <p>However, to fully realize the potential of graph prompt, the research community must address the current challenges hindering their advancement. By developing more theoretical foundations, scalable methods, and standardized benchmarks, we can unlock new possibilities in graph-based research and applications, ultimately pushing the boundaries of what GNNs can achieve.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2025/assets/bibliography/2025-04-28-rethinking-graph-prompts.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>