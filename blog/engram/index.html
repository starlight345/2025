<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),n=e=>e.innerText,t=e=>{let n=e.split(" "),t=n.slice(0,-1).join(" ");return[n.at(-1),t]},i=Array.from(document.getElementsByClassName("author")).map(n).map(e).map(t),o=i[0][0],a=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(n).map(e),"April 28, 2025"),r="In Search of the Engram in LLMs: A Neuroscience Perspective on the Memory Functions in AI Models",s="Large Language Models (LLMs) are enhancing our daily lives but also pose risks like spreading misinformation and violating privacy, highlighting the importance of understanding how they process and store information. This blogpost offers a fresh look into a neuroscience-inspired perspective of LLM's memory functions, based on the concept of engrams-the physical substrate of memory in living organism. We discuss a synergy between AI research and neuroscience, as both fields cover complexities of intelligent systems.";{let e=i.map(e=>`${e[0]}, ${e[1]}`).join(" and "),n=`\n@inproceedings{${(o+"2025"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${r}},\n  abstract = {${s}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${a}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=n}{let e=i.map(e=>e[0]),n=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${r}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=n}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>In Search of the Engram in LLMs: A Neuroscience Perspective on the Memory Functions in AI Models | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Large Language Models (LLMs) are enhancing our daily lives but also pose risks like spreading misinformation and violating privacy, highlighting the importance of understanding how they process and store information. This blogpost offers a fresh look into a neuroscience-inspired perspective of LLM's memory functions, based on the concept of engrams-the physical substrate of memory in living organism. We discuss a synergy between AI research and neuroscience, as both fields cover complexities of intelligent systems."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/2025/assets/css/main.css"> <link rel="canonical" href="https://starlight345.github.io/2025/blog/engram/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/2025/assets/js/theme.js"></script> <script src="/2025/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2025/assets/js/distillpub/template.v2.js"></script> <script src="/2025/assets/js/distillpub/transforms.v2.js"></script> <script src="/2025/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <d-front-matter> <script async type="text/json">{
      "title": "In Search of the Engram in LLMs: A Neuroscience Perspective on the Memory Functions in AI Models",
      "description": "Large Language Models (LLMs) are enhancing our daily lives but also pose risks like spreading misinformation and violating privacy, highlighting the importance of understanding how they process and store information. This blogpost offers a fresh look into a neuroscience-inspired perspective of LLM's memory functions, based on the concept of engrams-the physical substrate of memory in living organism. We discuss a synergy between AI research and neuroscience, as both fields cover complexities of intelligent systems.",
      "published": "April 28, 2025",
      "authors": [
        {
          "author": "Minsung Kim",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Seoul National University",
              "url": ""
            }
          ]
        },
        {
          "author": "Jea Kwon",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Max Planck Institute for Security and Privacy",
              "url": ""
            }
          ]
        },
        {
          "author": "Dong-Kyum Kim",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Max Planck Institute for Security and Privacy",
              "url": ""
            }
          ]
        },
        {
          "author": "Meeyoung Cha",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Max Planck Institute for Security and Privacy",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/reviewing/">reviewing</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>In Search of the Engram in LLMs: A Neuroscience Perspective on the Memory Functions in AI Models</h1> <p>Large Language Models (LLMs) are enhancing our daily lives but also pose risks like spreading misinformation and violating privacy, highlighting the importance of understanding how they process and store information. This blogpost offers a fresh look into a neuroscience-inspired perspective of LLM's memory functions, based on the concept of engrams-the physical substrate of memory in living organism. We discuss a synergy between AI research and neuroscience, as both fields cover complexities of intelligent systems.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#what-is-engram">What is "Engram"?</a></div> <div><a href="#explaining-ai-research-through-the-concept-of-engrams">Explaining AI Research Through the Concept of Engrams</a></div> <ul> <li><a href="#1-grafting-task-specific-skill-localization-in-llms">1. Grafting: Task-specific skill localization in LLMs</a></li> <li><a href="#2-rome-targeted-memory-editing-in-llms">2. ROME: Targeted Memory Editing in LLMs</a></li> </ul> <div><a href="#concluding-remarks-let-s-connect-ai-research-with-neuroscience">Concluding Remarks: Let's connect AI research with neuroscience!</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Large Language Models (LLMs) are increasingly prevalent, assisting with information retrieval, answering questions, and generating content. However, they’re a “double-edged sword”: while capable of remarkable feats, they also risk spreading misinformation, revealing sensitive data, or generating harmful content. Unlike traditional computer systems, where data storage is transparent and traceable, LLMs function more like “black boxes,” making it nearly impossible to pinpoint where specific information is stored and therefore challenging to edit. To effectively and efficiently edit models in this context, we need a clearer understanding of how memory is stored within these models and develop methodologies that allow for targeted editing.</p> <p>In neuroscience, a field that studies highly complex systems like the biological brain, researchers have made significant progress in understanding memory formation and storage. Memory storage in neuroscience involves physical traces in the brain known as <strong>engrams</strong>, networks of neurons and synapses that “record” information after learning or an event. This process strengthens the connections between neurons that fire together based on synaptic plasticity, creating a lasting representation of that memory or skill<d-cite key="schacter1978richard, josselyn2015finding, liu2012optogenetic"></d-cite>. Here, we pose an intriguing question: <em>Could something akin to engrams exist within Large Language Models (LLMs)?</em></p> <p>In this blog post, we will explore three key points to bridge the gap between neuroscience’s experimental findings and established theories about memory and the field of AI. In particular we will look into:</p> <ol> <li>How engrams are formulated in neuroscience,</li> <li>Two interesting works that seemingly track the engrams(memory locations) in LLMs</li> <li>The potential for LLM research and neuroscience to inspire one another <h2 id="what-is-engram">What is “Engram”?</h2> </li> </ol> <div id="fig_engram_concept"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-engram/engram_concept_v3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-engram/engram_concept_v3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-engram/engram_concept_v3-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-engram/engram_concept_v3.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. A schematic diagram illustrating the four stages of the memory system and the concept of engrams in neuroscience. During memory formation, external stimuli (such as the "SUN") activate specific neuronal circuits, resulting in the formation of memory engrams (red connections) via strengthened synaptic connections. During the retrieval stage, these engrams are reactivated to recall previously stored information (for example, "MOON") via corresponding neuronal circuits (yellow connections). Memory modification occurs when previously encoded engrams are updated or altered to include new synaptic changes (blue dashed connections). Finally, in memory forgetting, synaptic weakening or suppression causes engram degradation, resulting in an inability to recall specific information (for example, "?"). The central inset depicts the engram as a physical substrate of memory, with patterns of neuronal and synaptic ensembles. </div> </div> <p>Over a century ago, Richard Semon introduced the concept of the <strong>engram</strong><d-cite key="schacter1978richard"></d-cite>, suggesting that each memory leaves a lasting imprint on our neural networks, much like a footprint. In his early 20th-century writings, Semon described the engram as the physical substrate of memory, proposing that experiences create enduring changes in the brain’s structure, allowing them to be recalled later. This revolutionary idea laid the groundwork for modern memory research. Today, neuroscientists define an engram as the changes within neurons, synapses, and neural circuits that store a specific memory, enabling its reactivation and recall<d-cite key="josselyn2020memory"></d-cite>.</p> <p>To understand engram, we must first define memory. Memories follow a dynamic life cycle with four key stages: formation, retrieval, modification, and forgetting (<a href="#fig_engram_concept">Fig. 1</a>)<d-cite key="josselyn2015finding,guskjolen2023engram"></d-cite>. During formation, external stimuli activate neural circuits, strengthening synaptic connections through mechanisms like long-term potentiation (LTP) to create an engram<d-cite key="bliss1993synaptic, choi2018interregional"></d-cite>. Retrieval involves reactivating these neural circuits to recall stored information<d-cite key="kim2014memory, khalaf2018reactivation"></d-cite>. Next, modification allows previously encoded memories to be updated or altered through processes such as reconsolidation, making memory both stable and adaptable<d-cite key="schiller2010preventing, monfils2009extinction, ramirez2013creating"></d-cite>. Finally, forgetting prunes unnecessary or outdated memories, often through synaptic weakening or neural activity suppression<d-cite key="ryan2022forgetting, Wimber2015RetrievalIA"></d-cite>. These stages represent the intricate, dynamic processes underpinning memory, with engrams serving as the physical foundation of memories.</p> <p>The search for engrams has long captivated neuroscience, dating back to Karl Lashley’s groundbreaking work. Lashley’s 1950 lecture, <em>In Search of the Engram</em><d-cite key="lashley1950search"></d-cite>, investigated the physical basis of memory by training animals to navigate mazes. He attempted to identify specific memory storage regions by systematically removing or damaging parts of their brains. Surprisingly, his findings led to the principles of mass action and equipotentiality, which revealed that memory is distributed throughout the brain rather than in a single region. While Lashley was ultimately unable to locate the elusive engram, his pioneering research laid the groundwork for modern neuroscience, shaping our understanding of memory as a dynamic and distributed process.</p> <p>Recent advances in neuroscience have made it possible to identify and manipulate engrams. Using optogenetics and advanced imaging techniques, researchers have successfully mapped the neural circuits associated with specific memories in animal models<d-cite key="han2009selective, liu2012optogenetic"></d-cite>. They discovered that certain groups of neurons are activated and modified during learning, forming a network that represents the memory<d-cite key="abdou2018synapse, choi2018interregional"></d-cite>. This network can be reactivated to “recall” the memory, and interestingly, artificially activating these neurons can induce a conscious recollection of the memory in the brain<d-cite key="roy2016memory"></d-cite>.</p> <p>The concept of engrams has shed light on how memories are formed, retrieved, modified, and forgotten (<a href="#fig_engram_concept">Fig. 1</a>) and emphasizes the physical reality of memory as a process deeply embedded in the brain’s structure<d-cite key="josselyn2015finding, guskjolen2023engram"></d-cite>. However, it is important to distinguish the physical substrate from the broader notion of memory, which refers to the actual information or experience we recall—not necessarily the specific neurons that store information.</p> <p>This engram-based understanding could also serve as a model for artificial intelligence systems. If we could identify specific “engrams” within a neural network or large language model (LLM), we might be able to selectively edit or delete certain memories in AI models, similar to how neuroscientists can activate or deactivate specific memories in the brain experimentally. This cross-disciplinary exploration could open up new possibilities for making AI models more transparent, safe, and adaptable.</p> <h2 id="explaining-ai-research-through-the-concept-of-engrams">Explaining AI Research Through the Concept of Engrams</h2> <p>Having introduced the concept of engrams, we now examine LLM research through this lens. We have identified two research efforts that explore engram-like concepts in LLMs. The first is <strong>Grafting</strong>, a method that identifies and integrates specific parameter regions crucial for newly learned tasks. The second study is <strong>ROME</strong> (Rank-One Model Editing), which introduces targeted updates in LLMs without retraining the entire model. Although the original studies do not explicitly reference engrams, we are the first to bridge this concept to reinterpret and analyze their findings.</p> <h3 id="1-grafting-task-specific-skill-localization-in-llms">1. Grafting: Task-specific skill localization in LLMs</h3> <div id="graft_concept"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-engram/graft_concept_v2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-engram/graft_concept_v2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-engram/graft_concept_v2-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-engram/graft_concept_v2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2. A conceptual figure of model grafting on a neuroscience perspective </div> </div> <p>Panigrahi et al.(2023)<d-cite key="panigrahi2023task"></d-cite> explored how weights in a pretrained language model are adjusted after fine-tuning for a specific task. Surprisingly, they discovered that the performance improvement in the fine-tuned model can be achieved by grafting only a small fraction of its parameters to the pretrained model. This implies that this small subset of parameters encodes the essence of the newly acquired skill, capturing the critical information required to complete the specific task.</p> <p>Connecting this work to neuroscience, we can consider this subset of weight parameters to be a synaptic engram for finetuning skills.(<a href="#graft_concept">Fig. 2</a>), An intriguing parallel can be drawn to the work of Hayashi-Takagi et al. (2015)<d-cite key="hayashitakagi2015labeling"></d-cite>, which examined how synaptic clusters in the motor cortex encode learning-specific changes. The researchers used an optogenetic tool to label synaptic spines that had been recently potentiated during the acquisition of a motor skill. When they selectively reduced the size of these tagged spines, the motor skill was disrupted, whereas manipulating untagged spines had no effect. This demonstrated that the learned task’s memory was stored in a dense, task-specific cluster of modified synapses, known as a synaptic ensemble or engram. This discovery is similar to the concept of grafting in AI, where a small, localized region of modified parameters encodes a new, task-specific ability.</p> <p>However, we also recognize differences in these two domains in terms of the methods used to identify and label the “synaptic engram.” In the model grafting process within the AI domain, the researchers directly computed the engram through optimization.</p> <p><strong>Finding synaptic engrams in LLMs</strong></p> <p>To localize the weight parameters or ‘engram’ that have an important contribution to the performance of the fine-tuned task (e.g., sentiment analysis), Panigrahi et al.(2023)<d-cite key="panigrahi2023task"></d-cite> grafted the small region of the fine-tuned model \(\theta_{\textrm{ft}}\) to a pretrained model \(\theta_{\textrm{pre}}\) using a binary mask \(\gamma \in \{0,1\}^{\lvert\theta_{\textrm{ft}}\rvert}\), which identifies a subset of parameters. They then evaluated the performance of the resulting grafted model.</p> <div id="graft_org"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-engram/graft_org-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-engram/graft_org-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-engram/graft_org-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-engram/graft_org.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3. An illustration of model grafting process. Source: <i>A. Panigrahi et al., Task-specific skill localization in fine-tuned language models, 2023.</i> </div> </div> <p>The grafted model is defined as:</p> \[\overline{\theta_{\textrm{ft}}}(\gamma) = \gamma \odot \theta_{\textrm{ft}} + (1-\gamma) \odot \theta_{\textrm{pre}}\] <p>where \(\odot\) denotes element-wise multiplication. <a href="#graft_org">Figure 3</a> illustrates this equation.</p> <p>So if the grafted model \(\overline{\theta_{\textrm{ft}}}(\gamma)\) has a similar performance to the fine-tuned model \(\theta_{\textrm{ft}}\), we can infer that the region in \(\theta_{\textrm{ft}}\), corresponding to \(\gamma\) is the engram responsible for the skill for the fine-tuned task.</p> <p>To find this engram, they directly optimized the mask \(\gamma\) to minimize the task loss for the grafted model. Formally, this can be expressed as:</p> \[\underset{\gamma \in \{0, 1\}^{|\theta_{\textrm{ft}}|} : \|\gamma\|_{0} \le s}{\operatorname{argmax}} \mathcal{L}_{\mathcal{T}}(\gamma \odot \theta_{\textrm{ft}} + (1 - \gamma) \odot \theta_{\textrm{pre}})\] <p>where \(\mathcal{L}_{\mathcal{T}}\) is a metric for performance on the Task \(\mathcal{T}\) (e.g., classification error), and \(s\) is a sparsity constraint on the mask \(\gamma\).</p> <p>To enable optimization through gradient descent, they reparameterized the mask \(\gamma\) as a differentiable variable using a sigmoid function applied to a real-valued vector \(S\), i.e., \(\gamma = \sigma(S)\).</p> <p>Using this optimization objective directly, they successfully identified task-specific skill engrams. By grafting just 0.01% of \(\theta_{\textrm{ft}}\) (the identified engram) onto \(\theta_{\textrm{pre}}\), \(\overline{\theta_{\textrm{ft}}}(\gamma)\) achieved over 95% of the original fine-tuned model’s performance. This confirms that the identified engram is indeed responsible for the task-specific skill.</p> <p><strong>Leveraging Engrams in LLMs</strong></p> <p>In the AI domain, researchers have demonstrated how memory locations (i.e., engrams) can be identified through direct optimization techniques. Once these task-specific engrams are identified, they can be applied to practical scenarios like continuous learning. Panigrahi et al.(2023)<d-cite key="panigrahi2023task"></d-cite> demonstrated that by freezing the memory region responsible for a specific skill and ensuring subsequent tasks are learned in non-overlapping regions, it is possible to achieve “forget-free” continual learning. This approach enable models to retain previous skills while sequentially learning new ones.</p> <p>Identifying engrams in LLMs opens up a wide range of possibilities, including targeted memory editing. In the following section, we will delve into research that has explored memory editing techniques in LLMs.</p> <h3 id="2-rome-targeted-memory-editing-in-llms">2. ROME: Targeted Memory Editing in LLMs</h3> <p>Meng et al. (2022)<d-cite key="meng2022locating"></d-cite> introduced a mathematically grounded approach to update memory within LLMs. The authors treated memory as a factual association and developed a method for inserting new memory while preserving knowledge that is unrelated to the new memory. A factual association is a link between a subject (entity) and specific information about it, which a model has learned. For example, in the fact “The Eiffel Tower is located in Paris,” the subject is “Eiffel Tower,” and the associated information is “Paris.” The model recognizes this connection as a factual association, so when prompted with “The Eiffel Tower is located in…”, it outputs “Paris” based on this stored knowledge.</p> <p><strong>Finding engram cells</strong></p> <p>To edit a specific fact in the model, the authors first identified where that fact, or “engram,” is stored. Much like neuroscientists reactivating tagged engram cells to probe stored memories<d-cite key="liu2012optogenetic"></d-cite>, they exposed the model to an unrelated context or input prompt to identify where specific factual associations reside. For instance, to trace the memory for “The Eiffel Tower is located in Paris,” they first recorded the model’s firing patterns across all layers when prompted with this information. Then, they fed the model a different context or prompt, such as “The Colosseum is located in,” and reactivated a specific layer using only the previously stored firing patterns. They identified the layer where the model outputs “Paris” instead of “Rome” in the different context, indicating the presence of the Eiffel Tower’s engram. Their findings reveal that factual associations (i.e., engrams) are stored in the multilayer perceptron (MLP) modules of the middle layers, at the final subject token position.</p> <p><strong>Injecting a False Memory</strong></p> <p>Suppose memory A encodes the factual association “The Eiffel Tower is located in Paris,” and memory B encodes the factual association “The Colosseum is located in Rome.” (See <a href="#rome">Fig. 4</a> Top) Now, we want to inject a false memory into the model, such as “The Eiffel Tower is located in <em>Rome</em>.” To do this, we need to identify the specific engram and its corresponding firing pattern. The authors hypothesize that the MLP module can be modeled as linear associative memories. In the output linear layer of an MLP, a vector \(k\) serves as the input, and the value vector \(v = Wk\), where \(W\) represents the linear associative memory.</p> <p>To determine the firing pattern \(k_A\) for engram A, researchers use the prompt “The Eiffel Tower is located in” combined with various prefixed contexts. They calculate the \(k_A\) vector at the middle MLP layer where the factual association engram is located by averaging activity patterns generated from inputs with different prefixes, such as “The first thing. {prompt}” and “A few days ago. {prompt}.” By averaging the responses to these prefixed inputs, they obtain a reliable estimate of the \(k_A\) vector that represents the engram’s pattern in that layer. The corresponding value vector for this pattern is \(v_A = Wk_A\), which leads the model to output “Paris.”</p> <p>Next, to implant the false memory, the model modifies the output neurons \(v\) using optimization techniques to create a new vector \(v_B\) that leads the model to a false output “Rome.” By using gradient descent, \(v\) is iteratively adjusted until \(v_B\) reliably corresponds to “Rome.”</p> <p>After finding the firing patterns \(k_A\) and \(v_B\), the authors proposed a rank-one model editing (ROME) method (See <a href="#rome">Fig. 4</a> bottom). This method produces a weight change matrix, \(\Delta W\), which is then added to the original weight matrix, resulting in an updated matrix: \(\hat{W} = W + \Delta W\), where</p> \[\Delta W = \eta (v_B k_A^{T} - W k_A k_A^{T})C^{-1}\] <p>Here, \(C\) approximates the covariance of existing key vectors and \(C^{-1}\) is the inverse of the covariance matrix, directing the weight update in a way that minimizes changes to unrelated facts, and \(\eta\) is a constant.</p> <div id="rome"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-engram/rome_v1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-engram/rome_v1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-engram/rome_v1-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-engram/rome_v1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4. ROME (Rank-One Model Editing) method for memory editing in language models. The top section shows the original associations: "Eiffel Tower" with "Paris" (Memory A) and "Colosseum" with "Rome" (Memory B). ROME injects a false association, linking "Eiffel Tower" to "Rome." The bottom section illustrates the process: the original association $W_{AA} = v_A k_A^T$ is removed, and a new association $W_{AB} = v_B k_A^T$ is added. This update produces the edited weight matrix $\hat{W}$, resulting in the model associating "Eiffel Tower" with "Rome" instead of "Paris." </div> </div> <p>In neuroscience, <strong>Hebbian learning</strong> is summarized by “neurons that fire together, wire together.” Mathematically, this is often modeled as:</p> \[\Delta w_{ij} = \eta x_i y_j\] <p>where \(w_{ij}\) is the weight of the connection between neuron \(i\) and neuron \(j\), \(x_i\) and \(y_j\) are the activations of neurons \(i\) and \(j\), and \(\eta\) is a learning rate.</p> <p>This rule is similar to the update rule in ROME.</p> \[\Delta w_{ij} \propto [v_B k^{T}_A - W k_A k^{T}_A]_{ij} = (v_B)_i (k_A)_j - (v_A)_i (k_A)_j\] <p>In addition to injecting a new association (\(v_B k_A^{T}\)), ROME also effectively “removes” the old association by subtracting \(v_A k_A^{T}\). This process resembles anti-Hebbian unlearning, as it eliminates the previous association to ensure it is fully replaced by the new one. This update rule aligns ROME with the principle of “engram cells that fire together, wire together” <d-cite key="josselyn2020memory"></d-cite>.</p> <p>The theoretical foundations of the ROME method align with biological mechanisms demonstrated in neuroscience experiments. For instance, Ohkawa et al. (2015)<d-cite key="ohkawa2015artificial"></d-cite> explored how artificial associations between unrelated memories could be induced through synchronous activation of neuronal ensembles. Their research showed that hippocampal and amygdala engrams, representing neutral contexts and fear stimuli respectively, could be artificially linked in mice. By optogenetically co-activating these engrams, researchers created an artificial memory where the mice exhibited fear in a previously neutral context.</p> <p>This mechanism resembles the ROME method’s approach of pairing a specific \(k_A\) (representing the “neutral context” of the Eiffel Tower in Paris) with a new \(v_B\) (representing the fear-associated memory, or in this case, Rome). Both rely on synchronizing distinct ensembles—whether neurons in the brain or patterns in an artificial model—to generate a qualitatively new association.</p> <p>The findings from Ohkawa et al.<d-cite key="ohkawa2015artificial"></d-cite> provide a compelling biological analog to ROME’s editing principles, suggesting that artificial systems can mimic the brain’s capacity to update and reshape memory networks by associating novel information with pre-stored information.</p> <p>This exploration of the underlying mathematical framework reveals that ROME is not merely an engineering technique; it represents a sophisticated application of associative memory principles in AI.</p> <h2 id="concluding-remarks-lets-connect-ai-research-with-neuroscience">Concluding Remarks: Let’s connect AI research with neuroscience!</h2> <p>This blog gave a new interpretation of two LLM studies through the lens of neuroscience. Interestingly, although neuroscience and AI are conducted independently, LLM research can be viewed through the framework of neuroscience.</p> <p>LLMs are highly complex systems, and it is challenging to understand their processes and handling of information. Similarly, the brain is an equally complex system. Researchers in both domains may share a common goal: to understand these intricate “black box” systems. Achieving this understanding could enable us to edit or control LLMs to prevent harmful behaviors. In neuroscience, such insights could contribute to treatments for Alzheimer’s disease and other brain-related conditions.</p> <p>We are fascinated to find connection of the two introduced AI research with engrams. Moving forward, by integrating neuroscience insights with cutting-edge AI research, we may be able to uncover new ways to make LLMs more transparent, adaptable, and safe for everyday use. This effort might bring us closer to a future where AI models are as understandable and trustworthy as we hope our own minds can be.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2025/assets/bibliography/2025-04-28-engram.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>