<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},o=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),i=o[0][0],a=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"April 28, 2025"),l="Understanding Model Calibration - A gentle introduction and visual exploration of calibration and the expected calibration error\xa0(ECE)",r="To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some  issues with a measure that is still widely used to evaluate calibration.";{let e=o.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(i+"2025"+l.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${l}},\n  abstract = {${r}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${a}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=o.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${l}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Understanding Model Calibration - A gentle introduction and visual exploration of calibration and the expected calibration error (ECE) | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/2025/assets/css/main.css"> <link rel="canonical" href="https://starlight345.github.io/2025/blog/calibration/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/2025/assets/js/theme.js"></script> <script src="/2025/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2025/assets/js/distillpub/template.v2.js"></script> <script src="/2025/assets/js/distillpub/transforms.v2.js"></script> <script src="/2025/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <d-front-matter> <script async type="text/json">{
      "title": "Understanding Model Calibration - A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)",
      "description": "To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some  issues with a measure that is still widely used to evaluate calibration.",
      "published": "April 28, 2025",
      "authors": [
        {
          "author": "Maja Pavlovic",
          "authorURL": "https://majapavlo.github.io",
          "affiliations": [
            {
              "name": "Queen Mary University London",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/reviewing/">reviewing</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Understanding Model Calibration - A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)</h1> <p>To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#what-is-calibration">What is Calibration?</a></div> <ul> <li><a href="#confidence-calibration">(Confidence) Calibration</a></li> </ul> <div><a href="#evaluating-calibration-expected-calibration-error-ece">Evaluating Calibration - Expected Calibration Error (ECE)</a></div> <ul> <li><a href="#ece-visual-step-by-step-example">ECE - Visual Step by Step Example</a></li> </ul> <div><a href="#most-frequently-mentioned-drawbacks-of-ece">Most frequently mentioned Drawbacks of ECE</a></div> <ul> <li><a href="#pathologies-low-ece-high-accuracy">Pathologies - Low ECE ≠ high accuracy</a></li> <li><a href="#binning-approach">Binning Approach</a></li> <li><a href="#only-maximum-probabilities-considered">Only maximum probabilities considered</a></li> </ul> <div><a href="#final-thoughts">Final Thoughts</a></div> </nav> </d-contents> <h2 id="what-is-calibration">What is Calibration?</h2> <p><strong>Calibration</strong> makes sure that a model’s estimated probabilities match real-world likelihoods. For example, if a weather forecasting model predicts a 70% chance of rain on several days, then roughly 70% of those days should actually be rainy for the model to be considered well calibrated <d-cite key="dawid1982well, degroot1983comparison"></d-cite>. This makes model predictions more <em>reliable</em> and <em>trustworthy</em>, which makes calibration relevant for many applications across various domains.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f1_reliability_diagram-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f1_reliability_diagram-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f1_reliability_diagram-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f1_reliability_diagram.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 1 | Reliability Diagram </div> <p>Now, what <strong>calibration</strong> means more precisely depends on the specific definition being considered. We will have a look at the most common notion in machine learning (ML) formalised in <d-cite key="guo2017calibration"></d-cite> and termed <strong><em>confidence calibration</em></strong> in <d-cite key="kull2019beyond"></d-cite>. But first, let’s define a bit of formal notation for this blog. In this blogpost we consider a classification task with \(K\) possible classes, with labels \(Y \in \{1, ..., K\}\) and a classification model \(\hat{p} : \mathscr{X} \rightarrow \Delta^K\), that takes inputs in \(\mathscr{X}\) (e.g. an image or text) and returns a probability vector as its output. \(\Delta^K\) refers to the <em>K</em>-simplex, which just means that the elements of the output vector must sum to 1 and that each estimated probability in the vector is between 0 &amp; 1. These individual probabilities (<em>or confidences</em>) indicate how likely an input belongs to each of the \(K\) classes.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f2_notation_updated-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f2_notation_updated-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f2_notation_updated-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f2_notation_updated.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 2 | Notation - input example sourced from <d-cite key="uma2021learning"></d-cite> </div> <h3 id="confidence-calibration">(Confidence) Calibration</h3> <p>A model is considered confidence-calibrated if, for all confidences \(c,\) the model is correct \(c\) proportion of the time:</p> \[\mathbb{P} (Y = \text{arg max}(\hat{p}(X)) \; | \; \text{max}(\hat{p}(X))=c ) = c \;\;\:\: \forall c \in [0, 1] \; ,\] <div class="caption"> where $(X,Y)$ is a datapoint and $\hat{p} : \mathscr{X} \rightarrow \Delta^K$ returns a probability vector as its output </div> <p>This definition of calibration, ensures that the model’s final predictions align with their observed accuracy at that confidence level <d-cite key="guo2017calibration"></d-cite>. The left chart below visualises the perfectly calibrated outcome (green diagonal line) for all confidences using a binned reliability diagram <d-cite key="guo2017calibration"></d-cite>. On the right hand side it shows two examples for a specific confidence level across 10 samples.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f3_confidence_calibration-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f3_confidence_calibration-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f3_confidence_calibration-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f3_confidence_calibration.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 3 | Confidence Calibration </div> <p>For simplification, we assume that we only have 3 classes as in image 2 and we zoom into confidence \(c=0.7\), see image above. Let’s assume we have 10 inputs here whose most confident prediction (<em>max</em>) equals \(0.7\). If the model correctly classifies 7 out of 10 predictions (<em>true</em>), it is considered calibrated at confidence level \(0.7\). For the model to be fully calibrated this has to hold across all confidence levels from 0 to 1. At the same level \(c=0.7\), a model would be considered miscalibrated if it makes only 4 correct predictions.</p> <h2 id="evaluating-calibration-expected-calibration-error-ece">Evaluating Calibration - Expected Calibration Error (ECE)</h2> <p>One widely used evaluation measure for confidence calibration is the Expected Calibration Error (ECE) <d-cite key="naeini2015obtaining, guo2017calibration"></d-cite>. ECE measures how well a model’s estimated probabilities match the observed probabilities by taking a weighted average over the absolute difference between average accuracy (<em>acc</em>) and average confidence (<em>conf</em>). The measure involves splitting all \(n\) datapoints into M equally spaced bins:</p> \[ECE = \sum_{m=1}^M \frac{\mathopen| B_m \mathclose|}{n} \mathopen| acc(B_m) - conf(B_m) \mathclose| ,\] <p>where \(B\) is used for representing “bins” and \(m\) for the bin number, while <em>acc</em> and <em>conf</em> are:</p> \[\small{ acc(B_m) = \frac{1}{ \mathopen| B_m \mathclose|} \sum_{i\in B_m} \mathbb{1} (\hat{y}_i = y_i ) \;\: \text{&amp;} \;\: conf(B_m) = \frac{1}{ \mathopen| B_m \mathclose|} \sum_{i\in B_m} \hat{p}(x_i) }\] <p>\(\hat{y}_i\) is the model’s predicted class (<em>arg max</em>) for sample \(i\) and \(y_i\) is the true label for sample \(i\). \(\mathbb{1}\) is an indicator function, meaning when the predicted label \(\hat{y}_i\) equals the true label \(y_i\) it evaluates to 1, otherwise 0. Let’s look at an example, which will clarify <em>acc</em>, <em>conf</em> and the whole binning approach in a visual step-by-step manner.</p> <h3 id="ece---visual-step-by-stepexample">ECE - Visual Step by Step Example</h3> <p>In the image below, we can see that we have \(9\) samples indexed by \(i\) with estimated probabilities \(\hat{p}(x_i)\) (simplified as \(\hat{p}_i\)) for class <strong>cat (C)</strong>, <strong>dog (D)</strong> or <strong>toad (T)</strong>. The final column shows the true class \({y}_i\) and the penultimate column contains the predicted class \(\hat{y}_i\).</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f4_ece_table1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f4_ece_table1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f4_ece_table1-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f4_ece_table1.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 1 | ECE - Toy Example </div> <p>Only the maximum probabilities, which determine the predicted label are used in ECE <d-cite key="guo2017calibration"></d-cite>. Therefore, we will only bin samples based on the maximum probability across classes (<em>see left table in below image</em>). To keep the example simple we split the data into 5 <strong>equally spaced</strong> bins \(M=5\). If we now look at each sample’s maximum estimated probability, we can group it into one of the 5 bins (<em>see right side of image below</em>).</p> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f5_ece_table2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f5_ece_table2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f5_ece_table2-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f5_ece_table2.png" class="img-fluid rounded " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Table 2 &amp; Binning Diagram </div> <p>We still need to determine if the predicted class is correct or not to be able to determine the average accuracy per bin. If the model predicts the class correctly (i.e. \(y_i =\hat{y}_i\)), the prediction is highlighted in green; incorrect predictions are marked in red:</p> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f6_ece_table3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f6_ece_table3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f6_ece_table3-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f6_ece_table3.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Table 3 &amp; Binning Diagram </div> <p>We now have visualised all the information needed for ECE and will briefly run through how to calculate the values for bin 5 (\(B_5\)). The other bins then simply follow the same process, see below.</p> <div class="l-page-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f7_ece_table4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f7_ece_table4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f7_ece_table4-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f7_ece_table4.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Table 4 &amp; Example for bin 5 </div> <p>We can get the empirical probability of a sample falling into \(B_5\) , by assessing how many out of all \(9\) samples fall into \(B_5\), see \(\mathbf{(\;1\;)}\). We then get the average accuracy for \(B_5\), see \(\mathbf{(\;2\;)}\) and lastly the average estimated probability for \(B_5\), see \(\mathbf{(\;3\;)}\). Repeat this for all bins and in our small example of \(9\) samples we end up with an ECE of \(0.10445\). A perfectly calibrated model would have an ECE of 0.</p> <h4 id="expected-calibration-error-drawbacks">Expected Calibration Error Drawbacks</h4> <p>The images of binning above provide a visual guide of how ECE could result in very different values if we used more bins or perhaps binned the same number of items instead of using equal bin widths. Such and more drawbacks of ECE have been highlighted by several works early on <d-cite key="kumar2018trainable, nixon2019measuring, gupta2020calibration, zhang2020mix, roelofs2022mitigating, vaicenavicius2019evaluating, widmann2019calibration, futami2024informationtheoretic"></d-cite>. However, despite the known weaknesses ECE is still widely used to evaluate confidence calibration in ML <d-cite key="xiong2023can, yuan2024does, collins2023human, si2023prompting, mukhoti2023deep, gao2024spuq"></d-cite>. This motivated this blogpost, with the idea to highlight the most frequently mentioned drawbacks of ECE visually and to provide a simple clarification on the development of different notions of calibration.</p> <h2 id="most-frequently-mentioned-drawbacks-ofece">Most frequently mentioned Drawbacks of ECE</h2> <h3 id="pathologies---low-ece--highaccuracy">Pathologies - Low ECE ≠ high accuracy</h3> <p>A model which minimises ECE, does not necessarily have a high accuracy <d-cite key="kumar2018trainable, kull2019beyond, si2022re"></d-cite>. For instance, if a model always predicts the majority class with that class’s average prevalence as the probability, it will have an ECE of 0. This is visualised in the image above, where we have a dataset with 10 samples, 7 of those are cat, 2 dog and only one is a toad. Now if the model always predicts cat with on average 0.7 confidence it would have an ECE of 0. There are more of such pathologies <d-cite key="nixon2019measuring"></d-cite>. To not only rely on ECE, some researchers use additional measures such as the Brier score or LogLoss alongside ECE <d-cite key="kumar2018trainable, kull2019beyond"></d-cite>.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f8_pathologies-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f8_pathologies-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f8_pathologies-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f8_pathologies.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 4 | Pathologies Example </div> <h3 id="binning-approach">Binning Approach</h3> <p>One of the most frequently mentioned issues with ECE is its sensitivity to the change in binning <d-cite key="kumar2018trainable, nixon2019measuring, gupta2020calibration, zhang2020mix, roelofs2022mitigating, famiglini2023towards"></d-cite>. This is sometimes referred to as the <strong><em>Bias-Variance trade-off</em></strong> <d-cite key="nixon2019measuring, zhang2020mix"></d-cite>: Fewer bins reduce variance but increase bias, while more bins lead to sparsely populated bins increasing variance. If we look back to our ECE example with 9 samples and change the bins from 5 to 10 here too, we end up with the following:</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f9_binning_1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f9_binning_1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f9_binning_1-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f9_binning_1.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 5 | More Bins </div> <p>We can see that bin <em>8</em> and <em>9</em> each contain only a single sample and also that half the bins now contain no samples. The above is only a toy example, however since modern models tend to have higher confidence values samples often end up in the last few bins <d-cite key="naeini2015obtaining, zhang2020mix"></d-cite>, which means they get all the weight in ECE, while the average error for the empty bins contributes 0 to ECE.</p> <p>To mitigate these issues of fixed bin widths some authors <d-cite key="nixon2019measuring, roelofs2022mitigating"></d-cite> have proposed a more adaptive binning approach.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f10_binning_2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f10_binning_2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f10_binning_2-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f10_binning_2.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 6 | Adaptive Bins </div> <p>Binning-based evaluation with bins containing an equal number of samples are shown to have <em>lower bias</em> than a fixed binning approach such as ECE <d-cite key="roelofs2022mitigating"></d-cite>. This leads <d-cite key="roelofs2022mitigating"></d-cite> to urge against using equal width binning and to suggest the use of an alternative: ECEsweep, which maximizes the number of equal-mass bins while ensuring the calibration function remains monotonic <d-cite key="roelofs2022mitigating"></d-cite>. The Adaptive Calibration Error (ACE) and Threshold Adaptive calibration Error (TACE) are two other variations of ECE that use flexible binning <d-cite key="nixon2019measuring"></d-cite>. However, some find it sensitive to the choice of bins and thresholds, leading to inconsistencies in ranking different models <d-cite key="ashukha2020pitfalls"></d-cite>. Two other approaches aim to eliminate binning altogether: MacroCE does this by averaging over instance-level calibration errors of correct and wrong predictions <d-cite key="si2022re"></d-cite> and the KDE-based ECE does so by replacing the bins with non-parametric density estimators, specifically kernel density estimation (KDE) <d-cite key="zhang2020mix"></d-cite>.</p> <h3 id="only-maximum-probabilities-considered">Only maximum probabilities considered</h3> <p>Another frequently mentioned drawback of ECE is that it only considers the maximum estimated probabilities <d-cite key="nixon2019measuring, ashukha2020pitfalls, vaicenavicius2019evaluating, widmann2019calibration, kull2019beyond"></d-cite>. The idea that more than just the maximum confidence should be calibrated, is best illustrated with a simple example <d-cite key="vaicenavicius2019evaluating"></d-cite>:</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f11_max_probs_only_updated-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f11_max_probs_only_updated-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f11_max_probs_only_updated-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f11_max_probs_only_updated.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 7 | input example sourced from <d-cite key="schwirten2024ambiguous"></d-cite> </div> <p>Let’s say we trained two different models and now both need to determine if the same input image contains a <em>person</em>, an <em>animal</em> or <em>no creature</em>. The two models output vectors with slightly different estimated probabilities, but both have the same maximum confidence for “<em>no creature</em>”. Since ECE only looks at these top values it would consider these two outputs to be the same. Yet, when we think of real-world applications we might want our self-driving car to act differently in one situation over the other <d-cite key="vaicenavicius2019evaluating"></d-cite>. This restriction to the maximum confidence prompted various authors <d-cite key="vaicenavicius2019evaluating, kull2019beyond, widmann2019calibration"></d-cite> to reconsider the definition of calibration. The existing concept of calibration as “confidence calibration” (coined in <d-cite key="kull2019beyond"></d-cite>) makes a distinction between two additional interpretations of confidence: <strong>multi-class</strong> and <strong>class-wise calibration</strong>.</p> <p><br></p> <h4 id="multi-class-calibration">Multi-class Calibration</h4> <p>A model is considered multi-class calibrated if, for any prediction vector \(q=(q_1,...,q_K) \in \Delta^K\)​, the class proportions among all values of \(X\) for which a model outputs the same prediction \(\hat{p}(X)=q\) match the values in the prediction vector \(q\).</p> \[\mathbb{P} (Y = k \; | \; \hat{p}(X)=q ) = q_k \;\;\;\:\:\:\: \forall k \in \{1,...,K\}, \; \forall q \in \Delta^K\] <div class="caption"> where $(X,Y)$ is a datapoint and $\hat{p} : \mathscr{X} \rightarrow \Delta^K$ returns a probability vector as its output </div> <p>What does this mean in simple terms? Instead of \(c\) we now calibrate against a vector \(q\), with K classes. Let’s look at an example below:</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f12_multi-class-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f12_multi-class-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f12_multi-class-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f12_multi-class.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 8 | Multi-class Calibration </div> <p>On the left we have the space of all possible prediction vectors. Let’s zoom into one such vector that our model predicted and say the model has 10 instances for which it predicted the vector \(q\)=[0.1,0.2,0.7]. Now in order for it to be multi-class calibrated, the distribution of the true (<em>actual</em>) class needs to match the prediction vector \(q\). The image above shows a calibrated example with [0.1,0.2,0.7] and a not calibrated case with [0.1,0.5,0.4].</p> <p><br></p> <h4 id="class-wise-calibration">Class-wise Calibration</h4> <p>A model is considered class-wise calibrated if, for each class k, all inputs that share an estimated probability \(\hat{p}_k(X)\) align with the true frequency of class k when considered on its own:</p> \[\mathbb{P} (Y = k \; | \; \hat{p}_k(X)= q_k ) = q_k \;\;\;\;\;\; \forall k \in \{1,...,K\}\] <div class="caption"> where $(X,Y)$ is a datapoint; $q \in \Delta^K $ and $\hat{p} : \mathscr{X} \rightarrow \Delta^K$ returns a probability vector as its output </div> <p>Class-wise calibration is a <strong><em>weaker</em></strong> definition than <strong>multi-class</strong> calibration as it considers each class probability in <strong><em>isolation</em></strong> rather than needing the full vector to align. The image below illustrates this by zooming into a probability estimate for class 1 specifically: \(q_1=0.1\). Yet again, we assume we have 10 instances for which the model predicted a probability estimate of 0.1 for class 1. We then look at the true class frequency amongst all classes with \(q_1=0.1\). If the empirical frequency matches \(q_1\) it is calibrated.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f13_class-wise-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f13_class-wise-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f13_class-wise-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f13_class-wise.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 9 | Class-wise Calibration </div> <p>To evaluate such different notions of calibration, some updates are made to ECE to calculate a class-wise error. One idea is to calculate the ECE for each class and then take the average <d-cite key="nixon2019measuring, kull2019beyond"></d-cite>. Another idea is to swap the L1-distance used in ECE with the L2 and use several ECE metrics to more effectively assess the overall level of calibration <d-cite key="famiglini2023towards"></d-cite>. Others, introduce the use of the KS-test for class-wise calibration <d-cite key="gupta2020calibration"></d-cite> and <d-cite key="vaicenavicius2019evaluating"></d-cite> also suggest using statistical hypothesis tests instead of ECE based approaches. And other researchers develop a hypothesis test framework [TCal] to detect whether a model is significantly mis-calibrated <d-cite key="donghwan2023tcal"></d-cite> and <d-cite key="sun2024confidenceintervalell2expected"></d-cite> build on this by developing confidence intervals for the L2-ECE.</p> <p>All the approaches mentioned above <strong>share a key assumption: ground-truth labels are available</strong>. Within this gold-standard mindset a prediction is either true or false. However, annotators might unresolvably and justifiably disagree on the real label <d-cite key="aroyo2015truth, uma2021learning"></d-cite>. Let’s look at a simple example below:</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f14_one_hot-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f14_one_hot-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f14_one_hot-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f14_one_hot.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 10 | One-Hot-Vector </div> <p>We have the same image as in our entry example and can see that the chosen label differs between annotators. A common approach to resolving such issues in the labelling process is to use some form of aggregation <d-cite key="paun2022statistical, artstein2008inter"></d-cite>. Let’s say that in our example the majority vote is selected, so we end up evaluating how well our model is calibrated against such ‘ground truth’. One might think, the image is small and pixelated; of course humans will not be certain about their choice. However, rather than being an exception such disagreements are widespread <d-cite key="aroyo2024dices, sanders2022ambiguous, schwirten2024ambiguous"></d-cite>. So, when there is a lot of human disagreement in a dataset it might not be a good idea to calibrate against an aggregated ‘gold’ label <d-cite key="baan2022stop"></d-cite>. Instead of gold labels more and more researchers are using soft or smooth labels which are more representative of the human uncertainty <d-cite key="peterson2019cifar, sanders2022ambiguous, collins2023human"></d-cite>, see example below.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f15_soft_label-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f15_soft_label-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f15_soft_label-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f15_soft_label.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 11 | Soft-Label </div> <p>In the same example as above, instead of aggregating the annotator votes we could simply use their frequencies to create a distribution \(P_{vote}\) over the labels instead, which is then our new \(y_i\). This shift towards training models on collective annotator views, rather than relying on a single source-of-truth motivates another definition of calibration: calibrating the model against human uncertainty <d-cite key="baan2022stop"></d-cite>.</p> <p><br></p> <h4 id="human-uncertainty-calibration">Human Uncertainty Calibration</h4> <p>A model is considered human-uncertainty calibrated if, for each specific sample \(x\), the predicted probability for each class k matches the ‘<em>actual</em>’ probability \(P_{vote}\) of that class being correct.</p> \[\mathbb{P}_{vote} (Y = k \; | \; X = x ) = \hat{p}_k(x) \;\;\;\;\; \forall k \in \{1,...,K\}\] <div class="caption"> where $(X,Y)$ is a datapoint and $\hat{p} : \mathscr{X} \rightarrow \Delta^K$ returns a probability vector as its output </div> <p>This interpretation of calibration aligns the model’s prediction with human uncertainty, which means each prediction made by the model is individually reliable and matches human-level uncertainty for that instance. Let’s have a look at an example below:</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f16_human_uncertainty-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f16_human_uncertainty-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f16_human_uncertainty-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f16_human_uncertainty.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 12 | Human Uncertainty Calibration </div> <p>We have our sample data (<em>left</em>) and zoom into a single sample \(x\) with index \(i=1\). The model’s predicted probability vector for this sample is [0.1,0.2,0.7]. If the human labelled distribution \(y_i\) matches this predicted vector then this sample is considered calibrated.</p> <p>This definition of calibration is more granular and strict than the previous ones as it applies directly at the level of individual predictions rather than being averaged or assessed over a set of samples. It also relies heavily on having an accurate estimate of the human judgement distribution, which requires a large number of annotations per item. Datasets with such properties of annotations are gradually becoming more available <d-cite key="aroyo2024dices, nie2020learn"></d-cite>.</p> <p>To evaluate human uncertainty calibration three new measures are introduced in <d-cite key="baan2022stop"></d-cite> : <strong>the Human Entropy Calibration Error <em>(EntCE)</em>, the Human Ranking Calibration Score <em>(RankCS)</em> and the Human Distribution Calibration Error <em>(DistCE)</em></strong>.</p> \[EntCE(x_i)= H(y_i) - H(\hat{p}_i),\] <p>where \(H(.)\) signifies entropy.</p> <p><strong>EntCE</strong> aims to capture the agreement between the model’s uncertainty \(H(\hat{p}_i)\) and the human uncertainty \(H(y_i)\) for a sample \(i\). However, entropy is invariant to the permutations of the probability values; in other words it doesn’t change when you rearrange the probability values. This is visualised in the image below:</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f17_entce-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f17_entce-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f17_entce-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f17_entce.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 13 | EntCE drawbacks </div> <p>On the left, we can see the human label distribution \(y_i\) , on the right are two different model predictions for that same sample. All three distributions would have the same entropy, so comparing them would result in 0 EntCE. While this is not ideal for comparing distributions, entropy is still helpful in assessing the noise level of label distributions.</p> <p>\(RankCS = \frac{1}{N} \sum_{n=1}^{N} \mathbf{1} (argsort(y_i) = argsort(\hat{p}_i)),\) <br> </p> <p>where argsort simply returns the indices that would sort an array.</p> <p>So, <strong>RankCS</strong> checks if the sorted order of estimated probabilities \(\hat{p}_i\) matches the sorted order of \(y_i\) for each sample. If they match for a particular sample \(i\) one can count it as 1; if not, it can be counted as 0, which is then used to average over all samples N. <d-footnote>In the paper it is stated more generally: If the argsorts match, it means the ranking is aligned, contributing to the overall RankCS score.</d-footnote></p> <p>Since this approach uses ranking it doesn’t care about the actual size of the probability values. The two predictions below, while not the same in class probabilies would have the same ranking. This is helpful in assessing the overall ranking capability of models and looks beyond just the maximum confidence. At the same time though, it doesn’t fully capture human uncertainty calibration as it ignores the actual probability values.</p> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/f18_rankcs-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/f18_rankcs-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/f18_rankcs-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/f18_rankcs.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 14 | RankCS drawbacks </div> \[DistCE(x_i) = \mathbf{TVD}(y_i, \hat{p}_i)\] <p><strong>DistCE</strong> has been proposed as an additional evaluation for this notion of calibration. It simply uses the total variation distance \((TVD)\) between the two distributions, which aims to reflect how much they diverge from one another. <em>DistCE</em> and <em>EntCE</em> capture instance level information. So to get a feeling for the full dataset one can simply take the average expected value over the absolute value of each measure: \(E[\mid DistCE \mid]\) and \(E[\mid EntCE \mid]\). Perhaps future efforts will introduce further measures that combine the benefits of ranking and noise estimation for this notion of calibration.</p> <hr> <div class="row mt-2"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-calibration/calibration_overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-calibration/calibration_overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-calibration/calibration_overview-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-calibration/calibration_overview.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image 15 | Visual Summary: Definitions at a Glance </div> <h2 id="final-thoughts">Final Thoughts</h2> <p>We have run through the most common definition of calibration, the shortcomings of ECE and additional notions of calibration: multi-class, class-wise &amp; human-uncertainty calibration. We also touched on some of the newly proposed evaluation measures and their shortcomings. Despite several works arguing against the use of ECE for evaluating calibration, it remains widely used. The aim of this blogpost is to draw attention to these works and their alternative approaches. Determining which notion of calibration best fits a specific context and how to evaluate it should avoid misleading results. Maybe, however, ECE is simply so easy, intuitive and just good enough for most applications that it is here to stay?</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2025/assets/bibliography/2025-04-28-calibration.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>