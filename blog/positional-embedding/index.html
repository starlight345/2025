<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},i=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),o=i[0][0],a=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"April 28, 2025"),r="Positional Embeddings in Transformer Models: Evolution from Text to Vision Domains",s="Positional encoding has become an essential element in transformer models, addressing their fundamental property of permutation invariance and allowing them to understand sequential relationships within data. This blog post examines positional encoding techniques, emphasizing their vital importance in traditional transformers and their use with 2D data in Vision Transformers (ViT). We explore two contemporary methods\u2014ALiBi (Attention with Linear Biases) and RoPE (Rotary Position Embedding)\u2014analyzing their unique approaches to tackling the challenge of sequence length extrapolation during inference, a significant issue for transformers. Additionally, we compare these methods' fundamental similarities and differences, assessing their impact on transformer performance across various fields. We also look into how interpolation strategies have been utilized to enhance the extrapolation capabilities of these methods; we conclude this blog with an empirical comparison of ALiBi and RoPE in Vision Transformers. To the best of our knowledge, this represents the first direct comparison of these positional encoding methods with those used in standard Vision Transformers.";{let e=i.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(o+"2025"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${r}},\n  abstract = {${s}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${a}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=i.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${r}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Positional Embeddings in Transformer Models: Evolution from Text to Vision Domains | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Positional encoding has become an essential element in transformer models, addressing their fundamental property of permutation invariance and allowing them to understand sequential relationships within data. This blog post examines positional encoding techniques, emphasizing their vital importance in traditional transformers and their use with 2D data in Vision Transformers (ViT). We explore two contemporary methods—ALiBi (Attention with Linear Biases) and RoPE (Rotary Position Embedding)—analyzing their unique approaches to tackling the challenge of sequence length extrapolation during inference, a significant issue for transformers. Additionally, we compare these methods' fundamental similarities and differences, assessing their impact on transformer performance across various fields. We also look into how interpolation strategies have been utilized to enhance the extrapolation capabilities of these methods; we conclude this blog with an empirical comparison of ALiBi and RoPE in Vision Transformers. To the best of our knowledge, this represents the first direct comparison of these positional encoding methods with those used in standard Vision Transformers."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/2025/assets/css/main.css"> <link rel="canonical" href="https://starlight345.github.io/2025/blog/positional-embedding/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/2025/assets/js/theme.js"></script> <script src="/2025/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2025/assets/js/distillpub/template.v2.js"></script> <script src="/2025/assets/js/distillpub/transforms.v2.js"></script> <script src="/2025/assets/js/distillpub/overrides.js"></script> <style type="text/css">.caption{font-size:80%;line-height:1.2;text-align:left}</style> <d-front-matter> <script async type="text/json">{
      "title": "Positional Embeddings in Transformer Models: Evolution from Text to Vision Domains",
      "description": "Positional encoding has become an essential element in transformer models, addressing their fundamental property of permutation invariance and allowing them to understand sequential relationships within data. This blog post examines positional encoding techniques, emphasizing their vital importance in traditional transformers and their use with 2D data in Vision Transformers (ViT). We explore two contemporary methods—ALiBi (Attention with Linear Biases) and RoPE (Rotary Position Embedding)—analyzing their unique approaches to tackling the challenge of sequence length extrapolation during inference, a significant issue for transformers. Additionally, we compare these methods' fundamental similarities and differences, assessing their impact on transformer performance across various fields. We also look into how interpolation strategies have been utilized to enhance the extrapolation capabilities of these methods; we conclude this blog with an empirical comparison of ALiBi and RoPE in Vision Transformers. To the best of our knowledge, this represents the first direct comparison of these positional encoding methods with those used in standard Vision Transformers.",
      "published": "April 28, 2025",
      "authors": [
        {
          "author": "Abhinav Kumar",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Indian Institute of Technology, Roorkee",
              "url": ""
            }
          ]
        },
        {
          "author": "Adesh Gupta",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Indian Institute of Technology, Roorkee",
              "url": ""
            }
          ]
        },
        {
          "author": "Mansi Gupta",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Indian Institute of Technology, Roorkee",
              "url": ""
            }
          ]
        },
        {
          "author": "Shivank Garg",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Indian Institute of Technology, Roorkee",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/reviewing/">reviewing</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Positional Embeddings in Transformer Models: Evolution from Text to Vision Domains</h1> <p>Positional encoding has become an essential element in transformer models, addressing their fundamental property of permutation invariance and allowing them to understand sequential relationships within data. This blog post examines positional encoding techniques, emphasizing their vital importance in traditional transformers and their use with 2D data in Vision Transformers (ViT). We explore two contemporary methods—ALiBi (Attention with Linear Biases) and RoPE (Rotary Position Embedding)—analyzing their unique approaches to tackling the challenge of sequence length extrapolation during inference, a significant issue for transformers. Additionally, we compare these methods' fundamental similarities and differences, assessing their impact on transformer performance across various fields. We also look into how interpolation strategies have been utilized to enhance the extrapolation capabilities of these methods; we conclude this blog with an empirical comparison of ALiBi and RoPE in Vision Transformers. To the best of our knowledge, this represents the first direct comparison of these positional encoding methods with those used in standard Vision Transformers.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#background">Background</a></div> <div><a href="#preliminaries">Preliminaries</a></div> <div><a href="#absolute-positional-embeddings">Absolute Positional Embeddings</a></div> <div><a href="#relative-positional-embeddings">Relative Positional Embeddings</a></div> <div><a href="#positional-encoding-in-vision-transformers">Positional encoding in Vision Transformers</a></div> <div><a href="#rotary-positional-embeddings">Rotary Positional Embeddings</a></div> <ul> <li><a href="#the-mechanism-of-rope">The mechanism of RoPE</a></li> <li><a href="#the-two-dimensional-case">The two-dimensional case</a></li> <li><a href="#the-general-d-dimensional-case">The general D-dimensional case</a></li> </ul> <div><a href="#rope-in-vision-trasformers">RoPE in Vision Trasformers</a></div> <div><a href="#attention-with-linear-biases-alibi">Attention with Linear Biases (Alibi)</a></div> <ul> <li><a href="#mechanism-and-architecture">Mechanism and Architecture</a></li> <li><a href="#what-is-m">What is m?</a></li> <li><a href="#theoretical-foundation-and-advantages">Theoretical Foundation and Advantages</a></li> <li><a href="#performance-and-extrapolation">Performance and Extrapolation</a></li> </ul> <div><a href="#alibi-in-vision-transformers">Alibi in Vision Transformers</a></div> <div><a href="#comparative-analysis-rope-and-alibi">Comparative Analysis: RoPE and ALiBi</a></div> <div><a href="#extrapolation-via-interpolation">Extrapolation via Interpolation</a></div> <div><a href="#experimental-evaluation">Experimental Evaluation</a></div> <div><a href="#conclusions">Conclusions</a></div> </nav> </d-contents> <h2 id="background">Background</h2> <p>“Attention Is All You Need,” introduced by (Vaswani et al. [2017] <d-cite key="46201"></d-cite>), revolutionized the field of natural language processing and computer vision by proposing a purely attention-based model, the Transformer, an efficient alternative for recurrent or convolutional networks.</p> <p>Later, (Dosovitskiy et al. [2021] <d-cite key="50650"></d-cite>) applied this concept to computer vision, introducing Vision Transformers (ViTs). On its own, the Transformer architecture is position-invariant, i.e., it processes its input as an unordered set. Unlike RNNs, designed to handle ordered sequences, or CNNs, which leverage translation equivariance and locality, transformers lack the inherent ability to capture sequential patterns. This is because the self-attention mechanism is independent of the token index.</p> <p>(Vaswani et al. [2017] <d-cite key="46201"></d-cite>) introduced positional encoding to address this lack of inductive bias. Since then, researchers have explored various methods to encode positional information directly into tokens or within the self-attention interaction step. The following sections will discuss the broad advancements of positional encoding methods in 1D and their extension for 2D tasks in Vision Transformers.</p> <h2 id="preliminaries">Preliminaries</h2> <h3 id="self-attention-mechanism">Self-Attention Mechanism:</h3> <p>The self-attention mechanism processes sequences by representing each element as a vector of embedding dimension \(d\). For a sequence of length \(L\) and embedding dimension \(d\), the input is represented as a matrix \(X \in \mathbb{R}^{L \times d}\).</p> <p>The mechanism employs three learnable weight matrices: \(W_Q,W_K,\) and \(W_V\) \(\in \mathbb{R}^{d \times d_{model}}\), where \(d_{model}\) represents the dimensionality of the projected subspaces. These matrices transform the input into queries, keys, and values respectively:</p> \[Q = X\,W_Q \;\;\;\;\;\;\;\; K = X \, W_K \;\;\;\;\;\;\;\; V = X \, W_V\] <p>The attention matrix is computed using the following formula:</p> \[\text{Attention}(Q, K) = \text{softmax}(QK^T/√d_{model})\] \[\mathbf{Z} = \text{Attention}(Q, K) \cdot V\] <p>With respect to each token, let \(x_i\) be the context vector for \(i^{\text{th}}\) token, the corresponding query, key and value vectors, \(\mathbf{q_i}, \mathbf{k_i}\) and \(\mathbf{v_i}\) are defined as follows:</p> \[\mathbf{q_i} = x_i \: W_Q \; \; \; \; \; \; \mathbf{k_i} = x_i \: W_K \; \; \; \; \; \; \mathbf{v_i} = x_i\:W_V\] <p>and \(N\) being total number of tokens , the attention score between the \(m_{th}\) token (query) and the \(n_{th}\) token (key) is formulated as</p> \[a_{m,n} = \frac{\exp\left( \frac{\mathbf{q}_m \mathbf{k}_n^\top}{\sqrt{d}} \right)}{\sum_{j=1}^{N} \exp\left( \frac{\mathbf{q}_m \mathbf{k}_j^\top}{\sqrt{d}} \right)}\] <p>and \(\mathbf{z}_m\) (output vector) for the \(m_{th}\) token is calculated as</p> \[\mathbf{z}_m = \sum_{n=1}^{N} a_{m,n} \mathbf{v}_n\] <h3 id="multi-head-attention">Multi-Head Attention</h3> <p>In multi-head attention, the computation is extended across h parallel attention heads. Each \(head_i\) operates as:</p> \[\text{head}_i = \text{Attention}(Q\,W_Q^i, K\,W_K^i, V\,W_V^i)\] <p>The outputs from all heads are concatenated and projected back to the original dimensionality:</p> \[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1,\ldots,\text{head}_h)\:W_O\] <p>where \(W_O \in \mathbb{R}^{hd_k \times d_{model}} , W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k} , W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}, \ W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_k}, \ \text{and } W^O \in \mathbb{R}^{h d_k \times d_{\text{model}}}\) are learned matrices. Note \(d_{model} = h \times d_{k}\) and \(Q,K,V \in \mathbb{R}^{ L\times d_{\text{model}}}\)</p> <h2 id="absolute-positional-embeddings">Absolute Positional Embeddings</h2> <p>Absolute Positional embedding was introduced by (Vaswani et al. [2017] <d-cite key="46201"></d-cite>) in their famous 2017 paper “Attention is All You Need”. It involves the direct addition of positional embeddings into the embedding vector. These encodings are injected only once in the embedding vector before passing them into the transformer block. Mathematically, it’s equivalent to </p> \[x'_i := x_i + p_i\] <p>Where \(p_i\) is the positional embedding vector and \(x_i\) is the context vector corresponding to the \(i_{th}\) token. Note that \(x '_i,x_i ,p_i \in \mathbb{R}^d\). (Vaswani et al. [2017] <d-cite key="46201"></d-cite>), proposed the following formula for \(p_i\)</p> \[\left\{ \begin{array}{ll} p_{i,2t} = \sin(i/10000^{2t/d}) &amp; \\ p_{i,2t+1} = \cos(i/10000^{2t/d}) \\ \end{array} \right.\] <p>where \(p_{i,t}\) is the \(t^{th}\) element in the \(p_i\) vector.</p> <div class="l-page"> <iframe src="/2025/assets/html/2025-04-28-positional-embedding/sinusoidal.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption"> Figure 1: Plot of sinusoidal positional embeddings, $i$ (position in sequence) vs $t$ (index in embedding vector). Each vertical strip can be seen as an absolute position embedding vector to be added. </div> <p>One might question why a function of sines and cosines was chosen to represent positional encodings in a sequence—why not any other arbitrary function? The choice of sinusoidal functions was actually a strategic one, offering several advantages that benefit transformer models:</p> <ul> <li>Firstly the values are normalized between \([-1,1]\) allowing the model to learn parameters easily.</li> <li>The distance between neighboring positions is symmetrical and decays naturally with position.</li> <li>The formulation shows that every two consecutive elements in the embedding \(p_i\) share the same frequency denominator. For a pair with frequency \(w_k\), we can demonstrate that</li> </ul> \[\begin{array}{c}p_i \cdot p_{i+\phi} = \begin{bmatrix} \sin(iw_k) \\ \cos(iw_k) \end{bmatrix} \cdot \begin{bmatrix} \sin((i+\phi)w_k) \\ \cos((i+\phi)w_k) \end{bmatrix} \\[12pt] = \cos((i+\phi)w_k)\cos(iw_k) + \sin((i+\phi)w_k)\sin(iw_k) = \cos(\phi \: w_k)\end{array}\] <p>Hence the dot product \(p_i \cdot p_{i+\phi}\) is independent of position \(i\), and relative position \(\phi\) is retained. This consistency allows the model to capture regular intervals and patterns, making it especially effective for tasks requiring sequential dependencies.</p> <p>The authors hypothesized that sinusoidal positional encoding would enable the model to learn relative positions naturally, thanks to the properties outlined above, and might also allow it to extrapolate to sequence lengths longer than those seen during training. This rationale led to sinusoidal positional embeddings being one of the first methods adopted in transformer architectures.</p> <p>However, subsequent research challenged these assumptions. Studies found that absolute sinusoidal positional encoding was not well-suited for capturing relative positional information effectively (Shaw et al. [2018] <d-cite key="46989"></d-cite>). Moreover, it struggled with extrapolation, leading to poor performance on sequence lengths longer than those encountered in training (Press et al. [2021] <d-cite key="press2021train"></d-cite>).</p> <h3 id="learned-positional-embeddings">Learned Positional Embeddings</h3> <p>In addition to sinusoidal positional embedding, (Vaswani et al. [2017] <d-cite key="46201"></d-cite>) also explored learned positional encoding. They explored using a set of trainable vectors \(p \in \mathbb{R}^{L \times d}\) where \(L\) represents the maximum sequence length as positional embeddings. This method was applied in the (BERT [2018] <d-cite key="devlin2018bert"></d-cite>) and (GPT [2019] <d-cite key="radford2019language"></d-cite>) models as an alternative to sinusoidal positional encoding. However, it did not yield significant performance improvements over sinusoidal positional embedding. Moreover, the upper bound \(L\) limited the method’s ability to extrapolate to sequences longer than \(L\) without using interpolation techniques. Also, it introduces more trainable parameters, which increases the model’s size and computational cost.</p> <h2 id="relative-positional-embeddings">Relative Positional Embeddings</h2> <p>Unlike absolute positional encodings, which create embeddings for each position independently, relative positional embeddings focus on capturing the pairwise relationships between tokens in a sequence. Also rather than directly adding the embeddings to the context vectors, the relative positional information is added to keys and values during the attention calculation. Hence</p> \[\mathbf{q_m} = x_m \:W_Q \; \; \; \; \; \; \mathbf{k_n'} = (x_n+ \tilde{p}_r^k)\:W_K\; \; \; \; \; \; \mathbf{v_n'} = (x_n+ \tilde{p}_r^v)\:W_V\] <p>where \(\tilde{\mathbf{p}}_r^k, \tilde{\mathbf{p}}_r^v \in \mathbb{R}^d\) are trainable relative positional embeddings and \(r = clip(m-n, Rmin, Rmax)\) represents the relative distance between the two tokens at positions $m$ and $n$. The maximum relative position is clipped, assuming a precise relative position is not useful beyond a certain distance. Further, clipping the maximum distance enables the model to extrapolate at inference time. However, this approach may not encode useful information about the absolute position of the token, which is useful in 2D tasks like Image Classification (Islam et al. [2019] <d-cite key="islam2019much"></d-cite>)</p> <h2 id="positional-encoding-in-vision-transformers">Positional encoding in Vision Transformers</h2> <p>Motivated by the success of transformers in processing one-dimensional textual data, (Dosovitskiy et al. [2021] <d-cite key="50650"></d-cite>) extended this approach to two-dimensional image data by introducing the Vision Transformer (ViT). In this work, they proposed a scalable transformer architecture with minimal adjustments to handle image data. The authors explored several approaches to positional embeddings for 2D images, including standard 1D learnable positional encoding based on the raster order of patches, spatially-aware 2D learned positional encoding and relative positional encoding. Their experiments revealed a significant performance gap between models without positional embeddings and those using any form of positional encoding. Interestingly, they observed little to no difference in performance between the different methods of encoding positional information. They speculated that because the transformer encoder operates on patch-level inputs rather than individual pixels, how spatial information is encoded becomes less critical; however, as will be seen subsequently in this blog, more advanced positional methods proposed for 1-dimensional data also lead to better performance in ViTs.</p> <h2 id="rotary-positional-embeddings">Rotary Positional Embeddings</h2> <p>Traditional positional encoding methods have primarily involved adding positional information directly to context embeddings. While this approach introduces a positional bias, it can limit the depth of interaction with attention weights, potentially underutilizing valuable relative position information. Additionally, since this information is added after the query-key multiplication, it does not contribute to the critical query-key similarity calculations that influence attention mechanisms.</p> <p>Then came RoFormer (Su et al. [2021] <d-cite key="su2021roformer"></d-cite>), which introduced a novel approach with <em>Rotary Positional Embeddings (RoPE)</em>. Unlike conventional methods that add positional encodings to word embeddings, RoPE applies a <em>rotary transformation</em> to encode relative position information. This approach encodes relative position information without hindering the interaction of query-key pairs.</p> <h3 id="the-mechanism-of-rope">The mechanism of RoPE</h3> <p>Given the limitations of directly adding positional information, we need a formulation that encodes relative position information within the attention mechanism by applying a transformation solely to the query and key vectors.</p> <p>Let token at position \(m\) has the context embeddings \(x_m\) , we define function \(f(x_m,m)\) which takes the context vector \(x_m\) and its position in the sequence \(m\) and returns a transformed embedding having the positional information. We want the function \(f\) to be such that the dot product of two embeddings depends on their relative position to each other i.e.</p> \[f(x_m,m)\cdot f(x_n,n) = g(x_m,x_n,m-n)\] <h3 id="the-two-dimensional-case">The two-dimensional case</h3> <p>Consider the basic case where context vector \(x_m = \begin{pmatrix} x_m^{(1)} &amp;x_m^{(2)}\end{pmatrix}^T\) is 2 dimensional and in complex space it can be represented as \(x_m^{(1)} + ix_m^{(2)}\) . For the exponential function, we know that \(e^m \cdot e^n = e^{m+n}\) which is very similar to the properties we want from the function \(f\). Thus for defining \(f\), we can multiply \(f\) \(x_m\) with \(e^m\) , but there is one problem that is explosion of magnitude as the sequence grows. Hence \(e^{im\theta}\) is an ideal choice for multiplication, as it doesn’t cause any magnitude changes and rotates the vector \(x_m\) by an angle of \(m\theta\).</p> \[f(x_m,m) = x_m e^{im \theta}\] <p>Formally, in order to encode the positional information, the key \(k_m\) at position \(m\) and query \(q_n\) at position \(n\) are multiplied with \(e^{im\theta}\) and \(e^{in\theta}\) respectively as a transformation. Consequently, the \((m,n)^{th}\) element of the attention matrix becomes -</p> \[q_n' = q_ne^{in \theta} \; \; k_m' = k_me^{in\theta}\] \[A'_{(n,m)} = \text{Re}[q^{'}_nk^{'*}_m] = \text{Re}[q_nk^*_me^{i(n−m)θ}] \;\;\;\;\;\;\;\;\;\;\;\; \scriptstyle \text{where Re[z] is the real part of z}\] <p>Hence the \(e^{i(n−m)θ}\) in this formulation of attention is responsible injecting relative position of the tokens which are \((n-m)\) distance apart from each other. The formal derivation that it is a solution to equation formulated is detailed in RoFormer (Su et al. [2021] <d-cite key="su2021roformer"></d-cite>). Intuitively, we’re rotating 2D query and key vectors in the embedding space based on their sequence position. For example, the vector corresponding to first token is rotated by θ, the second by 2θ, and so on. The approach of rotating according to the position provides several benefits as discussed below -</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/RotaryPE1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/RotaryPE1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/RotaryPE1-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-positional-embedding/RotaryPE1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 2: Rotation of query, key vectors by $m\theta$ in 2D. </div> <ul> <li> <strong>Stability of Vectors</strong>: Adding tokens at the end of a sentence doesn’t affect the vectors for words at the beginning, facilitating efficient caching.</li> <li> <strong>Preservation of Relative Positions</strong>: If two words, say “pig” and “dog,” maintain the same relative distance in different contexts, their vectors are rotated by the same amount. This ensures that the angle, and consequently the dot product between these vectors, remains constant.</li> </ul> <h3 id="the-general-d-dimensional-case">The general D-dimensional case</h3> <p>The idea of extending the 2D result to the d dimensional case is to break the embedding vector in various 2 dimensional chunks and rotate each one of them by a specific angle \(m\theta_i\) where m depends on the position of token in the sequence and \(\theta_i\) varies according to the chunk. This can be intuitively thought of as a corkscrew rotation in higher dimensions.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/RotaryPE2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/RotaryPE2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/RotaryPE2-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-positional-embedding/RotaryPE2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 3: Visual representation of chunking of tokens and applying the rotation of different frequency $\theta_t$ on each chunk. </div> <p>More formally, we divide the d dimensional space into smaller 2 dimensional subspaces, apply the rotation in those smaller subspaces and combine them again, this can be achieved through</p> \[q_n = R^{d}_{\Theta,n}\:W_q\:x_n, \; \; \; \;k_m = R^{d}_{\Theta,m}\:W_k\:x_m\] <p>where the transformation matrix \(R^{d}_{\Theta,m}\) is defined as follows -</p> \[R^{d}_{\Theta,m} = \begin{pmatrix} \cos m\theta_1 &amp; -\sin m\theta_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\ \sin m\theta_1 &amp; \cos m\theta_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; \cos m\theta_2 &amp; -\sin m\theta_2 &amp; \cdots &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; \sin m\theta_2 &amp; \cos m\theta_2 &amp; \cdots &amp; 0 &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cos m\theta_{d/2} &amp; -\sin m\theta_{d/2} \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sin m\theta_{d/2} &amp; \cos m\theta_{d/2} \end{pmatrix}\] <p>where the frequencies \(\theta_t\) are defined as follows -</p> \[\theta_t = 10000^{-t/(d/2)} \; \; \; \; \; t \in \{0,1, \cdots,d/2\}\] <p>This sums up the mathematical generalization for the d dimensional case of rotary positional embeddings. In order to make the notations simpler and easier to digest, we take an equivalent representation in complex space. Given the query and key vectors \(q_n, k_m \in \mathbb{R}^d\), we define complex vectors \(\bar{q_n}, \bar{k_m} \in \mathbb{C}^{d/2}\) by considering the \(2t^{th}\) dimension as real part and \(2t+1^{th}\) dimension as the complex part of the vector.</p> <p>That is if \(q_n = (q^1_n,q^2_n,q^3_n,q^4_n, \cdots,q^{d-1}_n,q^d_n)\) then \(\bar{q_n} = (q^1_n+iq^2_n,q^3_n+iq^4_n, \cdots,q^{d-1}_n+iq^d_n) \in \mathbb{C}^{d/2}\) is its complex space counterpart, similar for key vectors. Now it can be proved that</p> \[q_nk_m^T = \operatorname{Re}[\bar{q_n} \bar{k_m}^T]\] <p>Using this idea of complex vectors, we can define a new transformation matrix \(\textbf{R} \in \mathbb{C}^{N \times (d/2)}\) as</p> \[\textbf{R}(n,t) = e^{i\theta_tn} \;\;\;\;\;\;\;\; \text{or} \;\;\;\;\;\;\;\;\textbf{R}(n) = \begin{pmatrix} e^{i\theta_1m} \\ e^{i\theta_2m} \\ \vdots \\ e^{i\theta_{d/2-1}m} \\ e^{i\theta_{d/2}m} \end{pmatrix}\] <p>and the transformation of RoPE can be reformed with Hadamard Product \(\circ\) as</p> \[\bar{q}' = \bar{q} \circ \textbf{R}, \; \; \; \; \bar{k}' = \bar{q} \circ \textbf{R}, \; \; \; \; \textbf{A}' = \operatorname{Re}(\bar{q_n} \bar{k_m}^T)\] <p>Where Hadamard product is the operation of taking element wise multiplication of corresponding matrices. We will be using this summarized notation to further define axial and mixed RoPE Positional Embeddings for ViT in later sections.</p> <p>RoFormer (Su et al. [2021] <d-cite key="su2021roformer"></d-cite>) has also shown that RoPE has <em>long-term decay</em>, meaning tokens with larger relative positional distances naturally exhibit weaker connections. This aligns intuitively with the idea that distant tokens should carry less influence on each other, improving context relevance. Additionally, RoPE enables <em>linear self-attention</em> models to incorporate relative positional embeddings, making it a versatile choice for efficient attention calculations. RoPE also enhances sequence length flexibility, allowing models to work with varying sequence lengths. However, RoPE’s extrapolation performance on sequence lengths much longer than those seen during training is somewhat limited.</p> <h2 id="rope-in-vision-trasformers">RoPE in Vision Trasformers</h2> <p>(Heo et al. [2024] <d-cite key="heo2024rotary"></d-cite>) described various method to extend the idea of rotary positional embeddings to 2 dimensional application in Vision Transformers. Two approaches namely “Axial 2D Rope” and “RoPE Mixed” are discussed in the paper.</p> <h3 id="axial-2d-rope">Axial 2D Rope</h3> <p>The core idea is to take and divide the embedding dimension into two and apply positional embedding for x-axis and y-axis separately, which is just repeating the 1d embedding twice. For indexing let patch positions in 2d image be \(\textbf{p}_n = (p_n^x, p_n^y)\) then the rotation matrix \(\textbf{R}\) from earlier section can be defined as</p> \[\textbf{R}(n,2t) = e^{i\theta_tp_n^x}, \; \; \; \; \textbf{R}(n,2t+1) = e^{i\theta_tp_n^y}\] <p>Hence we are interwining the rope along the two axes \(x\) and \(y\). As the range of indexes \((p_n^x, p_n^y)\) are reduced by square root, the RoPE frequencies \(\theta_t\) are also reduced by square root, i.e.</p> \[\theta_t = 100^{-t/(d/4)}, \;\;\;\;\;\text{where} \; t \in \{0,1,\cdots, d/4\}\] <p>Note that frequencies \(\theta_t\) for vision tasks are much larger than those used for language tasks. Also number of frequencies is halved so that \(d/2\) is covered by the contribution of both axes \(x,y\).</p> <h3 id="mixed-rope">Mixed RoPE</h3> <p>The axial frequencies are unable to handle diagonal directions since frequencies only depend on either $x$ or $y$ axes. As the relative positions in RoPE are embedded in the form of \(e^{i\theta_t(n-m)}\) the relative positions are always interpreted as either \(e^{i\theta_t(p_n^x-p_m^x)}\) or \(e^{i\theta_t(p_n^x-p_m^x)}\), so the axial frequencies are not mixed in the axial direction.</p> <p>In order to cater for mixed frequencies, the new rotation matrix \(\textbf{R}\) is proposed as</p> \[\textbf{R}(n,t) = e^{i(\theta_t^xp_n^x+\theta_t^yp_n^y)}\] <p>Unlike 1D RoPE and axial frequency RoPE, Mixed RoPE doesn’t define frequencies explicitly. Instead, it allows the network to learn frequencies \((\theta_t^x,\theta_t^y)\) for \(t \in \{0, 1, \cdots, d/2\}\) , as these are treated as learnable parameters. Separate sets of frequencies are used per head per self attention layer.</p> <hr> <p>While RoPE tries to tackle the problems of Absolute and Learned positional encoding, another method that addresses this fundamental question in the realm of transformer models, i.e., “how to achieve extrapolation at inference time for sequences that are longer than those encountered during training,” is Alibi(Attention with linear bias) introduced by (Press et al. [2021] <d-cite key="press2021train"></d-cite>).</p> <h2 id="attention-with-linear-biases-alibi"> <strong>Attention with Linear Biases (</strong>Alibi)</h2> <p>Unlike RoPE’s rotary approach, ALiBi introduces a novel mechanism for incorporating positional information into transformer models. The key innovation lies in its integration of positional encoding directly within the attention computation rather than at the token embedding level. This approach significantly differs from the previous one, focusing on relative positional relationships through a bias-based attention mechanism. One of ALiBi’s standout features is its complete removal of traditional positional embeddings at the input layer. Instead of adding positional encodings at the bottom of the network, ALiBi integrates them as biases within the attention layers themselves.</p> <h3 id="mechanism-and-architecture">Mechanism and Architecture</h3> <p>ALiBi operates by injecting positional information through a bias matrix added to the self-attention scores before the softmax operation. The magnitude of this bias is proportional to the distance between the query and key tokens, establishing an intrinsic relationship between position and attention strength. Mathematically, this transforms the standard attention computation by introducing a position-aware bias term.</p> <p>The architecture proposed by (Press et al. [2021] <d-cite key="press2021train"></d-cite>) specifically addresses autoregressive decoder-only scenarios, implementing causal attention through a lower-triangular matrix structure. This ensures that each token can only attend to its predecessors. The resulting attention pattern can be represented as shown in the figure</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/ALiBi1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/ALiBi1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/ALiBi1-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-positional-embedding/ALiBi1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 4: Visualization of ALiBi's attention computation mechanism. The method introduces a head-specific linear bias matrix (right) that gets added element-wise to the standard query-key dot product attention scores (left). This bias injection occurs before the softmax operation, while preserving the remaining transformer architecture. The bias strength is controlled by a $m$, which is predetermined for each attention head. Inspired from <d-cite key="press2021train"></d-cite> </div> <h3 id="what-is-textbfm-">What is \(\textbf{m}\) ?</h3> <p>\(\textbf{m}\) is a non-learnable, head-specific slope parameter whose values follow a geometric sequence defined that starts at \(2^{-\frac{8}{n}}\) and uses that same value as its ratio, where \(n\) represents the number of attention heads.</p> <p>This choice of slope values offers several benefits:</p> <ol> <li> <strong>Diverse Temporal Scales</strong>: Each attention head is able to focus on different temporal scales, allowing the model to capture a range of positional relationships.</li> <li> <strong>Ensemble Effect</strong>: The varied attention patterns create an ensemble-like effect, enriching the model’s representational power.</li> <li> <strong>Contextual Flexibility</strong>: With slopes customized for different scales, the model can apply the most effective attention mechanisms for different contexts, adapting to both short and long-range dependencies.</li> </ol> <h3 id="theoretical-foundation-and-advantages">Theoretical Foundation and Advantages</h3> <p>The core principle behind ALiBi lies in <em>temporal relevance</em>—a systematic adjustment of attention values based on the positional distance between queries and keys. This mechanism introduces an inductive <em>recency bias</em>, which has several advantages:</p> <ul> <li> <strong>Proportional Penalties</strong>: ALiBi applies penalties to attention scores based on the distance between tokens, with closer tokens receiving lower penalties and more attention.</li> <li> <strong>Enhanced Focus on Proximity</strong>: The method imposes stronger penalization on distant token relationships, naturally biasing the model toward more relevant, nearby tokens.</li> <li> <strong>Adaptive Penalization Across Heads</strong>: ALiBi allows each attention head to apply different penalty rates, enabling the model to capture both short-range and longer-range dependencies effectively.</li> </ul> <h3 id="performance-and-extrapolation">Performance and Extrapolation</h3> <p>A standout feature of ALiBi is its remarkable ability to extrapolate to sequence lengths far beyond those seen during training. Unlike traditional positional encoding methods, ALiBi sustains robust performance even as sequence lengths grow significantly. This capability overcomes a fundamental limitation in standard transformer architectures, representing a major advancement in the scalability and adaptability of attention-based models.</p> <h2 id="alibi-in-vision-transformers"><strong>Alibi in Vision Transformers</strong></h2> <p>Building on ALiBi’s success with sequential data, researchers extended its application to two-dimensional image data in Vision Transformers (ViT). Translating ALiBi from 1D to 2D domains posed some challenges like adapting its causal attention mechanism for general self-attention and designing spatially aware 2D penalty metrics. These adjustments enabled ALiBi to effectively capture spatial dependencies in images, expanding what positional encoding can do in vision models.</p> <h3 id="two-dimensional-adaptation">Two-Dimensional Adaptation</h3> <p>The adaptation to 2D data, proposed by (Fuller et al. [2023] <d-cite key="fuller2024croma"></d-cite>), introduced several key modifications to the original ALiBi framework:</p> <ol> <li> <strong>Symmetric Attention Structure</strong>: Unlike the lower triangular matrix in sequential processing, the 2D implementation employs a symmetric attention matrix, reflecting the bidirectional nature of spatial relationships in images.</li> <li> <p><strong>Spatial Distance Metrics</strong>: The linear distance penalty is replaced by a Euclidean metric that captures 2D spatial relationships between image patches: \(\text{distance_penalty} = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} \cdot m\)</p> <p>where \(m\) retains its role as a non-learnable head-specific parameter.</p> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/2D_Alibi-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/2D_Alibi-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/2D_Alibi-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-positional-embedding/2D_Alibi.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 5: Visualization of 2D ALiBi's attention computation mechanism.The bias matrix is now a symmetric matrix. </div> <h2 id="comparative-analysis-rope-and-alibi">Comparative Analysis: RoPE and ALiBi</h2> <p>These two approaches, while sharing certain fundamental principles, represent distinct philosophical approaches to positional encoding:</p> <h3 id="shared-characteristics">Shared Characteristics</h3> <ul> <li>Both RoPE and ALiBi on the principle of not adding positional encoding to word embedding, instead focusing on modifying the attention weights computed at every layer. This aligns with the thought that positional information and semantic information represent different things and that they should not be mixed.</li> <li>Both incorporate distance-dependent attention penalties. Both have an inductive recency bias and work on the philosophy that the farther a token is from a given token, the less relevant it is.</li> </ul> <h3 id="key-distinctions">Key Distinctions</h3> <ol> <li> <strong>Mathematical Foundation</strong> <ul> <li>RoPE employs trigonometric transformations with theoretical guarantees. It uses the embedding vector’s rotation to capture positional information, while ALiBi utilizes straightforward linear distance penalties for encoding positional information.</li> </ul> </li> <li> <strong>Extrapolation Strength</strong> <ul> <li>Alibi demonstrates strong extrapolation capability, while RoPE struggles to extrapolate well enough above a number of tokens based on what it has been trained on.</li> </ul> </li> </ol> <h2 id="extrapolation-via-interpolation">Extrapolation via Interpolation</h2> <p>While ALiBi remains the only positional encoding method to demonstrate successful extrapolation to sequence lengths far beyond those encountered during training (Fuller et al. [2023] <d-cite key="fuller2024croma"></d-cite>), and has been adopted in models like MPT-7B (MosaicML NLP Team [2023] <d-cite key="mosaicml2023introducing"></d-cite>), MPT-30B, BLOOM (Scao et al. [2023] <d-cite key="le2023bloom"></d-cite>), and BloombergGPT (Wu et al. [2023] <d-cite key="wu2023bloomberggpt"></d-cite>), it hasn’t gained widespread traction. Many popular large language models (LLMs), like LLaMA (Touvron et al. [2023] <d-cite key="touvron2023llama"></d-cite>) and OPT (Zhang et al. [2022] <d-cite key="zhang2022opt"></d-cite>), still use Rotary Positional Embeddings (RoPE) or learned positional embeddings instead.</p> <p>To address this challenge, (Chen et al. [2023] <d-cite key="chen2023extending"></d-cite>) proposed a novel <em>position interpolation</em> approach to extend RoPE’s context window. They observed that directly extrapolating RoPE often led to exploding attention scores, which contradicts the expected recency bias inherent to RoPE. This issue arises because the theoretical upper bound proposed in RoFormer (Su et al. [2021] <d-cite key="su2021roformer"></d-cite>) is loose, as (Chen et al. [2023] <d-cite key="chen2023extending"></d-cite>) elaborate. To tackle this exploding attention score issue, they proposed interpolating sequences longer than the longest sequence encountered during training. Mathematically for a pre-trained LLM with a context window of size \(L\), we have the following formula for sequence of length \(L'\) with \(L'&gt; L\) we have,</p> \[\mathbf{f}'(\mathbf{x}, m) = \mathbf{f} \left( \mathbf{x}, \frac{mL}{L'} \right) .\] <p>Here $f$ denotes the positional encoding function which takes the context vector $x$ and its position in the sequence $m$.</p> <p>Thus, they reduce their position indices from \([0, L') \to [0, L)\)  to match the original range of indices before computing RoPE. Consequently, as inputs to RoPE, the maximum relative distance between any two tokens is reduced from \(L' \to L\). This alignment of ranges of position indices and relative distances before and after extension helps mitigate the effect on attention score computation due to context window extensions, which make the model easier to adapt.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/Interpolation-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/Interpolation-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/Interpolation-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-positional-embedding/Interpolation.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 6: An illustration of the Position Interpolation method in RoPE. Consider a LLM model pre-trained with a maximum sequence length of 2048. The upper left shows normal LLM usage where position indices stay within the pre-trained range. The upper right shows length extrapolation where models handle unseen positions up to 4096. The lower left shows Position Interpolation, where we downscale position indices from [0, 4096] to [0, 2048], keeping them in the pre-trained range. </div> <p>Recent research on extending the context windows of pre-trained Large Language Models (LLMs) has predominantly focused on position interpolation using RoFormer (Su et al. [2021] <d-cite key="su2021roformer"></d-cite>) due to widespread use in many pre-trained LLMs. At the same time, the ALiBi positioning method has received comparatively less attention. A recent work by (Faisal Al-Khateeb et al. [2023] <d-cite key="al2023position"></d-cite>) proposed an innovative approach to position interpolation for ALibi that significantly differs from (Chen et al. [2023] <d-cite key="chen2023extending"></d-cite>) methodology. While (Chen et al’s. [2023] <d-cite key="chen2023extending"></d-cite>) scaling operation was motivated by the problem of exploding attention score magnitudes during extrapolation, ALibi presents a different challenge: it generates lower magnitude attention scores for tokens in the extrapolation regime than the interpolation regime. To address this, (Faisal Al-Khateeb et al. [2023] <d-cite key="al2023position"></d-cite>) introduced a dynamic slope scaling mechanism that adjusts the ALiBi slope to amplify attention scores, effectively preventing the decrease in magnitudes beyond the training context length. This approach involves scaling the slopes by a factor of \(L/L'\), where \(L\) represents the maximum sequence length during training and \(L'\) denotes the extended input sequence length during inference, expressed as \(m_j' = m_j(L/L')\). Importantly, this slope scaling is only applied when \(L'\) exceeds \(L\), ensuring the model maintains its original performance for sequences within the training length.</p> <p>In the domain of computer vision, (Dosovitskiy et al. [2021] <d-cite key="50650"></d-cite>), in their work on Vision Transformers, introduced a unique interpolation method to handle position embeddings during fine-tuning for higher-resolution images. Specifically, when adapting to higher resolutions, they maintained the original patch size, which increased the sequence length and caused pre-trained positional embeddings to lose spatial relevance. To address this, they applied 2D interpolation to the pre-trained position embeddings, preserving spatial relationships based on their original locations in the image. This approach differs from that of (Chen et al. [2023] <d-cite key="chen2023extending"></d-cite>), who performed interpolation at the level of position indices rather than on the embeddings themselves. It also aligns more closely with (Faisal Al-Khateeb et al’s. [2023] <d-cite key="al2023position"></d-cite>) rescaling technique, which similarly adjusts positional embeddings rather than rescaling indices for longer sequences.</p> <p>Notably, the method proposed by (Faisal Al-Khateeb et al’s. [2023] <d-cite key="al2023position"></d-cite>) required no additional training or fine-tuning. In contrast the approaches proposed by (Chen et al. [2023] <d-cite key="chen2023extending"></d-cite>) and (Dosovitskiy et al. [2021] <d-cite key="50650"></d-cite>) involved fine-tuning the model on longer sequences for effective extrapolation.</p> <h2 id="experimental-evaluation">Experimental Evaluation</h2> <p>We conducted experiments comparing four architectural variants to empirically verify performance differences :</p> <ol> <li>Standard Vision Transformer</li> <li>Vision Transformer without positional encoding</li> <li>Vision Transformer with 2D mixed-frequency RoPE</li> <li>Vision Transformer with 2D ALiBi</li> </ol> <h3 id="experimental-details">Experimental Details</h3> <p>We trained all our models using the following hyperparameter scheme and high performance training recipes <d-cite key="cubuk2020randaugment"></d-cite> <d-cite key="zhang2018mixup"></d-cite> <d-cite key="yun2019cutmix"></d-cite> <d-cite key="zhong2020random"></d-cite> <d-cite key="szegedy2016rethinking"></d-cite> :</p> <table> <thead> <tr> <th><strong>Hyperparameters</strong></th> <th><strong>Value</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Dataset</strong></td> <td>ImageNet100</td> </tr> <tr> <td><strong>Training Image Resolution</strong></td> <td>224x224</td> </tr> <tr> <td><strong>Number of Epochs</strong></td> <td>100</td> </tr> <tr> <td><strong>Number of Steps</strong></td> <td>50K</td> </tr> <tr> <td><strong>Number of Warmup Steps</strong></td> <td>10K</td> </tr> <tr> <td><strong>Warmup Schedule</strong></td> <td>Linear</td> </tr> <tr> <td><strong>Optimizer</strong></td> <td>AdamW</td> </tr> <tr> <td><strong>Base Learning Rate</strong></td> <td>1e-3</td> </tr> <tr> <td><strong>Learning Rate Schedule</strong></td> <td>Cosine Decay</td> </tr> <tr> <td><strong>Weight Decay</strong></td> <td>0.05</td> </tr> <tr> <td><strong>Optimizer Momentum</strong></td> <td>β1, β2 = 0.9, 0.999</td> </tr> <tr> <td><strong>Batch Size</strong></td> <td>256</td> </tr> <tr> <td><strong>RandAugment</strong></td> <td>(9, 0.5)</td> </tr> <tr> <td><strong>Mixup</strong></td> <td>0.8</td> </tr> <tr> <td><strong>Cutmix</strong></td> <td>1.0</td> </tr> <tr> <td><strong>Random Erasing</strong></td> <td>0.25</td> </tr> <tr> <td><strong>Label Smoothing</strong></td> <td>0.1</td> </tr> </tbody> </table> <h3 id="initialisation-scheme">Initialisation Scheme:</h3> <p><strong>CLS Token</strong>: Initialized using a standard normal distribution with mean 0 and variance 0.02 across all variants. The low variance prevents large initial values, reducing the risk of training instability.</p> <p><strong>Patch Embedding</strong>: We applied LeCun normal initialization for the convolutional patch embedding layer in all variants. Biases are initialized to 0, and weights use a truncated normal distribution based on input features.</p> <p><strong>Positional Embedding</strong>: For the standard Vision Transformer, the learned positional embedding in raster order are initialized with a standard normal distribution mean 0 and variance 0.02.</p> <p><strong>Classification Head</strong>: Both weights and biases in the final classification head are initialized to zero across all variants.</p> <p><strong>Multi-Head Attention Initialization</strong>: Attention layers use a uniform distribution, with bounds determined by the hidden dimension.</p> <p><strong>MLP Blocks and Layer Norms</strong>: Following PyTorch’s default initialization scheme:</p> <ul> <li>Linear layers use Kaiming uniform initialization.</li> <li>Layer normalization has weight initialized to 1 and bias to 0.</li> </ul> <h2 id="results">Results</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/results-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/results-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-positional-embedding/results-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-positional-embedding/results.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Note: Values correspond to Top-1 validation set accuracy </div> <table> <thead> <tr> <th>Resolution</th> <th>Standard Vision Transformer</th> <th>Vision Transformer without positional encoding</th> <th>Vision Transformer with 2D mixed-frequency RoPE</th> <th>Vision Transformer with 2D ALiBi</th> </tr> </thead> <tbody> <tr> <td>96x96</td> <td>18.80</td> <td>16.82</td> <td>18.98</td> <td>19.60</td> </tr> <tr> <td>128x128</td> <td>33.96</td> <td>30.84</td> <td>32.92</td> <td>36.44</td> </tr> <tr> <td>160x160</td> <td>44.10</td> <td>42.96</td> <td>44.84</td> <td>50.28</td> </tr> <tr> <td>192x192</td> <td>51.42</td> <td>49.22</td> <td>51.60</td> <td>55.84</td> </tr> <tr> <td>224x224</td> <td>55.48</td> <td>53.86</td> <td>55.26</td> <td>60.74</td> </tr> <tr> <td>256x256</td> <td>56.52</td> <td>55.80</td> <td>57.98</td> <td>61.52</td> </tr> <tr> <td>288x288</td> <td>57.14</td> <td>56.52</td> <td>58.76</td> <td>64.00</td> </tr> <tr> <td>320x320</td> <td>57.02</td> <td>55.86</td> <td>59.84</td> <td>64.62</td> </tr> <tr> <td>352x352</td> <td>56.48</td> <td>55.22</td> <td>59.70</td> <td>65.54</td> </tr> <tr> <td>384x384</td> <td>55.88</td> <td>54.16</td> <td>58.92</td> <td>65.76</td> </tr> <tr> <td>416x416</td> <td>55.20</td> <td>53.50</td> <td>58.44</td> <td>65.24</td> </tr> <tr> <td>448x448</td> <td>54.34</td> <td>52.40</td> <td>57.86</td> <td>64.24</td> </tr> <tr> <td>480x480</td> <td>53.42</td> <td>51.48</td> <td>56.88</td> <td>63.56</td> </tr> <tr> <td>512x512</td> <td>52.32</td> <td>49.80</td> <td>55.62</td> <td>62.20</td> </tr> </tbody> </table> <p><strong>Note</strong>: Due to limited computational resources, we trained and evaluated our models on ImageNet100 <d-footnote>https://www.kaggle.com/datasets/ambityga/imagenet100</d-footnote>, a subset of ImageNet-1k. The subset comprises 100 classes with approximately 130,000 images for training and 5,000 images across 50 classes for validation. Additionally, we trained our model for 100 epochs instead of the standard 300 epochs commonly used for Vision Transformers (ViTs). Consequently, our results differ from those obtained on the full ImageNet-1k dataset. Our implementations of RoPE ViT and ALiBi are from their official github repos <d-footnote>https://github.com/naver-ai/rope-vit</d-footnote>,<d-footnote>https://github.com/antofuller/CROMA</d-footnote> respectively.</p> <p>Further, we did not fine-tune the model on a higher sequence length. That is, all our results are for 0 shots. We extrapolated the standard Vision Transformer using 2D interpolation, whereas RoPE and Alibi did not require interpolation techniques as they can handle sequences of variable length. All models were trained using 2 T4 GPUs, with the following training times</p> <table> <thead> <tr> <th>Model</th> <th>Training Time</th> </tr> </thead> <tbody> <tr> <td>Standard Vision Transformer</td> <td>2 days 7 hours 47 minutes</td> </tr> <tr> <td>Vision Transformer without positional encoding</td> <td>2 days 8 hours 22 minutes</td> </tr> <tr> <td>Vision Transformer with 2D mixed-frequency RoPE</td> <td>2 days 10 hours 45 minutes</td> </tr> <tr> <td>Vision Transformer with 2D ALiBi</td> <td>2 days 8 hours 28 minutes</td> </tr> </tbody> </table> <h3 id="key-findings">Key Findings</h3> <ol> <li>Both ALiBi and RoPE demonstrate superior extrapolation capabilities compared to the baseline ViT.</li> <li>ALiBi exhibits better extrapolation performance over RoPE, similar to what has been observed in 1D data.</li> <li>ALiBi also offers computational efficiency, reducing training time.</li> </ol> <h2 id="conclusions">Conclusions</h2> <p>Our analysis of positional encoding mechanisms in Transformer architectures highlights several key insights:</p> <ol> <li>The evolution of positional encoding and its fundamental role across diverse domains within Transformer architectures.</li> <li>The sequence length extrapolation challenge, where RoPE and ALiBi demonstrate particular effectiveness in handling variable sequence lengths, especially ALiBi in extended contexts.</li> <li>The adaptation of positional encoding from 1D to 2D tasks, with ALiBi showing practical advantages over RoPE in terms of computational efficiency and implementation simplicity.</li> <li>The application of interpolation techniques to enhance the extrapolation capabilities of both RoPE and ALiBi positional encoding methods.</li> <li>The empirical validation of RoPE and ALiBi positional encoding in standard Vision Transformers.</li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2025/assets/bibliography/2025-04-28-positional-embedding.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>