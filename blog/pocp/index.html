<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},o=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),i=o[0][0],a=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"April 28, 2025"),r="Why RoPE Struggles to Maintain Long-Term Decay in Long Sequences?",l="Rotary Position Embedding (RoPE) improves upon traditional positional encodings but struggles with long-term decay in contexts exceeding its training length, limiting the model's generalization to longer sequences. Our experiments suggest that this issue may stem from a high proportion of obtuse angles on the complex plane between the linear transformations of query and key embeddings.";{let e=o.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(i+"2025"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${r}},\n  abstract = {${l}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${a}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=o.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${r}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Why RoPE Struggles to Maintain Long-Term Decay in Long Sequences? | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Rotary Position Embedding (RoPE) improves upon traditional positional encodings but struggles with long-term decay in contexts exceeding its training length, limiting the model's generalization to longer sequences. Our experiments suggest that this issue may stem from a high proportion of obtuse angles on the complex plane between the linear transformations of query and key embeddings."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/2025/assets/css/main.css"> <link rel="canonical" href="https://starlight345.github.io/2025/blog/pocp/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/2025/assets/js/theme.js"></script> <script src="/2025/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2025/assets/js/distillpub/template.v2.js"></script> <script src="/2025/assets/js/distillpub/transforms.v2.js"></script> <script src="/2025/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <d-front-matter> <script async type="text/json">{
      "title": "Why RoPE Struggles to Maintain Long-Term Decay in Long Sequences?",
      "description": "Rotary Position Embedding (RoPE) improves upon traditional positional encodings but struggles with long-term decay in contexts exceeding its training length, limiting the model's generalization to longer sequences. Our experiments suggest that this issue may stem from a high proportion of obtuse angles on the complex plane between the linear transformations of query and key embeddings.",
      "published": "April 28, 2025",
      "authors": [
        {
          "author": "Wei Shen",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Independent Researcher",
              "url": ""
            }
          ]
        },
        {
          "author": "Chao Yin",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Independent Researcher",
              "url": ""
            }
          ]
        },
        {
          "author": "Yuliang Liu",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Shanghai Innovation Institude",
              "url": ""
            }
          ]
        },
        {
          "author": "Zikai Xiao",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Zhejiang University",
              "url": ""
            }
          ]
        },
        {
          "author": "Xiaonan He",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Independent Researcher",
              "url": ""
            }
          ]
        },
        {
          "author": "Wang Yan",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Independent Researcher",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/reviewing/">reviewing</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Why RoPE Struggles to Maintain Long-Term Decay in Long Sequences?</h1> <p>Rotary Position Embedding (RoPE) improves upon traditional positional encodings but struggles with long-term decay in contexts exceeding its training length, limiting the model's generalization to longer sequences. Our experiments suggest that this issue may stem from a high proportion of obtuse angles on the complex plane between the linear transformations of query and key embeddings.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#why-rope-struggles-to-maintain-long-term-decay-in-long-sequences">Why RoPE Struggles to Maintain Long-Term Decay in Long Sequences</a></div> <div><a href="#introduction">Introduction</a></div> <div><a href="#experimental-insights-small-pocp-correlates-with-better-long-term-decay-in-rope">Experimental Insights: Small POCP Correlates with Better Long-Term Decay in RoPE</a></div> <ul> <li><a href="#a-limitation-of-rope">A Limitation of RoPE</a></li> <li><a href="#experiments-on-randomly-initialized-vectors">Experiments on Randomly Initialized Vectors</a></li> <li><a href="#experiments-in-llama2-7b-4k">Experiments in Llama2-7B-4k</a></li> <li><a href="#comparing-pocp-metrics-llama3-8b-8k-vs-llama3-8b-64k">Comparing POCP Metrics: Llama3-8B-8k vs. Llama3-8B-64k</a></li> </ul> <div><a href="#discussion">Discussion</a></div> <div><a href="#future-work">Future Work</a></div> </nav> </d-contents> <h2 id="why-rope-struggles-to-maintain-long-term-decay-in-long-sequences">Why RoPE Struggles to Maintain Long-Term Decay in Long Sequences</h2> <h2 id="introduction">Introduction</h2> <p>Rotary Position Embedding (RoPE) <d-cite key="su2024roformer"></d-cite> is an advanced variant designed to address the limitations of traditional positional encodings, particularly in capturing relative positional information. It has been integrated into several modern architectures, including Llama2, Mistral, Qwen, ChatGLM <d-cite key="touvron2023llama,jiang2023mistral,bai2023qwen,glm2024chatglm"></d-cite> . However, we observe that RoPE fails to maintain long-term decay in contexts that extend beyond its training length without additional post-training adjustments. This limitation hinders the model’s ability to effectively generalize or extrapolate to sequences longer than those encountered during training. Therefore, in this blog, we further investigate the root cause of this limitation. We conduct three experiments on different models. Our findings suggest that the issue may arise from a high <strong>P</strong>roportion of <strong>O</strong>btuse angles on the <strong>C</strong>omplex <strong>P</strong>lane (<strong>POCP</strong>) between \(W_Qx\) and \(W_Ky\), where \(W_Q\) and \(W_K\) are the linear transformation matrices in the model’s multi-head (or group query attention), and \(x\) and \(y\) represent the token embeddings within each transformer layer.</p> <p><strong>Definition of POCP : POCP</strong> specifically measures the angles between each 2D vector component of the query vector \(W_Qx\) and the key vector \(W_Ky\). To compute <strong>POCP</strong>, the \(d\)-dimensional vectors \(W_Qx\) and \(W_Ky\) are divided into \(\frac{d}{2}\) parts. The angles for each part are calculated, and <strong>POCP</strong> is the proportion of these angles that exceed 90 degrees:</p> \[\text{POCP} = \frac{2}{d}\sum_i^{\frac{d}{2}}\textbf{I}(\cos(W_Qx[2i:2i+1], W_Ky[2i:2i+1]) &lt; 0)\] <p>where \(\textbf{I}(A)\) is an indicator function that returns 1 if the event \(A\) occurs, and 0 otherwise.</p> <h2 id="experimental-insights-small-pocp-correlates-with-better-long-term-decay-in-rope">Experimental Insights: Small POCP Correlates with Better Long-Term Decay in RoPE</h2> <h3 id="a-limitation-of-rope">A Limitation of RoPE</h3> <p>Without further fine-tuning or the use of interpolation methods <d-cite key="peng2023yarn"></d-cite>, pre-trained models with RoPE struggle to handle significantly longer contexts.</p> <p>This phenomenon arises from a limitation of RoPE: its inability to maintain long-term decay in significantly longer contexts. The resulting high attention scores on these distant contexts may lead to increased perplexity, impairing the model’s ability to make accurate predictions in these contexts.</p> <hr> <p><strong>A Statistical Analysis of Llama2-7B-4k:</strong> As shown in the following figure, we collected \(100\) sequences of \(10,000\) tokens each from Llama2-7B-4k to examine the relationship between the original attention score (\((W_Ky)^TW_Qx\)) and the attention score after applying the RoPE function (\((R(\theta_j)W_Ky)^TR(\theta_i)W_Qx\)), where \(R(\theta_i)\) and \(R(\theta_j)\) denote the RoPE function at position \(i\) and \(j\), respectively.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-pocp/image-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-pocp/image-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-pocp/image-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-pocp/image.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: We collected data from all attention heads across all layers from 100 sequences of 10,000 tokens in Llama2-7B-4k. We then calculated both the original attention scores and the attention scores after applying the RoPE function for all heads at various layers and positions. Finally, we calculated the proportion of instances where the original attention scores exceed those after applying the RoPE function for each layer (vertical axis) and each position index (horizontal axis).</figcaption> </figure> <p>From the statistical analysis, we have observed the following findings::</p> <ul> <li>After applying the RoPE function, the original attention scores consistently decrease within the trained length but increase beyond it. This suggests that RoPE struggles to maintain long-term decay in significantly extended contexts. <ul> <li>Within a sequence length of 4k (referring to 4096 tokens), only 20% of the original attention scores exceed the corresponding scores after the application of RoPE.</li> <li>For sequences longer than 6000 tokens, 80% of the original attention scores exceed the scores obtained after applying the RoPE function.</li> </ul> </li> </ul> <hr> <p>To explore the root cause of RoPE’s inability to maintain long-term decay in significantly longer contexts, we investigate the relationship between \(W_Qx\), \(W_Ky\) and the RoPE function in the following sections. Specifically, we examine how \(W_Qx\) and \(W_Ky\) contribute to the high attention scores between \(R(\theta_i)W_Qx\) and \(R(\theta_j)W_Ky\) over long positional distance, where \(R(\theta_i)\) and \(R(\theta_j)\) denote the RoPE function at position \(i\) and \(j\).</p> <h3 id="experiments-on-randomly-initialized-vectors">Experiments on Randomly Initialized Vectors</h3> <p>We first design a simple experiment on two randomly initialized vectors, denoted as \(\hat{x}\) and \(\hat{y}\):</p> <p>We test the attention scores between \(R(\theta_i)W_Q\hat{x}\) and \(R(\theta_j)W_K\hat{y}\) (embedding size: \(128\) to \(4096\)) at various positional distances (0 to 16k), conditional on the the <strong>P</strong>roportion (\(0\%\) to \(100\%\)) of <strong>O</strong>btuse angles between \(W_Q\hat{x}\) and \(W_K\hat{y}\) on the <strong>C</strong>omplex <strong>P</strong>lane (<strong>POCP</strong>). The code is provided in the Appendix.</p> <p>As shown in the following figures, we have the following findings:</p> <ul> <li>Small <strong>POCP</strong> correlates with better long-term decay in RoPE. <ul> <li>When the <strong>POCP</strong> is below \(20\%\) , the attention score between \(W_Qx\) and \(W_Ky\) after applying the RoPE function decreases as the positional distance increases.</li> <li>When the <strong>POCP</strong> exceeds \(50\%\), the attention score between \(W_Qx\) and \(W_Ky\) after applying the RoPE function fluctuates over 4k contexts.</li> </ul> </li> <li>High-dimensional model embeddings correlate with better long-term decay in RoPE. <ul> <li>When the embedding size of model is \(128\), as in Llama2-7B, the model can maintain long-term decay if <strong>POCP</strong> is below \(20\%\). However, with an embedding size of \(4096\), the model can maintain long-term decay if <strong>POCP</strong> is below \(50\%\). It suggests that a model with a larger embedding size can maintain long-term decay without post-training, leading to better extrapolation ability.</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-pocp/Untitled.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_1-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-pocp/Untitled_1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_2-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-pocp/Untitled_2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_3-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-pocp/Untitled_3.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-pocp/sample_curve-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-pocp/sample_curve-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-pocp/sample_curve-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-pocp/sample_curve.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-pocp/sample_curve_1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-pocp/sample_curve_1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-pocp/sample_curve_1-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-pocp/sample_curve_1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Figure: We test the attention scores between \(W_Qx\) and \(W_Ky\) after applying the RoPE function (embedding size: \(128\) to \(4096\)) at various positional distances (0 to 16k), conditional on the proportion (\(0\%\) to \(100\%\)) of obtuse angles between \(W_Qx\) and \(W_Ky\) on the complex plane.</p> <h3 id="experiments-in-llama2-7b-4k">Experiments in Llama2-7B-4k</h3> <p>Additionally, we argue that embeddings initialized with random weights do not accurately reflect the actual embedding distribution in pre-trained LLMs. Therefore, we select \(W_Qx\) and \(W_Ky\) from first layer of Llama2-7B-4k <d-cite key="touvron2023llama"></d-cite> to replicate the experiments in Section 2.2.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_4-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-pocp/Untitled_4.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure: We randomly select \$$W_Q x\$$ and \$$W_K y\$$ from the first layer of Llama2-7B-4k.</figcaption> </figure> <p>Figure : We randomly select \(W_Q\)x and \(W_Ky\) from the first layer of Llama2-7B-4k.</p> <p>From the experimental results, we find:</p> <ul> <li>The attention scores on only a few heads between \(W_Qx\) and \(W_Ky\) after applying the RoPE function are not able to maintain long-term decay in the extended contexts. <ul> <li>Except for Head-24, the attention score between \(W_Qx\) and \(W_Ky\) after applying the RoPE function generally decreases as the positional distance increases, particularly when the distances range from 0 to 4k.</li> <li>Except for Head-12 and Head-24, the attention score between \(W_Qx\) and \(W_Ky\) after applying the RoPE function can slightly fluctuate around 0.</li> <li>The attention scores for Head-12 and Head-24 do not maintain long-term decay and fluctuate intensely beyond 4k.</li> </ul> </li> </ul> <p>Furthermore, we present the <strong>POCP</strong>, <strong>POCP</strong> in the low (last 32 dimensions in the embedding) and high (first 32 dimensions in the embedding) frequency areas, and the original attention score (the attention score between \(W_Qx\) and \(W_Ky\)) in the following table.</p> <table> <thead> <tr> <th>Head</th> <th>0</th> <th>4</th> <th>8</th> <th>12</th> <th>16</th> <th>20</th> <th>24</th> <th>28</th> </tr> </thead> <tbody> <tr> <td><strong>POCP</strong></td> <td>0.20</td> <td>0.22</td> <td>0.20</td> <td>0.28</td> <td>0.30</td> <td>0.28</td> <td>0.52</td> <td>0.25</td> </tr> <tr> <td>Original attention score</td> <td>21</td> <td>30</td> <td>14</td> <td>42</td> <td>8.0</td> <td>7.0</td> <td>-3.0</td> <td>17</td> </tr> <tr> <td> <strong>POCP</strong> in low freq (The last 32 dimensions in the embedding.)</td> <td>0.25</td> <td>0.13</td> <td>0.44</td> <td>0.25</td> <td>0.50</td> <td>0.63</td> <td>0.50</td> <td>0.13</td> </tr> <tr> <td> <strong>POCP</strong> in high freq (The first 32 dimensions in the embedding.)</td> <td>0.13</td> <td>0.31</td> <td>0.06</td> <td>0.38</td> <td>0.19</td> <td>0.06</td> <td>0.56</td> <td>0.38</td> </tr> </tbody> </table> <p>From this table, we find that:</p> <ul> <li> <strong>POCP</strong> significantly affects the model’s capacity to maintain long-term decay over extended contexts. <ul> <li>Except for Head-24, the <strong>POCP</strong> of all heads is below \(30\%\).</li> <li>The <strong>POCP</strong> of Head-24 exceeds \(50\%\), causing the attention scores between \(W_Qx\) and \(W_Ky\) after applying the RoPE function to fluctuate intensely across the entire 8k range.</li> </ul> </li> <li>High original attention scores tend to cause intense fluctuations on the extended contexts. <ul> <li>Although the <strong>POCP</strong> of Head-12 is only \(28\%\), its original attention score is high among all heads, resulting in more similar angles on the complex plane between \(W_Qx\) and \(W_Ky\). After applying the RoPE function, these similar angles cause the original attention score between each 2D component of \(W_Qx\) and \(W_Ky\) to increase or decrease concurrently, leading to the intense attention score fluctuations beyond 4k.</li> </ul> </li> <li>There are three heads where the <strong>POCP</strong> in low frequency area exceeds \(50\%\). Instead, there are only one head where the <strong>POCP</strong> in high frequency area exceeds \(50\%\).</li> </ul> <p>As a result, we speculate that the heads with high original attention scores and high <strong>POCP</strong> lead to the increased ppl in the extended contexts.</p> <h3 id="comparing-pocp-metrics-llama3-8b-8k-vs-llama3-8b-64k">Comparing POCP Metrics: Llama3-8B-8k vs. Llama3-8B-64k</h3> <p>Finally, we compare <strong>POCP</strong> between Llama3-8B-8k <d-cite key="llama3modelcard"></d-cite> and Llama3-8B-64k <d-cite key="gao2024prolong"></d-cite>. Specifically, the rotary base of RoPE in of Llama3-8B-8k is \(500,000\) and that of Llama3-8B-64k is \(8,000,000\). In addition, Llama3-8B-64k is fine-tuned based on Llama3-8B-8k by a large amount of long contexts.</p> <p>Similarly, we randomly select the \(W_Qx\) and \(W_Ky\) from first layer of Llama3-8B-8k and Llama3-8B-64k to observe the attention score between \(W_Qx\) and \(W_Ky\) after applying the RoPE function with various positional distances (0 to 16k).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_5-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_5-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_5-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-pocp/Untitled_5.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_6-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_6-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-pocp/Untitled_6-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-pocp/Untitled_6.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <table> <thead> <tr> <th>Head</th> <th>0</th> <th>4</th> <th>8</th> <th>12</th> <th>16</th> <th>20</th> <th>24</th> <th>28</th> </tr> </thead> <tbody> <tr> <td>Original attention score</td> <td>4.69</td> <td>-3.09</td> <td>1.78</td> <td>-2.33</td> <td>-1.55</td> <td>0.31</td> <td>-2.12</td> <td>-1.21</td> </tr> </tbody> </table> <p>Figure : We randomly select \(W_Q\)x and \(W_Ky\) from the first layer of Llama3-8B-8k and Llama3-8B-64k.</p> <p>For Head-0, which has the highest original attention score among all heads, we observe that Llama3-8B-64k maintains long-term decay over a 16k context, whereas Llama3-8B-8k can only do so over an 8k context. For other heads, the attention scores fluctuate around 0 over the entire 16k context. This supports our earlier speculation that high original attention scores tend to cause intense fluctuations on the extended contexts.</p> <p>Furthermore, we randomly select \(20\) tokens and analyze all \(32\) heads to compare <strong>POCP</strong> between Llama3-8B-8k and Llama3-8B-64k. Specifically, we use \(10 \times 19 \times 32\) \(W_Qx\) and \(W_Ky\) pairs to calculate <strong>POCP</strong> for both models and find that:</p> <ul> <li>The <strong>POCP</strong> for all token pairs in Llama3-8B-8k is \(0.467\), whereas for Llama3-8B-64k, it is \(0.427\).</li> <li>In \(74.6\%\) of token pairs, the <strong>POCP</strong> in Llama3-8B-8k is higher than in Llama3-8B-64k.</li> </ul> <p>These results demonstrate that <strong>POCP</strong> significantly influences the model’s ability to maintain long-term decay over long contexts. Additionally, post-training on long contexts primarily decreases the model’s <strong>POCP</strong>.</p> <h2 id="discussion">Discussion</h2> <p>Are there other factors besides <strong>POCP</strong> that influence the model’s ability to maintain long-term decay over long contexts?</p> <p>Aside from <strong>POCP</strong>, J. Su <d-cite key="kexuefm-9859"></d-cite> finds that the magnitude of \(W_K\) is an other factor that influence the model’s ability to extrapolate directly to longer contexts than those it was trained on.</p> <p>To support this findings, we conduct experiments and find that:</p> <ul> <li>This factor cannot influence the model’s ability to maintain long-term decay over long contexts.</li> <li>The magnitude of \(W_K\) in Llama3-8B-8k is lower than that in Llama3-8B-64k.</li> </ul> <p>This suggests that the magnitude of \(W_K\) may be a key factor in influencing the model’s extrapolation ability but does not affect its capacity to maintain long-term decay.</p> <hr> <p>Which role does RoPE play during post-training on long contexts?</p> <p>Since the RoPE function lacks training parameters, we train the model with long contexts to optimize \(W_Qx\) and \(W_Ky\). Thus, our focus is on how RoPE influences the learning process of \(W_Qx\) and \(W_Ky\). Based on our experiments and related studies, we suppose that RoPE acts as a regularization method, constraining \(W_Qx\) and \(W_Ky\) and their corresponding attention scores during the learning process.</p> <hr> <p>Why the model with RoPE cannot extrapolate directly to longer contexts than those it was trained on?</p> <p>Recent studies, such as POSE <d-cite key="zhu2023pose"></d-cite> or Yarn <d-cite key="peng2023yarn"></d-cite>, suppose that it is because the model has not seen some angles on the complex plane between \(W_Qx\) and \(W_Ky\) over extended contexts during training. These unseen angles mainly exist in the low frequency area.</p> <p>However, according to J. Su, although the model has encountered these angles during training, it still cannot extrapolate directly to extended contexts. This limitation arises because the model has not been exposed to the attention score distributions in these extended contexts, leading to unseen \(W_Qx\) and \(W_Ky\) in some higher layers. When applied to the RoPE function, these vectors may encounter out-of-distribution issues, potentially resulting in excessively high attention scores on certain tokens in the long distance dependency.</p> <p>In this blog, we suppose that:</p> <ul> <li>For directly extrapolating to longer contexts than those it was trained on, the model capture the whole distribution over \(R(\theta_i)W_Qx\) and \(R(\theta_i)W_Ky\) during training.</li> <li>To capture the full distribution over \(R(\theta_i)W_Qx\) and \(R(\theta_i)W_Ky\), it is necessary to constrain the representations of \(W_Qx\) and \(W_Ky\) during training. For example, maintaining a small <strong>POCP</strong> or performing key vector normalization can achieve this.</li> </ul> <h2 id="future-work">Future Work</h2> <p>In this blog, we explore a key limitation of RoPE: it cannot maintain long-term decay with significantly longer contexts. This issue may arises from the high <strong>POCP</strong>. In the future, we will investigate why the model cannot maintain a low <strong>POCP</strong> on long-context samples and design a new mechanism to address this issue.</p> <h1 id="appendix">Appendix</h1> <p>Here is a code implementation to calculate the <strong>POCP,</strong> given two randomly initialized vectors:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Note: This implementation differs from that in Llama2!!!
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">128</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">vector</span><span class="p">):</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">norm</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">vector</span>
    <span class="k">return</span> <span class="n">vector</span> <span class="o">/</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">e_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">e_x</span> <span class="o">/</span> <span class="n">e_x</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">generate_vectors_with_angle_less_than_90</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">cent</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">Embedding dimension must be even.</span><span class="sh">"</span>
    
    <span class="c1"># Randomly initialize vectors
</span>    <span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    
    <span class="n">sub_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">sub_dim</span><span class="p">):</span>
        <span class="n">q_sub</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="o">*</span><span class="mi">2</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">k_sub</span> <span class="o">=</span> <span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="o">*</span><span class="mi">2</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>
        
        <span class="c1"># If the dot product of the subcomponents is less than or equal to 0, adjust the subcomponent of K
</span>        <span class="k">while</span> <span class="n">torch</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">q_sub</span><span class="p">,</span> <span class="n">k_sub</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="o">*</span><span class="mi">2</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">k_sub</span> <span class="o">=</span> <span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="o">*</span><span class="mi">2</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>
    
    <span class="n">sample_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">sub_dim</span> <span class="o">*</span> <span class="n">cent</span><span class="p">)</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">sub_dim</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">sub_dim</span><span class="p">)),</span> <span class="n">sample_size</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">:</span>
        <span class="n">q_sub</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="o">*</span><span class="mi">2</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">k_sub</span> <span class="o">=</span> <span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="o">*</span><span class="mi">2</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>
        
        <span class="c1"># If the dot product of the subcomponents is less than or equal to 0, adjust the subcomponent of K
</span>        <span class="k">while</span> <span class="n">torch</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">q_sub</span><span class="p">,</span> <span class="n">k_sub</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="o">*</span><span class="mi">2</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">k_sub</span> <span class="o">=</span> <span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="o">*</span><span class="mi">2</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span>
           
    <span class="k">return</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span>

<span class="c1"># Define the RoPE function, considering relative position
</span><span class="k">def</span> <span class="nf">apply_rope</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">position</span><span class="p">):</span>
    <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">position</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">Embedding dimension must be even for RoPE.</span><span class="sh">"</span>
    
    <span class="n">half_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">freq_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">half_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">(</span><span class="n">freq_seq</span> <span class="o">/</span> <span class="n">half_dim</span><span class="p">))</span>
    
    <span class="c1"># Calculate sinusoid input
</span>    <span class="n">sinusoid_inp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">i,j-&gt;ij</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">position</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">inv_freq</span><span class="p">)</span>
    
    <span class="c1"># Calculate sin and cos, and reshape them to match the input tensor's shape
</span>    <span class="n">sin</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">sinusoid_inp</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">sinusoid_inp</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># Split the tensor into real and imaginary parts (even and odd indices)
</span>    <span class="n">tensor_1</span><span class="p">,</span> <span class="n">tensor_2</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">],</span> <span class="n">tensor</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
    
    <span class="c1"># Apply RoPE
</span>    <span class="n">tensor_1_rot</span> <span class="o">=</span> <span class="n">tensor_1</span> <span class="o">*</span> <span class="n">cos</span> <span class="o">-</span> <span class="n">tensor_2</span> <span class="o">*</span> <span class="n">sin</span>
    <span class="n">tensor_2_rot</span> <span class="o">=</span> <span class="n">tensor_1</span> <span class="o">*</span> <span class="n">sin</span> <span class="o">+</span> <span class="n">tensor_2</span> <span class="o">*</span> <span class="n">cos</span>
    
    <span class="c1"># Interleave the real and imaginary parts back together
</span>    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
    <span class="n">result</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor_1_rot</span>
    <span class="n">result</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor_2_rot</span>
    
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># Apply RoPE, setting the relative position
</span><span class="n">position_Q</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Position of Q
</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dic</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="k">for</span> <span class="n">cent</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]:</span>
    <span class="n">Q</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="nf">generate_vectors_with_angle_less_than_90</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">cent</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">position_K</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">16000</span><span class="p">,</span> <span class="mi">200</span><span class="p">):</span>
        <span class="n">Q_rope</span> <span class="o">=</span> <span class="nf">apply_rope</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">position_Q</span><span class="p">)</span>
        <span class="n">K_rope</span> <span class="o">=</span> <span class="nf">apply_rope</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">position_K</span><span class="p">)</span>

        <span class="c1"># Calculate Q^T * K
</span>        <span class="n">Q_T</span> <span class="o">=</span> <span class="n">Q_rope</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Transpose Q
</span>        <span class="n">QK</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">K_rope</span><span class="p">,</span> <span class="n">Q_T</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">))</span> <span class="c1"># Matrix multiplication
</span>
        <span class="c1">#print("Position:" + str(position_K) + ", QK^T:", QK)
</span>        <span class="n">item</span> <span class="o">=</span> <span class="sh">"</span><span class="s">proportion: %.2f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">cent</span>
        <span class="n">dic</span><span class="p">[</span><span class="n">item</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">QK</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

<span class="k">for</span> <span class="n">position_K</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">16000</span><span class="p">,</span> <span class="mi">200</span><span class="p">):</span>
    <span class="n">x</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">position_K</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">dic</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="c1">#plt.plot(x, softmax(val), linestyle='-', label=key)
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">key</span><span class="p">)</span>

<span class="c1"># Add title and labels
</span><span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">ROPE Curve (embedding size = %i)</span><span class="sh">'</span> <span class="o">%</span> <span class="n">embed_dim</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">positional distance</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">attention score</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Display legend
</span><span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Show grid lines
</span><span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Save the plot to a file
</span><span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">sample_curve.png</span><span class="sh">'</span><span class="p">)</span></code></pre></figure> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2025/assets/bibliography/2025-04-28-pocp.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>