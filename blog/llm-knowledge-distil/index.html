<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},i=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),a=i[0][0],o=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"April 28, 2025"),r="On LLM Knowledge Distillation - A Comparison between Forward KL and Reverse KL",l="In this blog post, we delve into knowledge distillation techniques for Large Language Models (LLMs), with a particular focus on using Kullback-Leibler (KL) Divergence as the optimization objective. Knowledge distillation is a powerful tool to reduce model size while maintaining comparable performance, making it especially useful in scenarios with constrained computational or serving resources. We specifically explore the nuances of Forward KL divergence and Reverse KL divergence, examining their roles in the distillation process. By comparing these two approaches, we aim to uncover their behaviours, strengths, and practical applications in LLM distillation.";{let e=i.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(a+"2025"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${r}},\n  abstract = {${l}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${o}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=i.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${r}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>On LLM Knowledge Distillation - A Comparison between Forward KL and Reverse KL | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="In this blog post, we delve into knowledge distillation techniques for Large Language Models (LLMs), with a particular focus on using Kullback-Leibler (KL) Divergence as the optimization objective. Knowledge distillation is a powerful tool to reduce model size while maintaining comparable performance, making it especially useful in scenarios with constrained computational or serving resources. We specifically explore the nuances of Forward KL divergence and Reverse KL divergence, examining their roles in the distillation process. By comparing these two approaches, we aim to uncover their behaviours, strengths, and practical applications in LLM distillation."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/2025/assets/css/main.css"> <link rel="canonical" href="https://starlight345.github.io/2025/blog/llm-knowledge-distil/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/2025/assets/js/theme.js"></script> <script src="/2025/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2025/assets/js/distillpub/template.v2.js"></script> <script src="/2025/assets/js/distillpub/transforms.v2.js"></script> <script src="/2025/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <d-front-matter> <script async type="text/json">{
      "title": "On LLM Knowledge Distillation - A Comparison between Forward KL and Reverse KL",
      "description": "In this blog post, we delve into knowledge distillation techniques for Large Language Models (LLMs), with a particular focus on using Kullback-Leibler (KL) Divergence as the optimization objective. Knowledge distillation is a powerful tool to reduce model size while maintaining comparable performance, making it especially useful in scenarios with constrained computational or serving resources. We specifically explore the nuances of Forward KL divergence and Reverse KL divergence, examining their roles in the distillation process. By comparing these two approaches, we aim to uncover their behaviours, strengths, and practical applications in LLM distillation.",
      "published": "April 28, 2025",
      "authors": [
        {
          "author": "Anonymous",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/reviewing/">reviewing</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>On LLM Knowledge Distillation - A Comparison between Forward KL and Reverse KL</h1> <p>In this blog post, we delve into knowledge distillation techniques for Large Language Models (LLMs), with a particular focus on using Kullback-Leibler (KL) Divergence as the optimization objective. Knowledge distillation is a powerful tool to reduce model size while maintaining comparable performance, making it especially useful in scenarios with constrained computational or serving resources. We specifically explore the nuances of Forward KL divergence and Reverse KL divergence, examining their roles in the distillation process. By comparing these two approaches, we aim to uncover their behaviours, strengths, and practical applications in LLM distillation.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#background">Background</a></div> <div><a href="#preliminaries">Preliminaries</a></div> <ul> <li><a href="#knowledge-distillation-in-conditional-language-modeling">Knowledge Distillation in Conditional Language Modeling</a></li> <li><a href="#forward-kl-divergence-and-reverse-kl-divergence">Forward KL Divergence and Reverse KL Divergence</a></li> </ul> <div><a href="#modern-llm-distillation-methods">Modern LLM Distillation Methods</a></div> <ul> <li><a href="#modeling-from-token-level">Modeling from Token Level</a></li> <li><a href="#modeling-from-sequence-level">Modeling from Sequence Level</a></li> </ul> <div><a href="#empirical-comparison-how-it-works-and-which-one-is-better">Empirical Comparison, how it works and which one is better?</a></div> <ul> <li><a href="#experiment-details">Experiment Details</a></li> <li><a href="#implementation-of-token-level-knowledge-distillation">Implementation of Token-level Knowledge Distillation</a></li> <li><a href="#empirical-study-on-task-agnostic-knowledge-distillation">Empirical Study on Task-Agnostic Knowledge Distillation</a></li> </ul> <div><a href="#closing-remarks">Closing Remarks</a></div> </nav> </d-contents> <h2 id="background">Background</h2> <p>In recent years, knowledge distillation (KD)<d-cite key="hinton2015distilling"></d-cite> has gathered significant attention, particularly in the area of generative large language models (LLMs). Open-source LLMs, such as Qwen2<d-cite key="yang2024qwen2"></d-cite> and LLaMA-3<d-cite key="dubey2024llama"></d-cite>, come in a wide range of sizes, from 0.5 billion to 405 billion parameters. This diversity makes them ideal candidates for KD: training a high-performing model with a larger teacher model and subsequently distilling its knowledge into a smaller student model. This process is especially valuable when computational resources are constrained.</p> <p>There are two primary approaches to knowledge distillation in LLMs:</p> <ol> <li> <p><strong>Sequence level distillation</strong>: This involves prompting the teacher model to generate responses, which are then used for distillation training. This approach is particularly effective when the teacher is a black-box system, accessible only via APIs.</p> </li> <li> <p><strong>Token level distillation</strong>: This approach aligns the intermediate outputs, such as logits or embeddings, between the teacher and student models. By focusing on token-level or layer-level alignment, it enables the distillation of deeper knowledge beyond the model’s final outputs.</p> </li> </ol> <p>In this blog post, we specifically focus on white-box knowledge distillation, which provides greater access to the teacher model’s internal representations and mechanisms. Unlike traditional knowledge distillation, which is primarily applied to classification-dominated tasks, knowledge distillation for modern generative language models presents unique challenges. For example, vanilla knowledge distillation using forward KL divergence as the loss function has been shown to introduce issues such as hallucination in language models. This could arise from forward KL’s inherent tendency toward mean-seeking optimization, which can lead to distorted or unrealistic outputs in generative tasks.</p> <p>In this blog post, we will:</p> <ul> <li>Provide an overview of knowledge distillation in generative language models, and the theoretical underpinnings of forward KL and reverse KL Divergence.</li> <li>Review and analyze recent advancements in KD for LLMs, conducting an empirical comparison of methods.</li> <li>Investigate forward KL divergence and reverse KL divergence fitting behaviour in detail.</li> </ul> <p>By bridging these gaps, we aim to advance the understanding and application of KD in generative language modeling tasks. </p> <h2 id="preliminaries">Preliminaries</h2> <h3 id="knowledge-distillation-in-conditional-language-modeling">Knowledge Distillation in Conditional Language Modeling</h3> <p>Given a source-target pair (commonly referred to as an instruction-response) $(x,y)$, a language model $M$ is trained to accept the input $x$ and produce an output $\hat{y}=M(x)$, with the optimization objective being to minimize the discrepancy between $\hat{y}$ and $y$. Here, $x$, $y$, and $\hat{y}$ are sentences, and gradients are computed at the sentence level.</p> <p>In the context of <strong>knowledge distillation</strong> for conditional language modeling, given an input source or instruction $x$, a <strong>teacher model</strong> generates a probability distribution $p(y\mid x)$, while a <strong>student model</strong>, parameterized by $\theta$, generates a distribution $q_\theta(y\mid x)$. The goal of knowledge distillation is to minimize the divergence between the teacher’s distribution $p(y\mid x)$ and the student’s distribution $q_\theta(y\mid x)$, enabling the student model to “mimic” the teacher’s behavior.</p> <p>To ensure stable training, the distillation loss is typically combined with the supervised fine-tuning loss, allowing the student model to balance imitation of the teacher with alignment to ground truth data.</p> <h3 id="kl-divergence">KL Divergence</h3> <p>The Kullback-Leibler (KL) divergence is a commonly used measure of the “distance” of two distributions. It can identity how far one distribution is to another. This is very useful in knowledge distillation cause the optimization goal we mentioned above is to make the student distribution similar enough to the teacher distribution. Using the denotes we have to formulate the KL divergence in knowledge distillation problems, given a student distribution (approximate distribution) $q_\theta(y\mid x)$ and a teacher distribution (true distribution) $p(y\mid x)$, the KL divergence can be formulated as<d-footnote>https://dibyaghosh.com/blog/probability/kldivergence.html</d-footnote>:</p> \[D_{KL}(p\|q_\theta)=\mathbb{E}_{y \sim p} \left[ \log \frac{p(y\mid x)}{q(y\mid x)} \right] \tag{1}\] <p>To be noticed, KL divergence is not a “symmetric” measure, which means that $D_{KL}(p|q_\theta)$ is not completely equal to $D_{KL}(q_\theta|p)$, even though the “meaning” is the same – how similar one distribution is to the other one.</p> <h3 id="forward-kl-divergence-and-reverse-kl-divergence">Forward KL Divergence and Reverse KL Divergence</h3> <p>The difference between $D_{KL}(p|q_\theta)$ and $D_{KL}(q_\theta|p)$ becomes very prominent when using this KL divergence in optimization, i.e. minimizing the difference between two distributions. When we let student distribution to fit the real distribution, or the teacher distribution here, different order of $p$ and $q_\theta$ will result in difference in fitting performance, especially in the first several steps.</p> <p>Suppose $D_{FKL}=D_{KL}(p|q_\theta)$ and $D_{RKL}=D_{KL}(q_\theta|p)$, where $D_{FKL}$ refers to forward KL divergence and $D_{RKL}$ refers to reverse KL divergence, the optimization goal can be formulated as:</p> \[\begin{aligned} \arg\min_{\theta} D_{FKL} &amp;= \arg\min_{\theta} D_{KL}(p\|q_\theta) \\ &amp;= \arg\min_{\theta} \mathbb{E}_{y \sim p} \left[ \log \frac{p(y|x)}{q_\theta(y|x)} \right] \\ &amp;= \arg\max_{\theta} \mathbb{E}_{y \sim p} \left[ \log q_\theta(y|x) \right] \end{aligned} \tag{2}\] <p>and for reverse KL:</p> \[\begin{aligned} \arg\min_{\theta} D_{RKL} &amp;= \arg\min_{\theta} D_{KL}(q_\theta \| p) \\ &amp;= \arg\min_{\theta} \mathbb{E}_{y \sim q_\theta} \left[ \log \frac{q_\theta(y|x)}{p(y|x)} \right] \\ &amp;= \arg\max_{\theta} \mathbb{E}_{y \sim q_\theta} \left[ \log p(y|x) \right] + \mathcal{H}(q_\theta(y|x)) \end{aligned} \tag{3}\] <p>Forward KL is a mean-seeking behavior, while Reverse KL is a Mode-Seeking behavior <d-footnote>https://dibyaghosh.com/blog/probability/kldivergence.html</d-footnote> . This observation is often demonstrated by using one gaussian distribution to fit sum of two gaussian distributions. the fit progress could be found in the following figures.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/fwd_kl.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/fwd_kl.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/fwd_kl.gif-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-llm-knowledge-distil/fwd_kl.gif" class="img-fluid " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/rev_kl.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/rev_kl.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/rev_kl.gif-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-llm-knowledge-distil/rev_kl.gif" class="img-fluid " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. One Gaussian distribution fitting sum of two Gaussian distributions. </div> <p>To understand this phenomenon, $D_{FKL}$ represents the expectation calculated under the $p$ distribution, so it will match $q_\theta$ to $p$ where $p$ is high and the $q_\theta$ is low during the first steps, so $D_{FKL}$ will firstly increase $q_\theta$ where $q_\theta$ is low and $p$ is high. Under the condition of fitting two Gaussian distributions with one Gaussian distribution, the forward KL divergence will make the fitting distribution to be mean-seeking behavior</p> <p>On the other hand, $D_{RKL}$ represents the expectation calculated under the $q_\theta$ distribution, so it will match $q_\theta$ to $p$ where $q_\theta$ is high and $p$ is low during the first steps, so $D_{RKL}$ will firstly decrease $q_\theta$ where $q_\theta$ is high and $p$ is low. Under the condition of fitting two Gaussian distributions with one Gaussian distribution, after reverse KL find a peak, it will stay at that Local optimum, which makes the fitting distribution to be mode-seeking behavior.</p> <p>The behavior changes when a stronger student model is employed. In the following figures, we illustrate this by fitting the sum of two Gaussian distributions using sum of two Gaussian distributions. Both forward KL and reverse KL are capable of approximating the sum. Under these optimization settings, forward KL converges to a solution around step 100, while reverse KL achieves convergence around step 350. This suggests that with a sufficiently powerful student model and enough training steps, forward KL and reverse KL are likely to exhibit similar performance.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/fwd_kl_dual.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/fwd_kl_dual.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/fwd_kl_dual.gif-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-llm-knowledge-distil/fwd_kl_dual.gif" class="img-fluid " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/rev_kl_dual.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/rev_kl_dual.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/rev_kl_dual.gif-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-llm-knowledge-distil/rev_kl_dual.gif" class="img-fluid " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Two Gaussian distributions fitting sum of two Gaussian distributions </div> <p>The detailed code to generate these images could be found in <code class="language-plaintext highlighter-rouge">toy_example_fkl_rkl.py</code> and <code class="language-plaintext highlighter-rouge">toy_example_fkl_rkl_v2.py</code></p> <h2 id="modern-llm-distillation-methods">Modern LLM Distillation Methods</h2> <p>To better suit knowledge distillation methods with modern large language model finetuning, multiple methods have been proposed. In this section, we summarize these methods from two levels, distillation from token level and from sequence (sentence) level.</p> <p>As previously discussed, source/input $x$, output $\hat{y}$ and target $y$ are all sentences. During finetuning and knowledge distillation, the gradients can be applied to the general sentence, or the separated tokens. Here we denote $y=\{y_t\}_{t=1}^T$ where $y_t$ refers to the token at position $t$, and $T$ refers to the length of the sentence, i.e. number of tokens in $y$.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/token-sequence-level-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/token-sequence-level-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/token-sequence-level-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-llm-knowledge-distil/token-sequence-level.png" class="img-fluid" width="" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 3. Illustration of token level knowledge distillation and sequence level knowledge distillation. </div> <h3 id="modeling-from-token-level">Modeling from Token Level</h3> <p>Most methods now are modeling the sentence distributions from token levels. By tokenizing the sentence $y$ into a sequence $\{y_t\}_{t=1}^T$, we can formulate the distillation optimization goal as:</p> \[\begin{aligned} \arg\min \mathcal{L}_{KL} &amp;= \arg\min D_{KL}(p\|q_\theta) \\ &amp;= \arg\min \sum_{t=1}^T D_{KL}( p( y_t \mid x,y_{1:t-1} ) \ \| \ q_{\theta}( y_t \mid x,y_{1:t-1} ) ) \end{aligned} \tag{4}\] <p>For token level knowledge distillation, the optimization goal per token is the same as the ones frequently used in embedding distillation, and computer vision. Forward KL divergence and reverse KL divergence are both commonly used loss functions in token level distillation. There’s no very clear observation or proof of which one would be a better choice for which case. Since the optimization goal is the same, the performance doesn’t seem to differ a lot when the model is fully trained. Sometimes both work, and inspired by this, some people begin to add the forward KL and reverse KL together. <d-cite key="wu2024rethinking"></d-cite> proposes Adaptive KL (AKL) divergence, which is an adaptive weighted sum of reverse KL and forward KL.</p> <h3 id="modeling-from-sequence-level">Modeling from Sequence Level</h3> <p>Different from token level distillation, sequence level distillation aims to let the student model match the teacher’s output probability over the whole sequence. For a generative model, it acquires knowledge by learning from the real-world distributions, which is natural language here. By performing Monte Carlo sampling from this distribution, the model generates sentences. From a token-level perspective, learning at each token position can be seen as a token classification task. However, from a sequence-level perspective, the entire sentence represents a sample drawn from the generative model’s learned distribution. This fundamental characteristic emphasizes a key distinction between sequence-level and token-level knowledge distillation in large language models. In sequence-level knowledge distillation, Monte Carlo sampling is typically used to draw samples that approximate the target distribution, capturing the sequence-level dynamics of the model’s behavior. This approach inherently differs from the token-level distillation process, where focus lies on individual token probabilities rather than the whole sequence.</p> <p>From the perspective of implementation, the monte carlo sampling refers to <code class="language-plaintext highlighter-rouge">model.generate</code>. For a given input <code class="language-plaintext highlighter-rouge">source</code>, we can get two kinds of outputs from the model:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tokenized_inputs = tokenizer(**tokenized_inputs)
output_logits = model(**tokenized_inputs) # This is the logits/token level distribution.
output_sentence = model.generate(**tokenized_inputs) # This is the decoded/sampled sentence from the model.
</code></pre></div></div> <p>where <code class="language-plaintext highlighter-rouge">output_logits</code> is the token level distribution, which is used to do token level distillation, and <code class="language-plaintext highlighter-rouge">output_sentence</code> is the sampled sequence, which is used in sequence level distillation.</p> <p>Forward KL and Reverse KL are often employed in sequence-level knowledge distillation. The forward KL optimization goal can be formulated as:</p> \[\arg\min D_{KL}(p\|q_\theta)=\arg\min \mathbb{E}_{y\sim p}\log \frac{p(y\mid x)}{q_\theta(y\mid x)}\] <p>we can directly sample sentences $y from the teacher distribution. In simple terms, this optimization function lets the teacher model generate responses and uses the forward KL divergence as the loss function in knowledge distillation.</p> <p>However, when the objective switches to reverse KL, i.e.,</p> \[\arg\min D_{KL}(q_\theta\|p)=\arg\min -\mathbb{E}_{y\sim q_\theta}\log \frac{(y\mid x)}{q_\theta(y\mid x)}\] <p>we need to sample from the student distribution. Since the student distribution is parameterized, it becomes infeasible to directly calculate the KL divergence for optimization, as in the case of forward KL. MiniLLM<d-cite key="gu2024minillm"></d-cite>addresses this challenge by approximating $D_{KL}(q_\theta|p)$ using policy gradient methods, making it possible to optimize effectively even under the reverse KL setup. GKD<d-cite key="agarwal2023gkd"></d-cite> introduce Generalized Knowledge Distillation (GKD) to provide flexibility in choosing both the generation source (teacher model or student model) and the loss functions between the student and teacher.</p> <h2 id="empirical-comparison-how-it-works-and-which-one-is-better">Empirical Comparison, how it works and which one is better?</h2> <p>In this section, we present our empirical study on using forward KL and reverse KL for large language model distillation. We walk through the implementation of token-level knowledge distillation and common problems in knowledge distillation.</p> <h3 id="experiment-details">Experiment Details</h3> <p>We use a subset of 20,000 examples randomly sampled from <code class="language-plaintext highlighter-rouge">HuggingFaceH4/ultrachat_200k</code><d-footnote>https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k</d-footnote> dataset as sampled from real natural language distribution. For base models, we use <code class="language-plaintext highlighter-rouge">Qwen2.5</code> series models as the training starting point. All experiments are completed using one node with 8 Nvidia A100 80G GPUs. Code and datasets will be released in [ADD GITHUB LINK HERE].</p> <h3 id="implementation-of-token-level-knowledge-distillation">Implementation of Token-level Knowledge Distillation</h3> <p>In this section, we use forward KL as an simple example. For easy implementation and experimentation, we recommend using <code class="language-plaintext highlighter-rouge">trl</code> trainers<d-footnote>https://github.com/huggingface/trl</d-footnote> and Huggingface <code class="language-plaintext highlighter-rouge">alignment-handbook</code><d-footnote>https://github.com/huggingface/alignment-handbook</d-footnote>.</p> <p>We inherits <code class="language-plaintext highlighter-rouge">DistilTrainer</code> from <code class="language-plaintext highlighter-rouge">trl</code>’s <code class="language-plaintext highlighter-rouge">SFTTrainer</code>, so that we don’t need to add some commonly used hyperparameters and functions. Similar implementation can be found in <a href="https://github.com/arcee-ai/DistillKit" rel="external nofollow noopener noopener noreferrer" target="_blank">this code repo</a>. Below is a comparison between the token level forward KL distillation implementation and the one at sequence level.</p> <div style="display: flex; width: 120%; height: 500px;"> <div style="flex: 1; resize: both; overflow: auto; border: 0px solid #ccc; padding: 10px;"> <pre>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Forward KL distillation at token level:
</span><span class="k">class</span> <span class="nc">TokenDistilTrainer</span><span class="p">(</span><span class="n">SFTTrainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">distillation_loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="p">...):</span>
        
        <span class="n">student_logits_scaled</span> <span class="o">=</span> <span class="n">student_logits</span> <span class="o">/</span> <span class="n">temperature</span>
        <span class="n">teacher_logits_scaled</span> <span class="o">=</span> <span class="n">teacher_logits</span> <span class="o">/</span> <span class="n">temperature</span>

        <span class="c1"># Compute probabilities and log probabilities for both student and teacher
</span>        <span class="n">student_log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">student_logits_scaled</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">teacher_log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">teacher_logits_scaled</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">student_probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">student_logits_scaled</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># noqa
</span>
        <span class="n">loss_kd</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">kl_div</span><span class="p">(</span>
            <span class="n">student_log_probs</span><span class="p">,</span>
            <span class="n">teacher_log_probs</span><span class="p">,</span>
            <span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">batchmean</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">log_teacher</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">temperature</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">max_seq_length</span>

        <span class="k">return</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">loss_kd</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">original_loss</span></code></pre></figure> </pre> </div> <div style="width: 2px; background-color: #ccc; cursor: col-resize;"></div> <div style="flex: 1; resize: both; overflow: auto; border: 0px solid #ccc; padding: 10px;"> <pre>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Forward KL distillation at sequence level
</span><span class="k">class</span> <span class="nc">SeqDistilTrainer</span><span class="p">(</span><span class="n">SFTTrainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">distillation_loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="p">...)</span>
        <span class="n">prompt_lengths</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">100</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">max_prompt_length</span> <span class="o">=</span> <span class="n">prompt_lengths</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span>
        <span class="n">prompt_input_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">][:,</span> <span class="p">:</span><span class="n">max_prompt_length</span><span class="p">].</span><span class="nf">clone</span><span class="p">()</span>

        <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">max_prompt_length</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  
        <span class="n">prompt_mask</span> <span class="o">=</span> <span class="n">positions</span> <span class="o">&lt;</span> <span class="n">prompt_lengths</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">prompt_input_ids</span><span class="p">[</span><span class="o">~</span><span class="n">prompt_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span>
        <span class="n">prompt_attention_masks</span> <span class="o">=</span> <span class="n">prompt_mask</span><span class="p">.</span><span class="nf">long</span><span class="p">()</span>

        <span class="n">generated_sequences</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">prompt_input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">prompt_attention_masks</span><span class="p">,</span>
            <span class="n">max_new_tokens</span><span class="o">=</span><span class="nf">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">args</span><span class="p">,</span> <span class="sh">'</span><span class="s">max_new_tokens</span><span class="sh">'</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
            <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)</span>

        <span class="n">gen_input_ids</span> <span class="o">=</span> <span class="n">generated_sequences</span>
        <span class="n">gen_attention_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">gen_input_ids</span> <span class="o">!=</span> <span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>

        <span class="n">gen_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">:</span> <span class="n">gen_input_ids</span><span class="p">,</span> 
            <span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">gen_attention_mask</span>
        <span class="p">}</span>

        <span class="n">student_outputs_gen</span> <span class="o">=</span> <span class="nf">student_model</span><span class="p">(</span><span class="o">**</span><span class="n">gen_inputs</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">teacher_outputs_gen</span> <span class="o">=</span> <span class="nf">teacher_model</span><span class="p">(</span><span class="o">**</span><span class="n">gen_inputs</span><span class="p">)</span>

        <span class="n">student_logits</span> <span class="o">=</span> <span class="n">student_outputs_gen</span><span class="p">.</span><span class="n">logits</span>
        <span class="n">teacher_logits</span> <span class="o">=</span> <span class="n">teacher_outputs_gen</span><span class="p">.</span><span class="n">logits</span>

        <span class="n">student_log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">student_logits</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">teacher_log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">teacher_logits</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">loss_kd</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">forward_kl_loss</span><span class="p">(</span>
            <span class="n">student_log_probs</span><span class="p">,</span>
            <span class="n">teacher_log_probs</span><span class="p">,</span>
            <span class="n">temperature</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">loss_kd</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">original_loss</span></code></pre></figure> </pre> </div> </div> <p>Here we mainly emphasize the distillation loss. For token level distillation, it’s relatively easier since we don’t need to sample the sequence from the teacher model. The <code class="language-plaintext highlighter-rouge">original_loss</code> refers to the original pretraining/supervised finetuning loss to stable the training. In token level distillation, we only need to call <code class="language-plaintext highlighter-rouge">model(**inputs)</code> to get the causal LM logits output, which means, probability distribution at each token position, and then call <code class="language-plaintext highlighter-rouge">F.kl_div</code> to calculate the forward KL divergence between the student logits and the teacher logits. However, for sequence level, we’ll need to call <code class="language-plaintext highlighter-rouge">model.generate(prompt)</code> first the sample the sequence output from the teacher model, and then use <code class="language-plaintext highlighter-rouge">model(**gen_inputs)</code> to get the logits.</p> <p>Two common problems arise during distillation training: (1) out-of-memory (OOM) issues and (2) low efficiency in sequence-level training. To address frequent OOM issues during distillation, we strongly recommend leveraging the DeepSpeed strategies. Specifically, refer to <a href="https://github.com/huggingface/trl/blob/10c2f63b2ac8564cca28aa1598a1f3ac6a5fc63c/trl/trainer/ppo_trainer.py#L1461" rel="external nofollow noopener noopener noreferrer" target="_blank">the PPOTrainer example here</a> and ensure <code class="language-plaintext highlighter-rouge">_prepare_deepspeed</code> is implemented accordingly to optimize resource utilization. And for the second issue, low efficiency in sequence-level training occurs because it requires calling <code class="language-plaintext highlighter-rouge">model.generate</code>, which involves sampling from the learned distribution. To accelerate this process, we can (1) optimize <code class="language-plaintext highlighter-rouge">model.generate</code> with <code class="language-plaintext highlighter-rouge">vllm</code> and optimzie the GPU utilization (2) save the precomputed sequence outputs first and then load them during training. This also works when the teacher model is exceptionally large.</p> <h3 id="empirical-study-on-task-agnostic-knowledge-distillation">Empirical Study on Task-Agnostic Knowledge Distillation</h3> <h4 id="token-level-forward-kl-and-reverse-kl">Token-level Forward KL and Reverse KL</h4> <p>In this section, we present our experiment results on token-level forward KL and reverse KL in LLM knowledge distillation in the Table. All models are evaluated using the same eval set on ROUGE1, ROUGE2, ROUGEL and BARTScore. We continue using the above denotations, where $q_\theta$ refers to the parameterized student model, and $p$ refers to the teacher model. Except for distilled models, we also present the supervised finetuning results as baselines. All experiments settings are kept the same, which are available in code repo.</p> <table> <thead> <tr> <th>$q_\theta$</th> <th>$p$</th> <th>$q_\theta$ Size</th> <th>$p$ Size</th> <th>Loss</th> <th>ROUGE1</th> <th>ROUGE2</th> <th>ROUGEL</th> <th>BARTScore</th> </tr> </thead> <tbody> <tr> <td>Instruct</td> <td>-</td> <td>7B</td> <td>-</td> <td>-</td> <td>0.4613</td> <td>0.2059</td> <td>0.2705</td> <td>-2.5047</td> </tr> <tr> <td>Ultra</td> <td>-</td> <td>1.5B</td> <td>-</td> <td>1.0*SFT</td> <td>0.5295</td> <td>0.2562</td> <td>0.3414</td> <td>-2.5242</td> </tr> <tr> <td>Ultra</td> <td>-</td> <td>7B</td> <td>-</td> <td>1.0*SFT</td> <td>0.5576</td> <td>0.283</td> <td>0.364</td> <td>-2.4594</td> </tr> <tr> <td>Instruct</td> <td>Instruct</td> <td>7B</td> <td>1.5B</td> <td>0.5<em>SFT+0.5</em>FKL</td> <td>0.5369</td> <td>0.2595</td> <td>0.3435</td> <td>-2.5134</td> </tr> <tr> <td>Ultra</td> <td>Instruct</td> <td>7B</td> <td>1.5B</td> <td>0.5<em>SFT+0.5</em>FKL</td> <td>0.5404</td> <td>0.2615</td> <td>0.3463</td> <td>-2.5104</td> </tr> <tr> <td>Ultra</td> <td>Instruct</td> <td>7B</td> <td>1.5B</td> <td>0.8<em>SFT+0.2</em>FKL</td> <td>0.5292</td> <td>0.2567</td> <td>0.3406</td> <td>-2.5235</td> </tr> <tr> <td>Ultra</td> <td>Instruct</td> <td>7B</td> <td>1.5B</td> <td>0.5<em>SFT+0.5</em>RKL</td> <td>0.5291</td> <td>0.2558</td> <td>0.3408</td> <td>-2.5211</td> </tr> </tbody> </table> <div class="caption"> Table 1. Token level forward KL and reverse KL knowledge distillation results. Since the base model used here is Qwen2.5 family, we use Instruct to refer to Qwen2.5-instruct series models, for instance, Instruct 1.5B refers to Qwen2.5-1.5B-Instruct. Ultra refers to Qwen2.5-instruct models finetuned on the Ultrachat subset training set. For loss functions, SFT refers to supervised finetuning cross entropy loss, FKL refers to forward KL divergence loss, and RKL refers to reverse KL divergence loss. </div> <p>From the above results, we can see that for token level, there’s not much difference between forward KL and reverse KL. By adding the supervised finetuning cross entropy loss, the general learning can be more stablized.</p> <h4 id="convergence-speed">Convergence speed</h4> <p>Let’s begin the comparison with a simple task. Consider a single-layer fully connected network without bias, where both the input and output dimensions are 64. The network’s output is directly passed through a softmax layer.</p> <p>We generated a fixed weight matrix with varying expected ranks and used this matrix as a fixed teacher model. Two student models with identical structures were trained, one guided by forward KL loss and the other by reverse KL loss.</p> <p>Since forward KL loss and reverse KL loss differ in their formulations, their loss values are not directly comparable. Instead, we assess their convergence speeds using two proxy metrics: the L2 distance between the student and teacher probability distributions, and the L2 distance between the teacher’s weights and the student’s weights.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank2-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank2.jpg" class="img-fluid " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank4-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank4.jpg" class="img-fluid " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank8-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank8-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank8-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank8.jpg" class="img-fluid " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank16-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank16-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank16-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank16.jpg" class="img-fluid " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank32-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank32-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank32-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank32.jpg" class="img-fluid " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank64-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank64-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank64-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-llm-knowledge-distil/kl_cmp_rank64.jpg" class="img-fluid " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Convergence speed for Reverse KL and Forword KL. </div> <p>Across all the images above, regardless of changes in the target matrix’s rank, forward KL consistently outperforms reverse KL in both weight-L2 and L2 loss.</p> <h2 id="closing-remarks">Closing Remarks</h2> <p>In this blog post, we examined knowledge distillation techniques for Large Language Models (LLMs), specifically comparing Forward KL and Reverse KL Divergence. Our empirical results demonstrate that both divergence measures perform similarly at the token level, with Forward KL showing faster convergence in simple scenarios. However, the choice between Forward KL and Reverse KL may depend on specific model architectures and training conditions.</p> <p>We also highlighted the challenges of applying knowledge distillation methods from computer vision to generative language models, emphasizing the need for specialized approaches in the context of LLMs. Future research could explore hybrid divergence strategies or adaptive weighting to further optimize distillation performance.</p> <p>Effective knowledge distillation remains crucial for developing efficient LLMs that maintain high performance while reducing computational requirements. By continuing to refine these techniques, we can enable broader deployment of powerful language models in diverse applications.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2025/assets/bibliography/2025-04-28-llm-knowledge-distil.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>