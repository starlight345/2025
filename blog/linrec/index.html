<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},r=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),a=r[0][0],i=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"April 28, 2025"),o="Linear Recurrences Accessible to Everyone",l="Investigating linear RNNs such as Mamba, can be challenging because they are currently not efficiently expressible in PyTorch. We propose the abstraction of linear recurrences to gain intuition for the computational structure of these emerging deep learning architectures. After deriving their parallel algorithm, we gradually build towards a simple template CUDA extension for PyTorch. We hope that making linear recurrences accessible to a wider audience inspires further research on linear-time sequence mixing.";{let e=r.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(a+"2025"+o.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${o}},\n  abstract = {${l}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${i}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=r.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${o}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Linear Recurrences Accessible to Everyone | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Investigating linear RNNs such as Mamba, can be challenging because they are currently not efficiently expressible in PyTorch. We propose the abstraction of linear recurrences to gain intuition for the computational structure of these emerging deep learning architectures. After deriving their parallel algorithm, we gradually build towards a simple template CUDA extension for PyTorch. We hope that making linear recurrences accessible to a wider audience inspires further research on linear-time sequence mixing."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/2025/assets/css/main.css"> <link rel="canonical" href="https://starlight345.github.io/2025/blog/linrec/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/2025/assets/js/theme.js"></script> <script src="/2025/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2025/assets/js/distillpub/template.v2.js"></script> <script src="/2025/assets/js/distillpub/transforms.v2.js"></script> <script src="/2025/assets/js/distillpub/overrides.js"></script> <d-front-matter> <script async type="text/json">{
      "title": "Linear Recurrences Accessible to Everyone",
      "description": "Investigating linear RNNs such as Mamba, can be challenging because they are currently not efficiently expressible in PyTorch. We propose the abstraction of linear recurrences to gain intuition for the computational structure of these emerging deep learning architectures. After deriving their parallel algorithm, we gradually build towards a simple template CUDA extension for PyTorch. We hope that making linear recurrences accessible to a wider audience inspires further research on linear-time sequence mixing.",
      "published": "April 28, 2025",
      "authors": [
        {
          "author": "Felix Sarnthein",
          "authorURL": "https://safelix.github.io",
          "affiliations": [
            {
              "name": "ELLIS Institute Tübingen <br>MPI-IS Tübingen <br>ETH Zürich",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/reviewing/">reviewing</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Linear Recurrences Accessible to Everyone</h1> <p>Investigating linear RNNs such as Mamba, can be challenging because they are currently not efficiently expressible in PyTorch. We propose the abstraction of linear recurrences to gain intuition for the computational structure of these emerging deep learning architectures. After deriving their parallel algorithm, we gradually build towards a simple template CUDA extension for PyTorch. We hope that making linear recurrences accessible to a wider audience inspires further research on linear-time sequence mixing.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <ul> <li><a href="#motivation">Motivation</a></li> <li><a href="#mamba-with-a-linear-recurrence">Mamba with a Linear Recurrence</a></li> </ul> <div><a href="#part-i-algorithm">Part I Algorithm</a></div> <ul> <li><a href="#parallel-calculation">Parallel Calculation</a></li> <li><a href="#gradient-calculation">Gradient Calculation</a></li> </ul> <div><a href="#part-ii-implementation">Part II Implementation</a></div> <ul> <li><a href="#preparing-a-cuda-extension-for-pytorch">Preparing a CUDA Extension for PyTorch</a></li> <li><a href="#reference-implementation-scan">Reference Implementation Scan</a></li> <li><a href="#tile-implementation">Tile Implementation</a></li> <li><a href="#pipe-implementation">Pipe Implementation</a></li> <li><a href="#reverse-and-backward-implementation">Reverse and Backward Implementation</a></li> <li><a href="#testing-and-benchmarking">Testing and Benchmarking</a></li> </ul> <div><a href="#discussion">Discussion</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <h3 id="motivation">Motivation</h3> <p>With the publication of Mamba<d-cite key="gu2024mamba"></d-cite>, the parallel scan algorithm<d-cite key="ladner1980parallelprefix"></d-cite><d-cite key="blelloch1990prefixapplication"></d-cite> received once again the attention of the Deep Learning community<d-footnote> e.g. Tweets by <a href="https://x.com/Algomancer/status/1740171408284782966" rel="external nofollow noopener noopener noreferrer" target="_blank">Adam Hibble</a>, <a href="https://x.com/francoisfleuret/status/1788489112283996367" rel="external nofollow noopener noopener noreferrer" target="_blank">Francois Fleuret</a> or <a href="https://github.com/pytorch/pytorch/issues/95408#issuecomment-2327232046" rel="external nofollow noopener noopener noreferrer" target="_blank">PyTorch Issues</a></d-footnote>. This algorithm allows to parallelize the inherently sequential operation of a scan if its update function is associative, for example, to compute cumulative sums and products, or linear recurrent neural network (RNN) layers<d-cite key="martin2018parallelizing"></d-cite>. In particular, it is a fundamental building block of an emerging class of architectures inspired by state space models (SSMs) such as S4<d-cite key="gu2024mamba"></d-cite>, S5<d-cite key="smith2023s5"></d-cite>, LRU<d-cite key="orvieto2023lru"></d-cite>, Hawk <d-cite key="de2024griffin"></d-cite>, xLSTM <d-cite key="beck2024xlstm"></d-cite>, or Mamba<d-cite key="gu2024mamba"></d-cite>. These models show promising results on a wide range of tasks while exhibiting linear runtime complexity \(O(L)\) in the sequence length \(L\). Unfortunately, investigating them can be challenging. Because the parallel scan is currently not efficiently expressible in PyTorch, most implementations are hidden in CUDA Kernels, which limits the accessibility of this important research area.</p> <p>This article aims to convey an intuitive understanding via the abstraction of a linear recurrence</p> \[y_l = y_{l-1} \cdot c_l + x_l\] <p>with inputs \(x_l\), coefficients \(c_l\), and outputs \(y_l\). In contrast to other great resources<d-cite key="rush2022annotatedS4"></d-cite><d-cite key="chen2024mamba"></d-cite><d-cite key="grootendorst2024mamba"></d-cite><d-cite key="rush2024mamba"></d-cite> explaining the parallel scan and the individual models themselves, we focus on the linear recurrence because its computational structure is common across all SSM or diagonal linear RNN-based models. Ignoring the complexities of the general parallel scan allows us to reach a rather simple parallel description of the linear recurrence algorithm in the first part of this article.</p> <p>Coincidentally, this algorithmic description can be mapped very efficiently onto the CUDA architecture<d-cite key="merrill2016cubscan"></d-cite>. So in the second part of the article, we will gradually work towards a simple CUDA implementation in the form of a PyTorch extension. As a side-effect to the educational aspect of the exercise, the code can be used as a template for easy prototyping and research. It allows to express Mamba in terms of ‘unfused’, primitive operations in PyTorch, much like matrix multiplications can express Flash-Attention<d-footnote> e.g. in Code by <a href="https://github.com/CLAIRE-Labo/flash_attention" rel="external nofollow noopener noopener noreferrer" target="_blank">Caglar Glucehre</a></d-footnote>. Finally, we show that the runtime of the parallel linear recurrence is practically as fast as a binary vector operation (e.g. <code class="language-plaintext highlighter-rouge">torch.add</code>).</p> <p>All the code is available at <a href="https://github.com/safelix/linrec" rel="external nofollow noopener noopener noreferrer" target="_blank">github.com/safelix/linrec</a>.</p> <h3 id="mamba-with-a-linear-recurrence">Mamba with a Linear Recurrence</h3> <p>Let us first convince ourselves that we can express Mamba, more precisely its sequence mixing layer, with a linear recurrence. The code for this section is available in this <a href="https://colab.research.google.com/drive/1Fk4tpzHluKFSLrLdcvWXiZwMEOd_3BtS" rel="external nofollow noopener noopener noreferrer" target="_blank">Google Colab</a>. To start, we express the linear recurrence with a simple loop:</p> <details> <summary>Show code: minimal imports</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">einops</span> <span class="kn">import</span> <span class="n">rearrange</span><span class="p">,</span> <span class="n">repeat</span><span class="p">,</span> <span class="n">einsum</span></code></pre></figure> </details> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># linear recurrence y_i = y_{i-1} * c_i + x_i
</span><span class="k">def</span> <span class="nf">linrec</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">coeffs</span><span class="p">:</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">prev</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">outputs</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">outputs</span><span class="p">[...,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev</span> <span class="o">*</span> <span class="n">coeffs</span><span class="p">[...,</span> <span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">inputs</span><span class="p">[...,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">prev</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[...,</span> <span class="n">i</span><span class="p">].</span><span class="nf">clone</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">outputs</span></code></pre></figure> <p>To continue, we dissect <a href="https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">mamba_simple.py</code></a> by the original authors. Since there are a lot of moving parts, we focus on the call to <a href="https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">selective_scan_fn</code></a> and how its arguments are prepared. In short, there is an input-independent model parameter <code class="language-plaintext highlighter-rouge">A</code> and a projection layer <code class="language-plaintext highlighter-rouge">in_proj</code>, which maps an input <code class="language-plaintext highlighter-rouge">u</code> to <code class="language-plaintext highlighter-rouge">x</code> and input-dependent parameters <code class="language-plaintext highlighter-rouge">dt</code>, <code class="language-plaintext highlighter-rouge">B</code>, and <code class="language-plaintext highlighter-rouge">C</code>. Then, the selective scan performs some reparametrizations, expands <code class="language-plaintext highlighter-rouge">x</code> with <code class="language-plaintext highlighter-rouge">B</code> into an inner dimension, computes the linear scan of <code class="language-plaintext highlighter-rouge">x</code> with coefficients <code class="language-plaintext highlighter-rouge">A</code>, and contracts the result with <code class="language-plaintext highlighter-rouge">C</code> back to the shape of <code class="language-plaintext highlighter-rouge">x</code>:</p> <details> <summary>Show code: define model parameters</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># model params from
# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py
</span><span class="n">d_model</span> <span class="o">=</span> <span class="mi">1024</span>                          <span class="c1"># W: width of u (default: d_model=1024 in mamba1-370m)
</span><span class="n">expand</span>  <span class="o">=</span> <span class="mi">2</span>                             <span class="c1"># expansion from d_state to d_inner
</span><span class="n">d_inner</span> <span class="o">=</span> <span class="n">expand</span> <span class="o">*</span> <span class="n">d_model</span>              <span class="c1"># D: width of x (default: expand=2 =&gt; d_inner=2048)
</span><span class="n">d_state</span> <span class="o">=</span> <span class="mi">16</span>                            <span class="c1"># N: width of one SSM-head  (default: d_state=16)
</span><span class="n">ngroups</span> <span class="o">=</span> <span class="mi">1</span>                             <span class="c1"># G: number heads that share B and C projection vectors
</span><span class="nf">assert</span><span class="p">(</span><span class="n">d_inner</span> <span class="o">%</span> <span class="n">ngroups</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span></code></pre></figure> </details> <details> <summary>Show code: prepare dummy data</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># prepare dummy data according to
# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span>
<span class="n">batchsize</span><span class="p">,</span> <span class="n">seqlen</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">10</span>

<span class="c1"># A is only input independent param
</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">((</span><span class="n">d_inner</span><span class="p">,</span> <span class="n">d_state</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="mi">15</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">A</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">log</span><span class="p">().</span><span class="nf">float</span><span class="p">())</span> <span class="c1"># for completeness
</span><span class="n">in_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_inner</span> <span class="o">+</span> <span class="n">d_inner</span> <span class="o">+</span> <span class="n">ngroups</span><span class="o">*</span><span class="n">d_state</span> <span class="o">+</span> <span class="n">ngroups</span><span class="o">*</span><span class="n">d_state</span> <span class="o">+</span> <span class="n">d_inner</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># prepare input u and input-dependent params
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">batchsize</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">dt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="nf">in_proj</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">[</span><span class="n">d_inner</span><span class="p">,</span> <span class="n">d_inner</span><span class="p">,</span> <span class="n">ngroups</span><span class="o">*</span><span class="n">d_state</span><span class="p">,</span> <span class="n">ngroups</span><span class="o">*</span><span class="n">d_state</span><span class="p">,</span> <span class="n">d_inner</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="sh">'</span><span class="s">B L (G N) -&gt; B G N L</span><span class="sh">'</span><span class="p">,</span> <span class="n">G</span><span class="o">=</span><span class="n">ngroups</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">d_state</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="sh">'</span><span class="s">B L (G N) -&gt; B G N L</span><span class="sh">'</span><span class="p">,</span> <span class="n">G</span><span class="o">=</span><span class="n">ngroups</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">d_state</span><span class="p">)</span>
<span class="n">u</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="sh">'</span><span class="s">B L D -&gt; B D L</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dt</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="sh">'</span><span class="s">B L D -&gt; B D L</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">softplus</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span> <span class="c1"># map to positive range</span></code></pre></figure> </details> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># selective S6 scan based on linear recurrence
</span><span class="k">def</span> <span class="nf">selective_scan_linrec</span><span class="p">(</span><span class="n">u</span><span class="p">:</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dt</span><span class="p">:</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">C</span><span class="p">:</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># prepare A, B, dt (B=batch, D=d_inner, N=d_state, L=seqlen)
</span>    <span class="n">A</span> <span class="o">=</span> <span class="nf">repeat</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="sh">'</span><span class="s">D N -&gt; B D N L</span><span class="sh">'</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="n">batchsize</span><span class="p">,</span> <span class="n">L</span><span class="o">=</span><span class="n">seqlen</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="nf">repeat</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="sh">'</span><span class="s">B G N L -&gt; B (G x) N L</span><span class="sh">'</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">d_inner</span><span class="o">//</span><span class="n">ngroups</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="nf">repeat</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="sh">'</span><span class="s">B G N L -&gt; B (G x) N L</span><span class="sh">'</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">d_inner</span><span class="o">//</span><span class="n">ngroups</span><span class="p">)</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="nf">repeat</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="sh">'</span><span class="s">B D L -&gt; B D N L</span><span class="sh">'</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">d_state</span><span class="p">)</span>

    <span class="c1"># reparameterize A, B
</span>    <span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">A</span> <span class="o">*</span> <span class="n">dt</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">B</span> <span class="o">*</span> <span class="n">dt</span>

    <span class="c1"># expand scalars u with vectors B to vectors h in inner dimension
</span>    <span class="n">h</span> <span class="o">=</span> <span class="nf">einsum</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="sh">'</span><span class="s">B D N L, B D L -&gt; B D N L</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># compute linear recurrence in inner dimension
</span>    <span class="n">h</span> <span class="o">=</span> <span class="nf">linrec</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">coeffs</span><span class="o">=</span><span class="n">A</span><span class="p">)</span>

    <span class="c1"># contract vectors h in inner dimension with vectors C to scalars y
</span>    <span class="n">y</span> <span class="o">=</span> <span class="nf">einsum</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="sh">'</span><span class="s">B D N L, B D N L -&gt; B D L</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span></code></pre></figure> <p>Finally, we test <code class="language-plaintext highlighter-rouge">selective_scan_linrec</code> by comparing it with two reference implementations:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># selective scan reference implementations
</span><span class="kn">from</span> <span class="n">mamba_ssm.ops.selective_scan_interface</span> <span class="kn">import</span> <span class="n">selective_scan_fn</span><span class="p">,</span> <span class="n">selective_scan_ref</span>
<span class="n">y_linrec</span> <span class="o">=</span> <span class="nf">selective_scan_linrec</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>          
<span class="n">y_sol</span> <span class="o">=</span> <span class="nf">selective_scan_fn</span><span class="p">(</span><span class="n">u</span><span class="o">=</span><span class="n">u</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="n">dt</span><span class="p">,</span> <span class="n">A</span><span class="o">=</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">)</span>   <span class="c1"># error: 7.629e-06
</span><span class="n">y_ref</span> <span class="o">=</span> <span class="nf">selective_scan_ref</span><span class="p">(</span><span class="n">u</span><span class="o">=</span><span class="n">u</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="n">dt</span><span class="p">,</span> <span class="n">A</span><span class="o">=</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">)</span>  <span class="c1"># error: 3.815e-06</span></code></pre></figure> <p>This illustrates how linear recurrences are the building block of Mamba, and it can be expanded to many other architectures such as S4<d-cite key="gu2024mamba"></d-cite>, S5<d-cite key="smith2023s5"></d-cite>, LRU<d-cite key="orvieto2023lru"></d-cite>, and even Mamba-2<d-cite key="dao2024mamba2"></d-cite>. In the special case of linear time-invariant (LTI) systems such as S4<d-cite key="gu2024mamba"></d-cite>, the coefficient <code class="language-plaintext highlighter-rouge">coeff</code> would be shared across sequence length. Note that the reparametrization, as well as the state expansion and contraction, are fused into the linear recurrence in practice. This makes the algorithm hardware-aware and drastically increases runtime by reducing memory accesses.</p> <h2 id="part-i-algorithm">Part I: Algorithm</h2> <p>In the previous section, we learned how the rather simple PyTorch function <code class="language-plaintext highlighter-rouge">linrec</code> can express the sequence mixing component of SSMs or linear RNNs such as Mamba. Unfortunately, this formulation is prohibitively slow even for toy problems. In this section, we will establish further intuitions for the linear recurrence, how to parallelize it, and how to calculate its gradients.</p> <p>Let us begin by defining the linear recurrence \(y_l = y_{l-1} \cdot c_l + x_l\) of inputs \(x_l\), coefficients \(c_l\), and outputs \(y_l\) starting at \(y_0=x_0\) and iterating for \(l=0 \ldots L-1\) steps. By unrolling the recurrence, we obtain an equivalent formulation of a weighted sum</p> \[y_l = \sum_{k=0}^{l} \underbrace{(\prod_{i=k+1}^{l} c_i)}_{=\tilde{c}_{k,l}} \cdot x_k = \sum_{k=0}^{l} \tilde{c}_{k,l} \cdot x_k,\] <p>where \(\tilde{c}_{k,l}\) are cumulative coefficients from \(k\) to \(l\) and \(\tilde{c}_{l,l}=1\). If we consider sequences \(x=[x_{k}]_{k=0}^{L'-1}\), \(c=[c_{i}]_{i=0}^{L'-1}\), and \(y=[y_{l}]_{l=0}^{L'-1}\), we can describe the linear recurrence as a linear sequence mixer \(y = f(x,c) = \tilde{C}^T x\). The mixing matrix \(\tilde{C}^T\) is lower triangular, where the diagonal contains a sequence of ones, the subdiagonal contains the sequence \(c\), and each lower diagonal entry at index \(l,k\) contains the cumulative product \(\tilde{c}_{k,l}\). In the special case of a single shared coefficient \(c_l=z \in [0,1]\), the function \(f\) is an exponential moving average filter. As such, it is an instance of a convolutional operator and therefore \(\tilde{C}\) a circulant matrix. This allows for parallelization via the fast Fourier transform (FFT), as for example in the original S4 implementation, but it is limited to this special case. Like the FFT, parallel scan is based on a divide-and-conquer approach but it additionally works for time-variant \(c\) and achieves a sequential runtime complexity of \(O(L)\) instead of \(O(L \text{log} L)\).</p> <h3 id="parallel-calculation">Parallel Calculation</h3> <p>We approach the divide-and-conquer algorithm from the bottom up. To compute a linear recurrence on two threads, we split the sequences at \(L'\) into two parts. The first thread simply computes the linear recurrence \([y_{l}]_{l=0}^{L'-1}\) up to element \(L'\). To see how the second thread avoids performing the same sequential computation, we decompose \(y_l\) into two sums</p> \[y_l = \underbrace{ \Big(\sum_{k=0}^{L'-1} \tilde{c}_{k,L'-1} \cdot x_k \Big) }_{= y_{L'-1}} \cdot \tilde{c}_{L'-1,l} + \underbrace{ \sum_{k=L'}^{l} \tilde{c}_{k,l} \cdot x_k }_{= \bar{y}_{L',l}} \qquad \text{for}\ l \geq L'.\] <p>Note that \(\bar{y}_{L',l}\) corresponds to a linear recurrence starting at \(L'\) up to \(l\). This means that the second thread can compute the recurrence \([\bar{y}_{L',l}]_{l=L'}^{L-1}\) independently and store the cumulative coefficients \([\tilde{c}_{L'-1,l}]_{l=L'}^{L-1}\) as a by-product. Finally, the second thread receives \(y_{L'-1}\) from the first and combines the terms as \([y_{l}]_{l=L'}^{L-1} = y_{L'-1} \cdot [\tilde{c}_{L'-1,l}]_{l=L'}^{L-1} + [\bar{y}_{L',l}]_{l=L'}^{L-1}\) where \(\cdot\) and \(+\) act element-wise. In PyTorch pseudo code, this would correspond to:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">y</span><span class="p">[...,</span> <span class="p">:</span><span class="n">Lp</span><span class="p">]</span> <span class="o">=</span> <span class="nf">linrec</span><span class="p">(</span><span class="n">x</span><span class="p">[...,</span> <span class="p">:</span><span class="n">Lp</span><span class="p">],</span> <span class="n">c</span><span class="p">[...,</span> <span class="p">:</span><span class="n">Lp</span><span class="p">])</span> <span class="c1"># thread 1
</span><span class="n">y</span><span class="p">[...,</span> <span class="n">Lp</span><span class="p">:]</span> <span class="o">=</span> <span class="nf">linrec</span><span class="p">(</span><span class="n">x</span><span class="p">[...,</span> <span class="n">Lp</span><span class="p">:],</span> <span class="n">c</span><span class="p">[...,</span> <span class="n">Lp</span><span class="p">:])</span> <span class="c1"># thread 2
</span><span class="n">y</span><span class="p">[...,</span> <span class="n">Lp</span><span class="p">:]</span> <span class="o">+=</span> <span class="n">y</span><span class="p">[...,</span> <span class="n">Lp</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">c</span><span class="p">[...,</span> <span class="n">Lp</span><span class="p">:],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># thread 2</span></code></pre></figure> <p>Now, the attentive reader might have noticed that the second thread still has to perform \(O(L)\) operations, so what did we gain? In the setting of \(T\) threads, every thread has to perform \(O(L/T)\) operations sequentially, then all threads communicate the transition elements with a \(O(\text{log} T)\) sequential overhead, and finally, the threads combine the result in \(O(L/T)\). This results in an overall algorithmic complexity of \(O(L/T + \text{log} T)\) for \(T\) threads.</p> <h3 id="gradient-calculation">Gradient Calculation</h3> <p>To implement \(f(x,c)\) in an auto-grad system such as PyTorch, we need to implement a <code class="language-plaintext highlighter-rouge">backward</code> function which back-propagates the gradient \(\delta^{(y)}:=\frac{\partial \mathcal{L}}{\partial y}^T\) through the linear recurrence and returns \(\delta^{(x)}:= \frac{\partial\mathcal{L}}{\partial x}^T =\) and \(\delta^{(c)}:= \frac{\partial \mathcal{L}}{\partial c}^T\). We will now calculate the vector-Jacobian-products \({\delta^{(x)}}^T=\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial x}\) and \({\delta^{(c)}}^T=\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial c}\) as derived from the chain rule:</p> <ol> <li> <p>Let us first consider the derivative of an output $y_l$ with respect to an input \(x_k\)</p> \[\frac{d y_l}{d x_k} = \begin{cases} \tilde{c}_{k,l} &amp;\text{if}\ k \leq l \\ 0 &amp;\text{if}\ l &lt; k \end{cases}\] <p>Inserting the derivative into \(\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial x}\), we again observe the structure of the weighted sum.</p> \[\delta_k^{(x)} = \sum_{l=0}^{L-1} \delta_l^{(y)} \cdot \frac{d y_l}{d x_k} = \sum_{l=k}^{L-1} \tilde{c}_{k, l} \cdot \delta_l^{(y)}\] <p>Rearranging the terms, we unroll \(\delta^{(x)}\) into a reversed linear recursive form</p> \[\delta_k^{(x)} = \delta_{k+1}^{(x)} \cdot c_{k+1} + \delta_k^{(y)}\] </li> <li> <p>Let us now consider the derivative of an output \(y_l\) with respect to a coefficient \(c_i\). We observe that \(c_i\) and \(x_k\) only interact with \(y_l\) if \(k &lt; i \leq l\) and therefore</p> \[\frac{d y_l}{d c_i} = \begin{cases} \displaystyle \sum_{k=0}^{i-1} (\prod_{j=k+1}^{i-1} c_j)(\prod_{j=i+1}^{l} c_j) \cdot x_k = y_{i-1} \cdot \tilde{c}_{i, l} &amp;\text{if}\ i \leq l \\ 0 &amp;\text{if}\ l &lt; i \end{cases}\] <p>Inserting the derivative into \(\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial c}\), we again observe the structure of the weighted sum, i.e.</p> \[\delta_i^{(c)} = \sum_{l=0}^{L-1} \delta_l^{(y)} \cdot \frac{d y_l}{d c_i} = \sum_{l=i}^{L-1} y_{i-1} \cdot \tilde{c}_{i,l} \cdot \delta_l^{(y)}\] <p>Rearranging the terms, we express \(\delta^{(c)}\) as a function of the known $y$ and \(\delta^{(x)}\)</p> \[\delta_i^{(c)} = y_{i-1} \cdot \delta_{i}^{(x)}\] </li> </ol> <p>This provides a very simple way to write a <code class="language-plaintext highlighter-rouge">backward</code> function in PyTorch. The only requirements are a shift function and a <code class="language-plaintext highlighter-rouge">forward</code> function with support for recurrence in the reverse direction.</p> <details> <summary>Show code: `shift` and `linrec_ref_fwd` functions</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">linrec_ref_fwd</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">coeffs</span><span class="p">:</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">prev</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">outputs</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])[::</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">reverse</span> <span class="k">else</span> <span class="mi">1</span><span class="p">]:</span>
        <span class="n">outputs</span><span class="p">[...,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev</span> <span class="o">*</span> <span class="n">coeffs</span><span class="p">[...,</span> <span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">inputs</span><span class="p">[...,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">prev</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[...,</span> <span class="n">i</span><span class="p">].</span><span class="nf">clone</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">outputs</span>

<span class="k">def</span> <span class="nf">shift</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shifts</span><span class="p">,</span> <span class="n">fillval</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> <span class="c1"># torch.roll without the copy of the wrap-around section
</span>    <span class="k">if</span> <span class="n">shifts</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">full_like</span><span class="p">(</span><span class="nb">input</span><span class="p">[...,</span> <span class="p">:</span><span class="n">shifts</span><span class="p">],</span> <span class="n">fillval</span><span class="p">),</span> <span class="nb">input</span><span class="p">[...,</span> <span class="p">:</span><span class="o">-</span><span class="n">shifts</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">shifts</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="nb">input</span><span class="p">[...,</span> <span class="o">-</span><span class="n">shifts</span><span class="p">:],</span> <span class="n">torch</span><span class="p">.</span><span class="nf">full_like</span><span class="p">(</span><span class="nb">input</span><span class="p">[...,</span> <span class="n">shifts</span><span class="p">:],</span> <span class="n">fillval</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></code></pre></figure> </details> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">linrec_ref_bwd</span><span class="p">(</span><span class="n">d_outputs</span><span class="p">:</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">coeffs</span><span class="p">:</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">coeffs</span> <span class="o">=</span> <span class="nf">shift</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">reverse</span> <span class="k">else</span> <span class="mi">1</span><span class="p">,</span> <span class="n">fillval</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">d_inputs</span> <span class="o">=</span> <span class="nf">linrec_ref_fwd</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">d_outputs</span><span class="p">,</span> <span class="n">coeffs</span><span class="o">=</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="n">reverse</span><span class="p">))</span>
    <span class="n">d_coeffs</span> <span class="o">=</span>  <span class="n">d_inputs</span> <span class="o">*</span> <span class="nf">shift</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">shifts</span><span class="o">=</span><span class="mi">1</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">reverse</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">fillval</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d_inputs</span><span class="p">,</span> <span class="n">d_coeffs</span></code></pre></figure> <p>But wait, in Mamba the <code class="language-plaintext highlighter-rouge">coeffs</code> are input-dependent parameters! Fortunately, this case is automatically handled by <code class="language-plaintext highlighter-rouge">torch.autograd</code> via the multi-variable chain rule. In this special case, \(x=z\) and \(c=g(z)\) depend on a common input \(z\), and applying the chain rule yields</p> \[\newcommand{\L}{\mathcal{L}} {\delta^{(z)}}^T := \frac{\partial\L}{\partial z} = \frac{\partial \L}{\partial y} \frac{\partial y}{\partial z} = \frac{\partial \L}{\partial y} \Big( \frac{\partial y}{\partial x} \frac{\partial x}{\partial z} + \frac{\partial y}{\partial c} \frac{\partial c}{\partial z} \Big) = {\delta^{(x)}}^T + {\delta^{(c)}}^T \frac{\partial c}{\partial z} .\] <p>The situation is similar for S4 where \(c=z_0\) depends on a single recurrent coefficient \(z_0\)</p> \[{\delta^{(z)}}^T := \frac{d \L}{d z_0} = \frac{\partial \L}{\partial y} \frac{\partial y}{\partial c} \frac{\partial c}{\partial z_0} = {\delta^{(c)}}^T \frac{\partial c}{\partial z_0} = \sum \delta_{i}^{(c)}.\] <p>PyTorch will derive those cases from the <code class="language-plaintext highlighter-rouge">backward</code> function of the linear recurrence.</p> <h2 id="part-ii-implementation">Part II: Implementation</h2> <p>In the previous sections, we learned how to express the backward function <code class="language-plaintext highlighter-rouge">linrec_ref_bwd</code> in terms of its forward function <code class="language-plaintext highlighter-rouge">linrec_ref_fwd</code> and we gained some intution into how the latter could be parallelized. But as the reader might be aware, long for-loops in PyTorch still represent a serious barrier to efficient code, particularly if they need to be compiled. Nevertheless, there exist implementations such as <a href="https://github.com/alxndrTL/mamba.py" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">mamba.py</code></a><d-cite key="mambapy"></d-cite> which apply the divide-and-conquer approach to express the scan in PyTorch. This is called a device-wide scan and requires the execution of many independent sub-operations, so-called <em>kernels</em>, on a GPU. To avoid this overhead, we would prefer to fuse the entire linear scan into a single kernel. But this is not possible because PyTorch currently does not provide a way to express for-loops which are executed in one kernel. In this section, we learn how to express the linear recurrence first as a for-loop and then as a parallel scan in the GPU programming language CUDA.</p> <p>The goal of this chapter is to familiarize the reader with the basic CUDA programming model in the context of the linear recurrence. Many resources such as the <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide" rel="external nofollow noopener noopener noreferrer" target="_blank">CUDA C++ Programming Guide</a>, the <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide" rel="external nofollow noopener noopener noreferrer" target="_blank">CUDA C++ Best Practices Guide</a>, or <a href="https://github.com/gpu-mode" rel="external nofollow noopener noopener noreferrer" target="_blank">the GPU Mode Lectures</a> venture very quickly into the intricacies of high-performance optimization for expert users. Furthermore, there exist PyTorch implementations of a parallel scan<d-cite key="johnryan265pscan"></d-cite><d-cite key="mambapy"></d-cite><d-cite key="acceleratedscan"></d-cite>, a Triton operator is available as <a href="https://triton-lang.org/main/python-api/generated/triton.language.associative_scan.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">tl.associative_scan</code></a>, and a PyTorch higher-order operator (HOP) is <a href="https://github.com/pytorch/pytorch/issues/95408" rel="external nofollow noopener noopener noreferrer" target="_blank">under development</a>. Here, however, we aim to provide an intuition for the computational structure of the problem and thereby lower the entry bar. The code for Part II is available at <a href="https://github.com/safelix/linrec" rel="external nofollow noopener noopener noreferrer" target="_blank">github.com/safelix/linrec</a>.</p> <h3 id="preparing-a-cuda-extension-for-pytorch">Preparing a CUDA Extension for PyTorch</h3> <p>Although there are a few resources on how to write PyTorch extensions such as the tutorials <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html" rel="external nofollow noopener noopener noreferrer" target="_blank">Custom C++ and CUDA Extensions</a> and <a href="https://pytorch.org/tutorials/advanced/cpp_custom_ops.html" rel="external nofollow noopener noopener noreferrer" target="_blank">Custom C++ and CUDA Operators</a>, the learning curve can be quite steep at times. Therefore, we will aim to provide a rough sketch of what is needed to get started. More in-depth explanations are generally available in the repository. We begin by installing the newest compatible combination of PyTorch, CUDA, and GCC<d-footnote> for more information on compilers see the <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#host-compiler-support-policy" rel="external nofollow noopener noopener noreferrer" target="_blank">CUDA Installation Guide</a></d-footnote>.</p> <figure class="highlight"><pre><code class="language-shell" data-lang="shell">conda create <span class="nt">-n</span> CUDA12.4 <span class="nt">-c</span> conda-forge <span class="nv">gxx</span><span class="o">==</span>13.2 <span class="nv">python</span><span class="o">==</span>3.12 nvidia::cuda<span class="o">==</span>12.4
conda activate CUDA12.4
pip <span class="nb">install </span>numpy pandas matplotlib ninja <span class="nv">torch</span><span class="o">==</span>2.5</code></pre></figure> <p>Now, we can write a function using the <a href="https://pytorch.org/cppdocs/" rel="external nofollow noopener noopener noreferrer" target="_blank">C++ frontend of PyTorch</a>, which might look a bit familiar. Since we want to call our function from within Python, the entry point into the code will not be a classical <code class="language-plaintext highlighter-rouge">main()</code> function. Instead, we compile the code to a shared library (<code class="language-plaintext highlighter-rouge">.so</code>) and load it dynamically into the Python runtime. This is conveniently handled by PyTorch and pybind.</p> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="cp">#include</span> <span class="cpf">&lt;torch/torch.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;torch/extension.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;pybind11/pybind11.h&gt;</span><span class="cp">
</span>
<span class="k">using</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="p">;</span>
<span class="n">Tensor</span> <span class="nf">linrec_ref_fwd</span><span class="p">(</span><span class="k">const</span> <span class="n">Tensor</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span> <span class="o">&amp;</span><span class="n">coeffs</span><span class="p">,</span> <span class="k">const</span> <span class="kt">bool</span> <span class="n">reverse</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">Tensor</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">::</span><span class="n">empty_like</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
    <span class="c1">// do something</span>
    <span class="k">return</span> <span class="n">outputs</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">PYBIND11_MODULE</span><span class="p">(</span><span class="n">TORCH_EXTENSION_NAME</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"linrec_ref_fwd"</span><span class="p">,</span> <span class="n">wrap_pybind_function</span><span class="p">(</span><span class="n">linrec_ref_fwd</span><span class="p">),</span> 
              <span class="s">"Reference CUDA implementation of linear recurrence forward pass."</span><span class="p">,</span>
              <span class="n">py</span><span class="o">::</span><span class="n">arg</span><span class="p">(</span><span class="s">"inputs"</span><span class="p">),</span> <span class="n">py</span><span class="o">::</span><span class="n">arg</span><span class="p">(</span><span class="s">"coeffs"</span><span class="p">),</span> <span class="n">py</span><span class="o">::</span><span class="n">arg</span><span class="p">(</span><span class="s">"reverse"</span><span class="p">)</span><span class="o">=</span><span class="nb">false</span><span class="p">);</span>
<span class="p">}</span></code></pre></figure> <p>In <code class="language-plaintext highlighter-rouge">build.py</code>, we define a build configuration and call <code class="language-plaintext highlighter-rouge">torch.utils.cpp_extension.load</code>, which in turn invokes <code class="language-plaintext highlighter-rouge">nvcc</code> and <code class="language-plaintext highlighter-rouge">gcc</code> and finally loads the pybind module into the Python runtime.</p> <details> <summary>Show code: build.py </summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">os</span><span class="p">,</span> <span class="n">shutil</span>
<span class="kn">from</span> <span class="n">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="n">torch.utils.cpp_extension</span> <span class="kn">import</span> <span class="n">load</span>
<span class="n">SRC_DIR</span> <span class="o">=</span> <span class="nc">Path</span><span class="p">(</span><span class="n">__file__</span><span class="p">).</span><span class="n">parent</span>
<span class="n">BUILD_DIR</span> <span class="o">=</span> <span class="n">SRC_DIR</span> <span class="o">/</span> <span class="sh">"</span><span class="s">.build</span><span class="sh">"</span>

<span class="c1">### CUDA/C++ Compilation Arguments
</span><span class="n">EXT_SOURCES</span> <span class="o">=</span> <span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">SRC_DIR</span> <span class="o">/</span> <span class="sh">"</span><span class="s">extension.cu</span><span class="sh">"</span><span class="p">)]</span>
<span class="n">INCLUDES</span> <span class="o">=</span> <span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">SRC_DIR</span><span class="p">)]</span> <span class="o">+</span> <span class="n">CUDA_RUNTIME_INCLUDES</span>

<span class="n">CUDA_FLAGS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># Options for specifying behavior of compiler/linker.
</span>    <span class="sh">"</span><span class="s">--generate-line-info</span><span class="sh">"</span><span class="p">,</span>                     <span class="c1"># Generate line-number information for device code.
</span>    <span class="sh">"</span><span class="s">-std=c++20</span><span class="sh">"</span><span class="p">,</span>                               <span class="c1"># Select a particular C++ dialect
</span>    <span class="c1"># Options for passing specific phase options
</span>    <span class="c1"># -Xptxas: options for ptxas, the PTX optimizing assembler.
</span>    <span class="sh">"</span><span class="s">-Xptxas</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">-warn-spills</span><span class="sh">"</span><span class="p">,</span>                  <span class="c1"># Warning if registers are spilled to local memory.
</span>    <span class="sh">"</span><span class="s">-Xptxas</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">-warn-lmem-usage</span><span class="sh">"</span><span class="p">,</span>              <span class="c1"># Warning if local memory is used.
</span>    <span class="c1"># Miscellaneous options for guiding the compiler driver
</span>    <span class="sh">"</span><span class="s">--keep</span><span class="sh">"</span><span class="p">,</span>                                   <span class="c1"># Keep all intermediate files that are generated during internal compilation steps.
</span>    <span class="c1"># Options for steering GPU code generation.
</span>    <span class="sh">"</span><span class="s">--use_fast_math</span><span class="sh">"</span><span class="p">,</span>                          <span class="c1"># Make use of fast math library.
</span>    <span class="c1"># Generic tool options.
</span>    <span class="sh">"</span><span class="s">--source-in-ptx</span><span class="sh">"</span><span class="p">,</span>                          <span class="c1"># Interleave source in PTX. May only be used in conjunction with --device-debug or --generate-line-info.
</span>    <span class="sh">"</span><span class="s">--resource-usage</span><span class="sh">"</span><span class="p">,</span>                         <span class="c1"># Show resource usage such as registers and memory of the GPU code. Implies '--ptxas-options --verbose'.
</span><span class="p">]</span>
<span class="n">CPP_FLAGS</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">-std=c++20</span><span class="sh">"</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">make_build_dir</span><span class="p">(</span><span class="n">clean</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">clean</span><span class="p">:</span>
        <span class="n">shutil</span><span class="p">.</span><span class="nf">rmtree</span><span class="p">(</span><span class="n">BUILD_DIR</span><span class="p">,</span> <span class="n">ignore_errors</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">BUILD_DIR</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">extension</span><span class="p">(</span><span class="n">extra_cflags</span><span class="o">=</span><span class="p">[],</span> <span class="n">extra_cuda_cflags</span><span class="o">=</span><span class="p">[],</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">clean</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="nf">make_build_dir</span><span class="p">(</span><span class="n">clean</span><span class="o">=</span><span class="n">clean</span><span class="p">)</span>
    <span class="n">ext</span> <span class="o">=</span> <span class="nf">load</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">pylinrec</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">sources</span><span class="o">=</span><span class="n">EXT_SOURCES</span><span class="p">,</span>
        <span class="n">extra_include_paths</span><span class="o">=</span><span class="n">INCLUDES</span><span class="p">,</span>
        <span class="n">extra_cflags</span><span class="o">=</span><span class="n">CPP_FLAGS</span> <span class="o">+</span> <span class="n">extra_cflags</span><span class="p">,</span>
        <span class="n">extra_cuda_cflags</span><span class="o">=</span><span class="n">CUDA_FLAGS</span> <span class="o">+</span> <span class="n">extra_cuda_cflags</span><span class="p">,</span>
        <span class="n">build_directory</span><span class="o">=</span><span class="nf">str</span><span class="p">(</span><span class="n">BUILD_DIR</span><span class="p">),</span>
        <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ext</span></code></pre></figure> </details> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="n">torch</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="n">build</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">_C</span> <span class="o">=</span> <span class="n">build</span><span class="p">.</span><span class="nf">extension</span><span class="p">()</span>  <span class="c1"># compiles and loads .build/pylinrec.so as a module
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">_C</span><span class="p">.</span><span class="nf">linrec_ref_fwd</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="nf">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span> <span class="c1"># empty outputs</span></code></pre></figure> <h3 id="reference-implementation">Reference Implementation</h3> <p>With the build pipeline in place, we translate the reference implementation to CUDA. In the figure below, we see that a GPU consists of many small compute units, so-called streaming multiprocessors or SMs. With a slight oversimplification, SMs execute independent blocks of computation. To partition the work among these blocks, we have to consider the computational structure of our <code class="language-plaintext highlighter-rouge">linrec_ref_fwd</code> function. Note that the <code class="language-plaintext highlighter-rouge">inputs</code> and <code class="language-plaintext highlighter-rouge">coeffs</code> tensors are stored as one flattened array in memory and their their last dimension represents the sequence index. This means that consecutive sequence elements are also consecutive in memory. Therefore, the \(i\)-th block can simply process the \(i\)-th sequence at the memory index <code class="language-plaintext highlighter-rouge">seqLen*blockIdx.x</code>.</p> <div class="row mt-3" style="margin-bottom: 0px"> <div class="col-12 d-flex justify-content-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-linrec/GA100-arch-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-linrec/GA100-arch-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-linrec/GA100-arch-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-linrec/GA100-arch.png" class="img-fluid" width="auto" height="auto" max-width="600px" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The Nvidia A100 GPU Architecture. Source: <a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#page=20" rel="external nofollow noopener noopener noreferrer" target="_blank"> NVIDIA A100 White Paper</a> </div> <p>With this insight, we can easily implement <code class="language-plaintext highlighter-rouge">linrec_ref_fwd_kernel</code> with a parallel for-loop, where each block just computes the linear recurrence of its assigned sequence. Recall that the function <code class="language-plaintext highlighter-rouge">linrec_ref_fwd</code> from the previous section is invoked from within Python and runs on the CPU. To launch the kernel on the GPU, it schedules one block for each sequence. The number of sequences is determined by the number of batches and channels.</p> <details> <summary>Show code: kernel launch from `linrec_ref_fwd` </summary> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="n">Tensor</span> <span class="nf">linrec_ref_fwd</span><span class="p">(</span><span class="k">const</span> <span class="n">Tensor</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span> <span class="o">&amp;</span><span class="n">coeffs</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Input checks: matching dimensions, strides, devices, dtype</span>
    <span class="n">Tensor</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">::</span><span class="n">empty_like</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span> <span class="c1">// prepare outputs</span>

    <span class="c1">// Infer number of sequences and sequence length</span>
    <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">stride</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">);</span>        <span class="c1">// inner most dimension is last</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">seqlen</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span>         <span class="c1">// the sequence length</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">numseq</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">/</span> <span class="n">seqlen</span><span class="p">;</span> <span class="c1">// the number of sequences: batches*channels*...</span>

    <span class="c1">// Launch kernel function</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">threads</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> 
    <span class="k">const</span> <span class="kt">int</span> <span class="n">blocks</span> <span class="o">=</span> <span class="n">numseq</span><span class="p">;</span>
    <span class="n">linrec_ref_fwd_kernel</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">threads</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span> 
        <span class="n">coeffs</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span> <span class="n">outputs</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span> <span class="n">seqlen</span>
    <span class="p">);</span>
    <span class="k">return</span> <span class="n">outputs</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> </details> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">kT</span><span class="p">&gt;</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">linrec_ref_fwd_kernel</span><span class="p">(</span><span class="k">const</span> <span class="n">kT</span><span class="o">*</span> <span class="n">inputs</span><span class="p">,</span> <span class="k">const</span> <span class="n">kT</span><span class="o">*</span> <span class="n">coeffs</span><span class="p">,</span> <span class="n">kT</span><span class="o">*</span> <span class="n">outputs</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">seqLen</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Layout: dim=(numseq,seqLen), strides=(seqLen,1)</span>
    <span class="kt">int</span> <span class="n">seqBaseIdx</span> <span class="o">=</span> <span class="n">seqLen</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="c1">// threads block process sequences independently: inputs[seqBaseIdx + i]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">[</span><span class="n">seqBaseIdx</span><span class="p">];</span>         <span class="c1">// get pointer to sequence</span>
    <span class="n">coeffs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">coeffs</span><span class="p">[</span><span class="n">seqBaseIdx</span><span class="p">];</span>         <span class="c1">// get pointer to sequence</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">outputs</span><span class="p">[</span><span class="n">seqBaseIdx</span><span class="p">];</span>       <span class="c1">// get pointer to sequence</span>

    <span class="c1">// Linear Recurrence</span>
    <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>                         <span class="c1">// set start element</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">seqLen</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>               <span class="c1">// linear scan</span>
        <span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">coeffs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></figure> <p>Note that the kernel is a <em>templated</em> C++ function taking a datatype <code class="language-plaintext highlighter-rouge">kT</code> as a compile-time parameter. This allows us to instantiate the kernel for the different required datatypes, a common theme in CUDA programs. More generally, templating allows to select and dispatch any compile-time configuration at runtime using the <code class="language-plaintext highlighter-rouge">static constexpr</code> feature from C++20.</p> <details> <summary>Show code: the dispatch function, a compile-time matching mechanism </summary> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">array</span> <span class="n">CONFIG_LIST</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">I</span> <span class="o">=</span> <span class="mi">0</span><span class="p">&gt;</span>
<span class="kr">inline</span> <span class="kt">void</span> <span class="nf">dispatch</span><span class="p">(</span><span class="k">auto</span> <span class="n">config</span><span class="p">,</span> <span class="k">auto</span> <span class="o">&amp;&amp;</span><span class="n">func</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">funcname</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">static</span> <span class="k">constexpr</span> <span class="k">auto</span> <span class="n">CONFIG</span> <span class="o">=</span> <span class="n">CONFIG_LIST</span><span class="p">[</span><span class="n">I</span><span class="p">];</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">CONFIG</span> <span class="o">==</span> <span class="n">config</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">func</span><span class="p">.</span><span class="k">template</span> <span class="k">operator</span><span class="p">()</span><span class="o">&lt;</span><span class="n">CONFIG</span><span class="p">&gt;();</span>  <span class="c1">// call with compile-time config</span>
        <span class="k">return</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="nf">constexpr</span> <span class="p">(</span><span class="n">I</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">CONFIG_LIST</span><span class="p">.</span><span class="n">size</span><span class="p">())</span> <span class="p">{</span>
        <span class="n">dispatch</span><span class="o">&lt;</span><span class="n">CONFIG_LIST</span><span class="p">,</span> <span class="n">I</span> <span class="o">+</span> <span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">funcname</span><span class="p">);</span>
        <span class="k">return</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">TORCH_CHECK_NOT_IMPLEMENTED</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span> <span class="s">"'"</span><span class="p">,</span> <span class="n">funcname</span><span class="p">,</span> <span class="s">"' is not compiled for this config."</span><span class="p">)</span>
<span class="p">}</span></code></pre></figure> </details> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="k">static</span> <span class="k">constexpr</span> <span class="k">auto</span> <span class="n">CONFIG_LIST</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">array</span><span class="p">{</span><span class="cm">/*config1*/</span><span class="p">,</span> <span class="cm">/*config2*/</span><span class="p">,</span> <span class="p">...};</span>
<span class="n">dispatch</span><span class="o">&lt;</span><span class="n">CONFIG_LIST</span><span class="o">&gt;</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">]</span><span class="o">&lt;</span><span class="k">auto</span> <span class="n">config</span><span class="o">&gt;</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">mytemplatedfunc</span><span class="o">&lt;</span><span class="cm">/*config*/</span><span class="o">&gt;&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">threads</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span> 
        <span class="n">inputs</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(),</span>
        <span class="n">outputs</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(),</span>
    <span class="p">);</span>
<span class="p">},</span> <span class="s">"mytemplatedfunc"</span><span class="p">);</span> <span class="c1">// name for errors</span></code></pre></figure> <h3 id="tile-implementation">Tile Implementation</h3> <p>In the previous section, each block of work executes a single sequential thread. But zooming in on one streaming multiprocessor, we see that a block can execute operations on up to 1024 threads in synchronization! Furthermore, the SM has a total of 65536 registers which means that one block can hold and process a tile containing two <code class="language-plaintext highlighter-rouge">inputs</code> and <code class="language-plaintext highlighter-rouge">coeffs</code> sequences of length up to 32768 at once. How can we efficiently make use of all these resources?</p> <div class="row mt-3" style="margin-bottom: 0px"> <div class="col-12 d-flex justify-content-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-linrec/GA100-sm-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-linrec/GA100-sm-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-linrec/GA100-sm-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-linrec/GA100-sm.png" class="img-fluid" width="auto" height="350px" max-width="265" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The Nvidia A100 Streaming Multiprocessor. Source: <a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#page=21" rel="external nofollow noopener noopener noreferrer" target="_blank"> NVIDIA A100 White Paper</a> </div> <p>We begin by partitioning the work among blocks in the same way as the reference implementation, i.e. all threads in the same block share the same pointer. Then, we need to distribute the sequence of length <code class="language-plaintext highlighter-rouge">seqLen</code> among the number of threads <code class="language-plaintext highlighter-rouge">numThreads</code> to obtain <code class="language-plaintext highlighter-rouge">elemsPerThread</code>. If there is a remainder, a tail of last threads will receive one element less. Note that all threads execute the same code, but self-assign different base indices and sequence lengths depending on their <code class="language-plaintext highlighter-rouge">threadId</code>. Finally, they load the respective subsequences of <code class="language-plaintext highlighter-rouge">inputs</code> and <code class="language-plaintext highlighter-rouge">coeffs</code> into their thread-local arrays. An important detail here is that the argument <code class="language-plaintext highlighter-rouge">kMaxElemsPerThread</code> determines the size and indexing into the thread-local array at compile time. This guarantees that the array is statically mapped to the registers on the SM.</p> <details> <summary>Show code: kernel launch from `linrec_tile_fwd` </summary> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="n">Tensor</span> <span class="nf">linrec_tile_fwd</span><span class="p">(</span><span class="k">const</span> <span class="n">Tensor</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span> <span class="o">&amp;</span><span class="n">coeffs</span><span class="p">,</span> <span class="k">const</span> <span class="n">Option</span><span class="o">&amp;</span> <span class="n">options</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Input checks: matching dimensions, strides, devices, dtype</span>
    <span class="n">Tensor</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">::</span><span class="n">empty_like</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span> <span class="c1">// Prepare Outputs</span>

    <span class="c1">// Infer number of sequences and sequence length</span>
    <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">stride</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">);</span>        <span class="c1">// inner most dimension is last</span>
    <span class="kt">int</span> <span class="n">seqlen</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span>               <span class="c1">// the sequence length</span>
    <span class="kt">int</span> <span class="n">numseq</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">/</span> <span class="n">seqlen</span><span class="p">;</span>       <span class="c1">// the number of sequences over batches, channels, etc</span>

    <span class="c1">// Unpack and determine compile-time arguments</span>
    <span class="kt">int</span> <span class="n">kMaxElemsPerThread</span> <span class="o">=</span> <span class="n">get</span><span class="p">(</span><span class="n">options</span><span class="p">,</span> <span class="s">"kMaxElemsPerThread"</span><span class="p">,</span> <span class="mi">16</span><span class="p">);</span> 
    <span class="kt">int</span> <span class="n">kMaxThreadsPerWarp</span> <span class="o">=</span> <span class="n">get</span><span class="p">(</span><span class="n">options</span><span class="p">,</span> <span class="s">"kMaxThreadsPerWarp"</span><span class="p">,</span> <span class="mi">32</span><span class="p">);</span> 
    <span class="kt">int</span> <span class="n">kMaxThreadsPerBlock</span> <span class="o">=</span> <span class="n">get</span><span class="p">(</span><span class="n">options</span><span class="p">,</span> <span class="s">"kMaxThreadsPerBlock"</span><span class="p">,</span> <span class="mi">1024</span><span class="p">);</span> 

    <span class="c1">// Dispatch templated function: instantiate compile-time configuration</span>
    <span class="k">static</span> <span class="k">constexpr</span> <span class="k">auto</span> <span class="n">CONFIG_LIST</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">array</span><span class="p">{</span><span class="cm">/*config1*/</span><span class="p">,</span> <span class="cm">/*config2*/</span><span class="p">,</span> <span class="p">...};</span>
    <span class="k">auto</span> <span class="n">config</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">array</span><span class="p">{</span><span class="n">kMaxElemsPerThread</span><span class="p">,</span> <span class="n">kMaxThreadsPerWarp</span><span class="p">,</span> <span class="n">kMaxThreadsPerBlock</span><span class="p">};</span>
    <span class="n">dispatch</span><span class="o">&lt;</span><span class="n">CONFIG_LIST</span><span class="o">&gt;</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">]</span><span class="o">&lt;</span><span class="k">auto</span> <span class="n">config</span><span class="o">&gt;</span><span class="p">()</span> <span class="p">{</span>
        <span class="c1">// select kernel based on compile-time arguments </span>
        <span class="k">static</span> <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">kMaxElemsPerThread</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
        <span class="k">static</span> <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">kMaxThreadsPerWarp</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
        <span class="k">static</span> <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">kMaxThreadsPerBlock</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
        <span class="k">auto</span> <span class="n">kernel</span> <span class="o">=</span> <span class="n">linrec_tile_fwd_kernel</span><span class="o">&lt;</span><span class="kt">float</span><span class="p">,</span> <span class="n">kMaxElemsPerThread</span><span class="p">,</span> <span class="n">kMaxThreadsPerWarp</span><span class="p">,</span> <span class="n">kMaxThreadsPerBlock</span><span class="o">&gt;</span><span class="p">;</span>

        <span class="c1">// determine run-time arguments</span>
        <span class="kt">int</span> <span class="n">blocks</span> <span class="o">=</span> <span class="n">numseq</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">threads</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="n">ceildiv</span><span class="p">(</span><span class="n">seqlen</span><span class="p">,</span> <span class="n">kMaxElemsPerThread</span><span class="p">),</span> <span class="n">kMaxThreadsPerBlock</span><span class="p">);</span>

        <span class="c1">// launch kernel</span>
        <span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">threads</span><span class="p">,</span> <span class="n">smem</span><span class="p">,</span> <span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span>
            <span class="n">coeffs</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span> <span class="n">outputs</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span> <span class="n">seqlen</span><span class="p">);</span>
        <span class="n">C10_CUDA_KERNEL_LAUNCH_CHECK</span><span class="p">();</span>
    <span class="p">},</span> <span class="s">"linrec_tile_fwd_kernel"</span><span class="p">);</span> <span class="c1">// name for errors</span>
    <span class="k">return</span> <span class="n">outputs</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> </details> <div class="l-body-outset"> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">kT</span><span class="p">,</span> <span class="n">ushort</span> <span class="n">kMaxElemsPerThread</span><span class="p">,</span> <span class="n">ushort</span> <span class="n">kMaxThreadsPerWarp</span><span class="p">,</span> <span class="n">ushort</span> <span class="n">kMaxThreadsPerBlock</span><span class="p">&gt;</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">__launch_bounds__</span><span class="p">(</span><span class="n">kMaxThreadsPerBlock</span><span class="p">)</span>
<span class="n">linrec_tile_fwd_kernel</span><span class="p">(</span><span class="k">const</span> <span class="n">kT</span><span class="o">*</span> <span class="n">inputs</span><span class="p">,</span> <span class="k">const</span> <span class="n">kT</span><span class="o">*</span> <span class="n">coeffs</span><span class="p">,</span> <span class="n">kT</span><span class="o">*</span> <span class="n">outputs</span><span class="p">,</span> <span class="kt">int</span> <span class="k">const</span> <span class="n">seqLen</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Layout: dim=(X,L), strides=(L,1)</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">seqBaseIdx</span> <span class="o">=</span> <span class="n">seqLen</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="c1">// process sequences independently: inputs[seqBaseIdx+i]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">[</span><span class="n">seqBaseIdx</span><span class="p">];</span>               <span class="c1">// get pointer to sequence</span>
    <span class="n">coeffs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">coeffs</span><span class="p">[</span><span class="n">seqBaseIdx</span><span class="p">];</span>               <span class="c1">// get pointer to sequence</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">outputs</span><span class="p">[</span><span class="n">seqBaseIdx</span><span class="p">];</span>             <span class="c1">// get pointer to sequence</span>

    <span class="c1">// Determine Tile Layout</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">numThreads</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">threadId</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>                                                  <span class="c1">// index of current thread</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">elemsPerThread</span> <span class="o">=</span> <span class="n">ceildiv</span><span class="p">(</span><span class="n">seqLen</span><span class="p">,</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="n">numThreads</span><span class="p">);</span>                      <span class="c1">// distribute seqLen among numThreads</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">numTailThreads</span> <span class="o">=</span> <span class="n">numThreads</span> <span class="o">*</span> <span class="n">elemsPerThread</span> <span class="o">-</span> <span class="n">seqLen</span><span class="p">;</span>                   <span class="c1">// last numTailThreads have one elem less</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">threadTailId</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="n">threadId</span> <span class="o">-</span> <span class="p">(</span><span class="n">numThreads</span> <span class="o">-</span> <span class="n">numTailThreads</span><span class="p">);</span>              <span class="c1">// tail start indicated by ..., 0, 1, 2, ...</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">threadSeqLen</span> <span class="o">=</span> <span class="p">(</span><span class="n">threadTailId</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">?</span> <span class="n">elemsPerThread</span> <span class="o">:</span> <span class="p">(</span><span class="n">elemsPerThread</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span> <span class="c1">// sequence length processed by every thread</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">threadBaseIdx</span> <span class="o">=</span> <span class="n">threadId</span> <span class="o">*</span> <span class="n">elemsPerThread</span> <span class="o">-</span> <span class="n">max</span><span class="p">(</span><span class="n">threadTailId</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>        <span class="c1">// base index to process by every thread</span>

    <span class="c1">// Load inputs and coeffs of tile into thread-local arrays</span>
    <span class="n">kT</span> <span class="n">threadAccOutput</span><span class="p">[</span><span class="n">kMaxElemsPerThread</span><span class="p">];</span>
    <span class="n">kT</span> <span class="n">threadAccCoeff</span><span class="p">[</span><span class="n">kMaxElemsPerThread</span><span class="p">];</span>
    <span class="k">for</span><span class="p">(</span><span class="n">ushort</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">kMaxElemsPerThread</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">threadAccOutput</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">threadSeqLen</span><span class="p">)</span> <span class="o">?</span> <span class="n">inputs</span><span class="p">[</span><span class="n">threadBaseIdx</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>  <span class="c1">// load or fill with 0</span>
        <span class="n">threadAccCoeff</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">threadSeqLen</span><span class="p">)</span> <span class="o">?</span> <span class="n">coeffs</span><span class="p">[</span><span class="n">threadBaseIdx</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">:</span> <span class="mi">1</span><span class="p">;</span>   <span class="c1">// load or fill with 1</span>
    <span class="p">}</span>
    <span class="c1">// Compute parallel scan on a tile (=subsequence) that fits into one thread block </span>
    <span class="n">_linrec_scan_tile_parallel_</span><span class="o">&lt;</span><span class="n">kT</span><span class="p">,</span> <span class="n">kMaxElemsPerThread</span><span class="p">,</span> <span class="n">kMaxThreadsPerWarp</span><span class="p">,</span> <span class="n">kMaxThreadsPerBlock</span><span class="p">,</span> <span class="n">algocode</span><span class="o">&gt;</span><span class="p">(</span>
        <span class="n">threadAccOutput</span><span class="p">,</span> <span class="n">threadAccCoeff</span><span class="p">,</span> <span class="n">numThreads</span>
    <span class="p">);</span>
    <span class="k">for</span><span class="p">(</span><span class="n">ushort</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">threadSeqLen</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Store outputs</span>
        <span class="n">outputs</span><span class="p">[</span><span class="n">threadBaseIdx</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">threadAccOutput</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></figure> </div> <p>Recall the parallel form of the linear recurrence from Part I. The second thread calculated the linear recurrence \([\bar{y}_{L',l}]_{l=L'}^{L-1}\) and stored the cumulative coefficients \([\tilde{c}_{L'-1,l}]_{l=L'}^{L-1}\) on its sub-sequence. This is exactly implemented in algorithmic level 1 of <code class="language-plaintext highlighter-rouge">_linrec_scan_tile_parallel_</code>. For level 2, we need to introduce the concept of a warp. In CUDA, a warp represents a group of 32 threads that execute the same instruction at the same time. Therefore, communication between threads in the same warp incurrs no overhead. For example, the <code class="language-plaintext highlighter-rouge">__shfl_up_sync</code> instruction copies a variable to the thread <code class="language-plaintext highlighter-rouge">threadId+delta</code>. This allows to propagate and accumulate the transition elements across threads so that every thread receives its offset <code class="language-plaintext highlighter-rouge">warpAccOutput</code> (\(y_{L'-1}\)) within 6 steps. Once that is achieved, the simple re-combination remains. We need to compute the exact warp size because it might not be full.</p> <div class="l-body-outset"> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">kT</span><span class="p">,</span> <span class="n">ushort</span> <span class="n">kMaxElemsPerThread</span><span class="p">,</span> <span class="n">ushort</span> <span class="n">kMaxThreadsPerWarp</span><span class="p">,</span> <span class="n">ushort</span> <span class="n">kMaxThreadsPerBlock</span><span class="p">&gt;</span>
<span class="n">__forceinline__</span>  <span class="n">__device__</span>  <span class="kt">void</span> <span class="nf">_linrec_scan_tile_parallel_</span><span class="p">(</span><span class="n">kT</span><span class="o">*</span> <span class="n">threadAccOutput</span><span class="p">,</span> <span class="n">kT</span><span class="o">*</span> <span class="n">threadAccCoeff</span><span class="p">,</span> <span class="k">const</span> <span class="n">ushort</span> <span class="n">numThreads</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Level 1: Accumulate elements within this thread</span>
    <span class="k">for</span><span class="p">(</span><span class="n">ushort</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">kMaxElemsPerThread</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">threadAccOutput</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">threadAccOutput</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">threadAccCoeff</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">threadAccOutput</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
        <span class="n">threadAccCoeff</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="o">=</span> <span class="n">threadAccCoeff</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">threadAccCoeff</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
    
    <span class="c1">// Level 2: Accumulate elements across threads within this warp</span>
    <span class="c1">// Determine Warp Configuration</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">laneId</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">%</span> <span class="n">kMaxThreadsPerWarp</span><span class="p">;</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">warpId</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="n">kMaxThreadsPerWarp</span><span class="p">;</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">numWarps</span> <span class="o">=</span> <span class="n">ceildiv</span><span class="p">(</span><span class="n">numThreads</span><span class="p">,</span> <span class="n">kMaxThreadsPerWarp</span><span class="p">);</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">lastWarpSize</span> <span class="o">=</span> <span class="n">numThreads</span> <span class="o">-</span> <span class="n">kMaxThreadsPerWarp</span> <span class="o">*</span> <span class="p">(</span><span class="n">numWarps</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">thisWarpSize</span> <span class="o">=</span> <span class="p">(</span><span class="n">warpId</span><span class="o">==</span><span class="n">numWarps</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">?</span> <span class="n">lastWarpSize</span> <span class="o">:</span> <span class="n">kMaxThreadsPerWarp</span><span class="p">;</span>
    
    <span class="n">kT</span> <span class="n">warpAccOutput</span> <span class="o">=</span> <span class="n">__shfl_up_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">threadAccOutput</span><span class="p">[</span><span class="n">kMaxElemsPerThread</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// get transition elements between threads</span>
    <span class="n">kT</span> <span class="n">warpAccCoeff</span>  <span class="o">=</span> <span class="n">__shfl_up_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">threadAccCoeff</span><span class="p">[</span><span class="n">kMaxElemsPerThread</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// get transition elements between threads</span>
    <span class="n">warpAccOutput</span> <span class="o">=</span> <span class="p">(</span><span class="n">laneId</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">?</span> <span class="mi">0</span> <span class="o">:</span> <span class="n">warpAccOutput</span><span class="p">;</span>  <span class="c1">// set default 1 for first lane (=thread in warp)</span>
    <span class="n">warpAccCoeff</span>  <span class="o">=</span> <span class="p">(</span><span class="n">laneId</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">?</span> <span class="mi">1</span> <span class="o">:</span> <span class="n">warpAccCoeff</span><span class="p">;</span>   <span class="c1">// set default 0 for first lane (=thread in warp)</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">ushort</span> <span class="n">delta</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">thisWarpSize</span><span class="p">;</span> <span class="n">delta</span> <span class="o">*=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span> 
        <span class="n">kT</span> <span class="n">prevAccOutput</span> <span class="o">=</span> <span class="n">__shfl_up_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">warpAccOutput</span><span class="p">,</span> <span class="n">delta</span><span class="p">);</span>
        <span class="n">kT</span> <span class="n">prevAccCoeff</span>  <span class="o">=</span> <span class="n">__shfl_up_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">warpAccCoeff</span><span class="p">,</span> <span class="n">delta</span><span class="p">);</span>

        <span class="c1">// don't update warpAccOutput and warpAccCoeff in delta lower lanes</span>
        <span class="n">warpAccOutput</span> <span class="o">=</span> <span class="p">(</span><span class="n">laneId</span> <span class="o">&lt;</span> <span class="n">delta</span><span class="p">)</span> <span class="o">?</span> <span class="n">warpAccOutput</span> <span class="o">:</span> <span class="n">prevAccOutput</span> <span class="o">*</span> <span class="n">warpAccCoeff</span> <span class="o">+</span> <span class="n">warpAccOutput</span><span class="p">;</span>
        <span class="n">warpAccCoeff</span>  <span class="o">=</span> <span class="p">(</span><span class="n">laneId</span> <span class="o">&lt;</span> <span class="n">delta</span><span class="p">)</span> <span class="o">?</span> <span class="n">warpAccCoeff</span>  <span class="o">:</span> <span class="n">prevAccCoeff</span> <span class="o">*</span> <span class="n">warpAccCoeff</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">ushort</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">kMaxElemsPerThread</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// distribute accumulates into thread elements</span>
        <span class="n">threadAccOutput</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">warpAccOutput</span> <span class="o">*</span> <span class="n">threadAccCoeff</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">threadAccOutput</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></figure> </div> <p>When more than one warp per block is used, we need to propagate the transition elements across warps. This can be achieved by passing the warp-level transition elements to the first warp (via block-shared memory) where the propagation is performed. Then each thread recombines its entries with the propagated offsets <code class="language-plaintext highlighter-rouge">blockAccOutput</code>. In this way, a linear recurrence of length 32768 could be computed with 5*32+20 floating point operations on 1024 threads if all registers could be used for the thread-local arrays. Pretty cool!</p> <h3 id="pipe-implementation">Pipe Implementation</h3> <p>In order to support linear recurrences exceeding the maximum tile size, we introduce variables <code class="language-plaintext highlighter-rouge">tileBaseIdx</code> and <code class="language-plaintext highlighter-rouge">tileSeqLen</code> which allow us to sequentially accumulate across tiles in an outer loop. This requires some minor adjustments to the <code class="language-plaintext highlighter-rouge">threadBaseIdx</code> and <code class="language-plaintext highlighter-rouge">threadSeqLen</code> calculation. The kernel now processes tiles in a pipelined manner and it could even be feasible to overlap asynchronous memory loading with actual computation. At this point, we would like to draw the reader’s attention to the chosen data type for most indices. Since the <code class="language-plaintext highlighter-rouge">tileSeqLen</code> is limited by the number of registers of an SM, we know that all variables in the range of the tile are guaranteed to be smaller than 65536. We can therefore safely use 8-bit <code class="language-plaintext highlighter-rouge">ushort</code> indexing and make more registers available for the thread-local arrays containing the actual data.</p> <details> <summary>Show code: `linrec_pipe_fwd` kernel </summary> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">kT</span><span class="p">,</span> <span class="n">ushort</span> <span class="n">kMaxElemsPerThread</span><span class="p">,</span> <span class="n">ushort</span> <span class="n">kMaxThreadsPerWarp</span><span class="p">,</span> <span class="n">ushort</span> <span class="n">kMaxThreadsPerBlock</span><span class="p">&gt;</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">__launch_bounds__</span><span class="p">(</span><span class="n">kMaxThreadsPerBlock</span><span class="p">)</span>
<span class="n">linrec_pipe_fwd_kernel</span><span class="p">(</span><span class="k">const</span> <span class="n">kT</span><span class="o">*</span> <span class="n">inputs</span><span class="p">,</span> <span class="k">const</span> <span class="n">kT</span><span class="o">*</span> <span class="n">coeffs</span><span class="p">,</span> <span class="n">kT</span><span class="o">*</span> <span class="n">outputs</span><span class="p">,</span> <span class="kt">int</span> <span class="k">const</span> <span class="n">seqLen</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Layout: dim=(X,L), strides=(L,1)</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">seqBaseIdx</span> <span class="o">=</span> <span class="n">seqLen</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="c1">// process sequences independently: inputs[seqBaseIdx+i]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">[</span><span class="n">seqBaseIdx</span><span class="p">];</span>               <span class="c1">// get pointer to sequence</span>
    <span class="n">coeffs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">coeffs</span><span class="p">[</span><span class="n">seqBaseIdx</span><span class="p">];</span>               <span class="c1">// get pointer to sequence</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">outputs</span><span class="p">[</span><span class="n">seqBaseIdx</span><span class="p">];</span>             <span class="c1">// get pointer to sequence</span>

    <span class="n">__shared__</span> <span class="n">kT</span> <span class="n">seqAccOutput</span><span class="p">;</span> <span class="c1">// for sequential accumulation between tiles</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">seqAccOutput</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="p">}</span> <span class="n">__syncwarp</span><span class="p">();</span> <span class="c1">// avoid divergence</span>

    <span class="c1">// Determine Tile Layout</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">numThreads</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">threadId</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>                     <span class="c1">// index of current thread</span>
    <span class="k">const</span> <span class="n">ushort</span> <span class="n">elemsPerTile</span> <span class="o">=</span> <span class="n">kMaxElemsPerThread</span> <span class="o">*</span> <span class="n">numThreads</span><span class="p">;</span>                                        <span class="c1">// the default number of elements per tile</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">tileBaseIdx</span> <span class="o">=</span> <span class="o">!</span><span class="n">rev</span> <span class="o">?</span> <span class="mi">0</span><span class="p">;</span> <span class="n">tileBaseIdx</span> <span class="o">&lt;</span> <span class="n">seqLen</span><span class="p">;</span> <span class="n">tileBaseIdx</span> <span class="o">+=</span> <span class="n">elemsPerTile</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// linear scan over tiles</span>
        <span class="k">const</span> <span class="n">ushort</span> <span class="n">tileSeqLen</span> <span class="o">=</span> <span class="n">min</span><span class="p">(</span><span class="n">seqLen</span> <span class="o">-</span> <span class="n">tileBaseIdx</span><span class="p">,</span> <span class="n">elemsPerTile</span><span class="p">);</span>                              <span class="c1">// length of the tile to scan with thread block</span>
        <span class="k">const</span> <span class="n">ushort</span> <span class="n">elemsPerThread</span> <span class="o">=</span> <span class="n">ceildiv</span><span class="p">(</span><span class="n">tileSeqLen</span><span class="p">,</span> <span class="n">numThreads</span><span class="p">);</span>                                  <span class="c1">// distribute tileSeqLen among numThreads</span>
        <span class="k">const</span> <span class="n">ushort</span> <span class="n">numTailThreads</span> <span class="o">=</span> <span class="n">numThreads</span> <span class="o">*</span> <span class="n">elemsPerThread</span> <span class="o">-</span> <span class="n">tileSeqLen</span><span class="p">;</span>                         <span class="c1">// last numTailThreads have one elem less</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">threadTailId</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="n">threadId</span> <span class="o">-</span> <span class="p">(</span><span class="n">numThreads</span> <span class="o">-</span> <span class="n">numTailThreads</span><span class="p">);</span>                        <span class="c1">// tail start indicated by ..., 0, 1, 2, ...</span>
        <span class="k">const</span> <span class="n">ushort</span> <span class="n">threadSeqLen</span> <span class="o">=</span> <span class="p">(</span><span class="n">threadTailId</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">?</span> <span class="n">elemsPerThread</span> <span class="o">:</span> <span class="p">(</span><span class="n">elemsPerThread</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span>           <span class="c1">// sequence length processed by every thread</span>
        <span class="k">const</span> <span class="n">ushort</span> <span class="n">threadBaseIdx</span> <span class="o">=</span> <span class="n">threadId</span> <span class="o">*</span> <span class="n">elemsPerThread</span> <span class="o">-</span> <span class="n">max</span><span class="p">(</span><span class="n">threadTailId</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>                  <span class="c1">// base index to process by every thread</span>

        <span class="c1">//</span>
        <span class="c1">// Load inputs and coeffs of tile into thread-local arrays</span>
        <span class="n">kT</span> <span class="n">threadAccOutput</span><span class="p">[</span><span class="n">kMaxElemsPerThread</span><span class="p">];</span>
        <span class="n">kT</span> <span class="n">threadAccCoeff</span><span class="p">[</span><span class="n">kMaxElemsPerThread</span><span class="p">];</span>
        <span class="k">for</span><span class="p">(</span><span class="n">ushort</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">kMaxElemsPerThread</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">threadAccOutput</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">threadSeqLen</span><span class="p">)</span> <span class="o">?</span> <span class="n">inputs</span><span class="p">[</span><span class="n">tileBaseIdx</span> <span class="o">+</span> <span class="n">threadBaseIdx</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>  <span class="c1">// load or fill with 0</span>
            <span class="n">threadAccCoeff</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">threadSeqLen</span><span class="p">)</span> <span class="o">?</span> <span class="n">coeffs</span><span class="p">[</span><span class="n">tileBaseIdx</span> <span class="o">+</span> <span class="n">threadBaseIdx</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">:</span> <span class="mi">1</span><span class="p">;</span>   <span class="c1">// load or fill with 1</span>
        <span class="p">}</span>

        <span class="c1">// Combine seqAccOutput with first threadAccOutput</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">){</span>
            <span class="n">threadAccOutput</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">seqAccOutput</span> <span class="o">*</span> <span class="n">threadAccCoeff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">threadAccOutput</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
        <span class="p">}</span> <span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">// avoid race condition</span>

        <span class="n">_linrec_scan_tile_parallel_</span><span class="o">&lt;</span><span class="n">kT</span><span class="p">,</span> <span class="n">kMaxElemsPerThread</span> <span class="n">kMaxThreadsPerWarp</span><span class="p">,</span> <span class="n">kMaxThreadsPerBlock</span><span class="o">&gt;</span><span class="p">(</span>
            <span class="n">threadAccOutput</span><span class="p">,</span> <span class="n">threadAccCoeff</span><span class="p">,</span> <span class="n">numThreads</span>
        <span class="p">);</span>
    
        <span class="c1">// Store last threadAccOutput into seqAccOutput</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="n">numThreads</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">seqAccOutput</span> <span class="o">=</span> <span class="n">threadAccOutput</span><span class="p">[</span><span class="n">kMaxElemsPerThread</span><span class="o">-</span><span class="mi">1</span><span class="p">];</span>
        <span class="p">}</span> <span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">// avoid race condition</span>

        <span class="k">for</span><span class="p">(</span><span class="n">ushort</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">threadSeqLen</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Store outputs</span>
            <span class="n">outputs</span><span class="p">[</span><span class="n">tileBaseIdx</span> <span class="o">+</span> <span class="n">threadBaseIdx</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">threadAccOutput</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></figure> </details> <h3 id="reverse-and-backward-implementation">Reverse and Backward Implementation</h3> <p>From Part I, we know that computing the backward pass through the linear recurrence mainly consists of a reversed linear recurrence and an index shift. We support the reverse recurrence by loading and storing the tiles in reverse order into the thread-local arrays. For the tile layout, we reverse the <code class="language-plaintext highlighter-rouge">threadId</code> and thereby the <code class="language-plaintext highlighter-rouge">threadBaseIdx</code> if the runtime-argument <code class="language-plaintext highlighter-rouge">rev</code> is true:</p> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="k">const</span> <span class="n">ushort</span> <span class="n">threadIdrev</span> <span class="o">=</span> <span class="o">!</span><span class="n">rev</span> <span class="o">?</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">:</span> <span class="p">(</span><span class="n">numThreads</span> <span class="o">-</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span></code></pre></figure> <p>With the base indices reversed, we still need to copy the data in reverse order. To keep our kernels nice and tidy, we move the memory I/O functionality into a separate <code class="language-plaintext highlighter-rouge">memio.h</code> file:</p> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">kT</span><span class="p">,</span> <span class="k">typename</span> <span class="nc">count_t</span><span class="p">&gt;</span>
<span class="n">__forceinline__</span>  <span class="n">__device__</span>  <span class="kt">void</span> <span class="nf">copy_naive</span><span class="p">(</span><span class="n">kT</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">dst</span><span class="p">,</span> 
                                              <span class="k">const</span> <span class="n">kT</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">src</span><span class="p">,</span>  
                                              <span class="k">const</span> <span class="n">count_t</span> <span class="n">dstElemsPerThread</span><span class="p">,</span> 
                                              <span class="k">const</span> <span class="n">count_t</span> <span class="n">srcElemsPerThread</span><span class="p">,</span> 
                                              <span class="k">const</span> <span class="kt">bool</span> <span class="n">rev</span><span class="p">,</span> <span class="k">const</span> <span class="n">kT</span> <span class="n">fillval</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">count_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">dstElemsPerThread</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">count_t</span> <span class="n">j</span> <span class="o">=</span> <span class="o">!</span><span class="n">rev</span> <span class="o">?</span> <span class="n">i</span> <span class="o">:</span> <span class="p">(</span><span class="n">srcElemsPerThread</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">i</span><span class="p">;</span>
        <span class="n">dst</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">srcElemsPerThread</span><span class="p">)</span> <span class="o">?</span> <span class="n">src</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">:</span> <span class="n">fillval</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></figure> <p>The logic to shift indices correctly between tiles and threads is implemented is this file as well. Finally, we experiment with different approaches to copying, such as vectorization and coalescing<d-footnote> for more information on memory accesses see <a href="https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/" rel="external nofollow noopener noopener noreferrer" target="_blank">CUDA Pro Tip: Grid-Stride Loops</a>, <a href="https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/" rel="external nofollow noopener noopener noreferrer" target="_blank">CUDA Pro Tip: Vectorized Memory Access</a>, and <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#device-memory-spaces" rel="external nofollow noopener noopener noreferrer" target="_blank">CUDA Best Practices: Memory Optimizations</a>.</d-footnote>. The memory loading method is determined by the compile-time parameter <code class="language-plaintext highlighter-rouge">memcode</code> where <code class="language-plaintext highlighter-rouge">memcode=0</code> denotes the naïve baseline described in the code above.</p> <h3 id="tuning-and-benchmarking">Tuning and Benchmarking</h3> <p>To gain a better understanding of our implementations, we compile these configurations:</p> <div class="l-body-outset"> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="k">static</span> <span class="k">constexpr</span> <span class="k">auto</span> <span class="n">CONFIG_NAMES</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">array</span><span class="p">{</span><span class="s">"kMaxElemsPerThread"</span><span class="p">,</span> <span class="s">"kMaxThreadsPerWarp"</span><span class="p">,</span> 
                                                <span class="s">"kMaxThreadsPerBlock"</span><span class="p">,</span> <span class="s">"memcode"</span><span class="p">,</span> <span class="s">"algocode"</span><span class="p">};</span>
<span class="k">static</span> <span class="k">constexpr</span> <span class="k">auto</span> <span class="n">CONFIG_LIST</span> <span class="o">=</span> <span class="n">product</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">array</span><span class="p">{</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">},</span> <span class="n">std</span><span class="o">::</span><span class="n">array</span><span class="p">{</span><span class="mi">32</span><span class="p">},</span> 
                                                <span class="n">std</span><span class="o">::</span><span class="n">array</span><span class="p">{</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">},</span> 
                                                <span class="n">std</span><span class="o">::</span><span class="n">array</span><span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">},</span> <span class="n">std</span><span class="o">::</span><span class="n">array</span><span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">});</span></code></pre></figure> </div> <p>One of the biggest pitfalls in writing CUDA kernels occurs when intermediate variables are not stored in the register file but as local memory on the DRAM<d-footnote>for more information see <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#local-memory" rel="external nofollow noopener noopener noreferrer" target="_blank">CUDA Best Practices: Local Memory</a> and <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses:~:text=to%20these%20constraints.-,Local%20Memory,-Local%20memory%20accesses" rel="external nofollow noopener noopener noreferrer" target="_blank">Cuda Programming Guide: Local Memory</a></d-footnote>. This happens, when local arrays cannot be statically allocated or when the number of live variables exceeds the number of registers. To find out more, we wrap the function <code class="language-plaintext highlighter-rouge">cudaFuncGetAttributes</code> which returns meta data for a given kernel. Invoking <code class="language-plaintext highlighter-rouge">python eval/func_attrs.py --algocode 3</code> shows that <code class="language-plaintext highlighter-rouge">linrec_tile_fwd</code>, <code class="language-plaintext highlighter-rouge">linrec_tile_bwd</code>, and <code class="language-plaintext highlighter-rouge">linrec_pipe_fwd</code> make full use of the registers and only in a few configurations exhibit register spilling. On the other hand, <code class="language-plaintext highlighter-rouge">linrec_pipe_fwd</code> has high register pressure which results in slight spilling to local memory in some configurations.</p> <p>Now, we compare configurations with <code class="language-plaintext highlighter-rouge">python eval/tune.py linrec_pipe_{f|b}wd --algocode 3</code>. The script evaluates the kernels on random test data with <code class="language-plaintext highlighter-rouge">#SMs*100</code> sequences of increasing length. Before benchmarking, we quickly confirm that all errors are in the numerical regime. The tables below depict the performance of the best configurations on an <code class="language-plaintext highlighter-rouge">A100-SXM4-80GB</code> (with <code class="language-plaintext highlighter-rouge">#SM=108</code>). We first note that the runtime seems to be dominated by the kernel launch for <code class="language-plaintext highlighter-rouge">seqLen&lt;256</code> and then it increases linearly. To put these numbers in relation, we transform them into throughput with <code class="language-plaintext highlighter-rouge">(bytes * 1e-9) / (ms * 1e-3)</code> and compare them to the theoretical bandwidth of <code class="language-plaintext highlighter-rouge">2039 GB/s</code> in the case of the used GPU<d-footnote> for more information on bandwidth and throughput see <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#bandwidth" rel="external nofollow noopener noopener noreferrer" target="_blank">CUDA Best Practices: Performance Metrics</a>.</d-footnote>. We observe that the best configurations effectively use only 75% of the available memory bandwidth . This could be explained either by inefficient memory accesses or by many sequential operations per accessed byte. If we now consider the typical tile size of 512 or 1024 for the best-performing configurations, we notice that it is surprisingly small compared to the maximum size of 16384. It is thus more efficient to sequentially process smaller tiles than to reduce sequential operations by processing large tiles in parallel. From this, we conclude that linear recurrences are memory-bound and that memory access patterns are most important for performance.</p> <div class="l-page" style="display: flex; justify-content: center;"> <div style="display: flex; overflow-x: auto;"> <table style="margin-top: 10px; margin-bottom: 0px;"> <thead> <tr> <th style="text-align: left;">Sequence Length</th> <th style="text-align: right;">16</th> <th style="text-align: right;">32</th> <th style="text-align: right;">64</th> <th style="text-align: right;">128</th> <th style="text-align: right;">256</th> <th style="text-align: right;">512</th> <th style="text-align: right;">1024</th> <th style="text-align: right;">2048</th> <th style="text-align: right;">4096</th> <th style="text-align: right;">8192</th> <th style="text-align: right;">16384</th> <th style="text-align: right;">32768</th> <th style="text-align: right;">65536</th> </tr> </thead> <tbody> <tr style="text-align: right;"> <td style="text-align: left;">Runtime (ms)</td> <td>0.02</td> <td>0.02</td> <td>0.02</td> <td>0.02</td> <td>0.03</td> <td>0.05</td> <td>0.09</td> <td>0.17</td> <td>0.35</td> <td>0.69</td> <td>1.35</td> <td>2.70</td> <td>5.38</td> </tr> <tr style="text-align: right;"> <td style="text-align: left;">Memory I/O (GB)</td> <td>&lt;0.01</td> <td>&lt;0.01</td> <td>0.01</td> <td>0.02</td> <td>0.03</td> <td>0.07</td> <td>0.13</td> <td>0.27</td> <td>0.53</td> <td>1.06</td> <td>2.12</td> <td>4.25</td> <td>8.49</td> </tr> <tr style="text-align: right;"> <td style="text-align: left;">Throughput (GB/s)</td> <td>126.6</td> <td>238.2</td> <td>476.5</td> <td>852.6</td> <td>1157.1</td> <td>1322.4</td> <td>1472.7</td> <td>1533.7</td> <td>1524.7</td> <td>1549.8</td> <td>1574.5</td> <td>1573.3</td> <td>1578.4</td> </tr> <tr style="text-align: right;"> <td style="text-align: left;">kMaxElemsPerThread</td> <td>4</td> <td>4</td> <td>4</td> <td>4</td> <td>8</td> <td>8</td> <td>8</td> <td>8</td> <td>8</td> <td>8</td> <td>8</td> <td>8</td> <td>8</td> </tr> <tr style="text-align: right;"> <td style="text-align: left;">kMaxThreadsPerBlock</td> <td>32</td> <td>32</td> <td>32</td> <td>32</td> <td>32</td> <td>64</td> <td>64</td> <td>64</td> <td>64</td> <td>64</td> <td>64</td> <td>64</td> <td>64</td> </tr> <tr style="text-align: right;"> <td style="text-align: left;">memcode</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> </tr> </tbody> </table> </div> </div> <div class="caption"> Best performing configurations for `linrec_pipe_fwd` on `A100-SXM4-80GB`. </div> <div class="l-page" style="display: flex; justify-content: center;"> <div style="display: flex; overflow-x: auto;"> <table style="margin-top: 10px; margin-bottom: 0px"> <thead> <tr style="text-align: right;"> <th style="text-align: left;">Sequence Length</th> <th style="text-align: right;">16</th> <th style="text-align: right;">32</th> <th style="text-align: right;">64</th> <th style="text-align: right;">128</th> <th style="text-align: right;">256</th> <th style="text-align: right;">512</th> <th style="text-align: right;">1024</th> <th style="text-align: right;">2048</th> <th style="text-align: right;">4096</th> <th style="text-align: right;">8192</th> <th style="text-align: right;">16384</th> <th style="text-align: right;">32768</th> <th style="text-align: right;">65536</th> </tr> </thead> <tbody> <tr style="text-align: right;"> <td style="text-align: left;">Runtime (ms)</td> <td>0.03</td> <td>0.03</td> <td>0.03</td> <td>0.03</td> <td>0.05</td> <td>0.08</td> <td>0.15</td> <td>0.29</td> <td>0.58</td> <td>1.17</td> <td>2.34</td> <td>4.70</td> <td>9.39</td> </tr> <tr style="text-align: right;"> <td style="text-align: left;">Memory I/O (GB)</td> <td>&lt;0.01</td> <td>0.01</td> <td>0.01</td> <td>0.03</td> <td>0.06</td> <td>0.11</td> <td>0.22</td> <td>0.44</td> <td>0.88</td> <td>1.77</td> <td>3.54</td> <td>7.08</td> <td>14.16</td> </tr> <tr style="text-align: right;"> <td style="text-align: left;">Throughput (GB/s)</td> <td>135.0</td> <td>270.0</td> <td>519.2</td> <td>1000.0</td> <td>1227.3</td> <td>1384.6</td> <td>1479.5</td> <td>1505.2</td> <td>1513.1</td> <td>1518.5</td> <td>1509.8</td> <td>1506.9</td> <td>1506.9</td> </tr> <tr style="text-align: right;"> <td style="text-align: left;">kMaxElemsPerThread</td> <td>4</td> <td>4</td> <td>4</td> <td>4</td> <td>8</td> <td>8</td> <td>8</td> <td>8</td> <td>8</td> <td>8</td> <td>8</td> <td>8</td> <td>8</td> </tr> <tr style="text-align: right;"> <td style="text-align: left;">kMaxThreadsPerBlock</td> <td>256</td> <td>256</td> <td>256</td> <td>256</td> <td>512</td> <td>64</td> <td>64</td> <td>64</td> <td>128</td> <td>128</td> <td>128</td> <td>64</td> <td>64</td> </tr> <tr style="text-align: right;"> <td style="text-align: left;">memcode</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> </tr> </tbody> </table> </div> </div> <div class="caption"> Best performing configurations for `linrec_pipe_bwd` on `A100-SXM4-80GB`. </div> <p>We wrap the <code class="language-plaintext highlighter-rouge">_C.linrec_*</code> bindings into PyTorch operators to integrate them with autograd and compile systems. While we could automatically tune our kernels for a good configuration given an input shape and a GPU, we manually set <code class="language-plaintext highlighter-rouge">kMaxElemsPerThread=8</code>, <code class="language-plaintext highlighter-rouge">kMaxThreadsPerBlock=64</code>, and <code class="language-plaintext highlighter-rouge">memcode=0</code>. Finally, we compare the CUDA implementations with Triton implementations based on <code class="language-plaintext highlighter-rouge">tl.associative_scan</code>, a PyTorch implementation based on the higher-order associative scan <a href="https://github.com/pytorch/pytorch/blob/main/torch/_higher_order_ops/associative_scan.py" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">torch._higher_order_ops.associative_scan</code></a>, and the PyTorch for-loop reference from the first chapter:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">linrec</span><span class="p">.</span><span class="n">impl</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">linrec_ref</span>         <span class="c1"># CUDA: Reference Implementation
</span><span class="n">linrec</span><span class="p">.</span><span class="n">impl</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">linrec_tile</span>        <span class="c1"># CUDA: Tile Implementation
</span><span class="n">linrec</span><span class="p">.</span><span class="n">impl</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">linrec_pipe</span>        <span class="c1"># CUDA: Pipe Implementation
</span><span class="n">linrec</span><span class="p">.</span><span class="n">impl</span><span class="p">.</span><span class="n">triton</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">linrec_tile</span>      <span class="c1"># Triton: Tile Implementation
</span><span class="n">linrec</span><span class="p">.</span><span class="n">impl</span><span class="p">.</span><span class="n">triton</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">linrec_pipe</span>      <span class="c1"># Triton: Pipe Implementation
</span><span class="n">linrec</span><span class="p">.</span><span class="n">impl</span><span class="p">.</span><span class="n">python</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">linrec_hop</span>       <span class="c1"># PyTorch: Higher-Order Implementation
</span><span class="n">linrec</span><span class="p">.</span><span class="n">impl</span><span class="p">.</span><span class="n">python</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">linrec_ref</span>       <span class="c1"># PyTorch: Reference Implementation</span></code></pre></figure> <p>Comparing these implementations in the figure below, we observe that simply translating the for-loop-based PyTorch implementation into CUDA already yields a significant speedup. The PyTorch higher-order operation allows for further speed-ups, but cannot reach beyond 1000 GB/s and does not efficiently run in backward mode. The tile and pipe implementations are significantly faster than all other implementations and remain consistent even in the backward mode. In absolute terms, they are a bit slower but still comparable to the <code class="language-plaintext highlighter-rouge">torch.add</code> operation. In other words, the scan behaves almost like a binary element-wise operation despite its sequential computational structure. This is possible, because it is memory-bound and requires the same amount of memory I/O as <code class="language-plaintext highlighter-rouge">torch.add</code>. Finally, our implementation makes rather efficient use of the available DRAM bandwidth depicted in brown.</p> <div class="row mt-3" style="margin-bottom: 0px"> <div class="col-12 d-flex justify-content-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-linrec/bench-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-linrec/bench-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-linrec/bench-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-linrec/bench.png" class="img-fluid" width="auto" height="auto" max-width="600px" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Linear recurrence implementations in comparison. </div> <p>The measurements were performed with <code class="language-plaintext highlighter-rouge">torch==2.5.1</code> on a on an <code class="language-plaintext highlighter-rouge">A100-SXM4-80GB</code>.</p> <h2 id="discussion">Discussion</h2> <p>In the previous sections we learned that linear recurrences can be computed in parallel, how they can be mapped onto the CUDA architecture, and why they behave similarly to a simple <code class="language-plaintext highlighter-rouge">torch.add</code>. The goal was to understand linear recurrences as a fundamental building block of an ‘unfused’ Mamba implementation where inputs and coefficients are assumed to be pre-computed. In this section, we briefly discuss this fundamental assumption. Then, we conclude with an outlook on promising directions for future research on linear recurrences.</p> <p>The problem of memory-bound operations such as linear recurrences is that they waste computational resources. Framed more positively, they present an opportunity to perform computations ‘for free’ once the memory is transferred to shared or thread-local memory. Mamba for example uses on-device state expansion: the function <a href="https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L21" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">selective_scan_fn</code></a> expands a loaded sequence to multiple sequences, performs multiple linear recurrences, contracts it to the original dimension, and writes the result back. This means that the expanded state is never materialized on the DRAM which reduces memory transfers and enables the use of longer sequences and larger states. In Mamba-2, the transition coefficients are shared across all channels in one head, which enables even more computation per transferred byte using a similar algorithm to flash linear attention<d-cite key="yang2024fla"></d-cite>, GLA<d-cite key="yang2024gla"></d-cite>, DeltaNet<d-cite key="yang2024deltanet"></d-cite> and Gated DeltaNet<d-cite key="yang2024gateddeltanet"></d-cite>. These developments show that memory I/O plays a crucial role in architecture design considerations. The paper by Golami et al <d-cite key="gholami2020memorywall"></d-cite> with its main figure below suggest that maximizing model expressivity per transferred byte will become even more important in the future.</p> <div class="row mt-3" style="margin-bottom: 0px"> <div class="col-12 d-flex justify-content-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-linrec/hw_scaling-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-linrec/hw_scaling-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-linrec/hw_scaling-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-linrec/hw_scaling.png" class="img-fluid" width="auto" height="auto" max-width="600px" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Memory I/O scales slower than computation. Source: AI and Memory Wall, Fig. 1 <d-cite key="gholami2020memorywall"></d-cite> </div> <p>That said, state expansion and parameter sharing might not be the only way to increase model expressivity per transferred byte. For some applications, it might even be counterproductive to introduce such assumptions. There are still many open questions in the space of (linear) RNNs for which the cheaply available computation per transferred byte might be useful. For example, diagonal linear RNNs have less expressive power than dense, or non-linear RNNs<d-cite key="cirone2024theoreticalfoundations"></d-cite>, in particular for state-tracking tasks<d-cite key="merrill2024illusionstate"></d-cite>. Complex or at least negative transition coefficients can help to mitigate this problem as discussed by Grazzi, Siems, et al<d-cite key="grazzi2024unlockingstate"></d-cite>. Alternatively, non-linear RNNs can be parallelized via approximation as iterated linearized dynamics<d-cite key="lim2024elk"></d-cite><d-cite key="gonzalez2024deer"></d-cite>, or accelerated sequentially with hardware-aware implementations such as FlashRNN<d-cite key="pöppel2024flashrnn"></d-cite>. For all of these computational structures, learning long-range interaction requires intricate parametrizations. Here, the dominant approaches are motivated by dynamical systems<d-cite key="gu2022s4"></d-cite><d-cite key="orvieto2023lru"></d-cite><d-cite key="rusch2024ossm"></d-cite> and associative memory<d-cite key="schlag2021fwp"></d-cite><d-cite key="yang2024deltanet"></d-cite><d-cite key="beck2024xlstm"></d-cite> to tackle the vanishing gradient problem<d-cite key="pascanu2012vanishing"></d-cite><d-cite key="zucchet2024rnnoptimization"></d-cite>.</p> <p>In conclusion, linear recurrences present a simple mathematical object with surprising modeling expressivity and computational opportunities. These properties make them an interesting object of study for various areas of deep learning. We hope that this blog post lowers the entry bar and sparks interest to pursue research on linear-time sequence mixing.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2025/assets/bibliography/2025-04-28-linrec.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>