<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},a=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),o=a[0][0],i=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"April 28, 2025"),r="Restating the Proof of Linear Convergence for Linear GNNs",l="We lead the readers through the core proof of a pioneering paper that studies the training dynamics of linear GNNs. First, we reorganize the proof and provide a more concise and reader-friendly version, highlighting several key components. In doing so, we identify a hidden error and correct it, demonstrating that it has no impact on the main result. Additionally, we offer a dialectical discussion on the strengths and an overlooked aspect of the approach.";{let e=a.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(o+"2025"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${r}},\n  abstract = {${l}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${i}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=a.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${r}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Restating the Proof of Linear Convergence for Linear GNNs | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="We lead the readers through the core proof of a pioneering paper that studies the training dynamics of linear GNNs. First, we reorganize the proof and provide a more concise and reader-friendly version, highlighting several key components. In doing so, we identify a hidden error and correct it, demonstrating that it has no impact on the main result. Additionally, we offer a dialectical discussion on the strengths and an overlooked aspect of the approach."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/2025/assets/css/main.css"> <link rel="canonical" href="https://starlight345.github.io/2025/blog/linear-gnn-convergence-restated/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/2025/assets/js/theme.js"></script> <script src="/2025/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2025/assets/js/distillpub/template.v2.js"></script> <script src="/2025/assets/js/distillpub/transforms.v2.js"></script> <script src="/2025/assets/js/distillpub/overrides.js"></script> <d-front-matter> <script async type="text/json">{
      "title": "Restating the Proof of Linear Convergence for Linear GNNs",
      "description": "We lead the readers through the core proof of a pioneering paper that studies the training dynamics of linear GNNs. First, we reorganize the proof and provide a more concise and reader-friendly version, highlighting several key components. In doing so, we identify a hidden error and correct it, demonstrating that it has no impact on the main result. Additionally, we offer a dialectical discussion on the strengths and an overlooked aspect of the approach.",
      "published": "April 28, 2025",
      "authors": [
        {
          "author": "Huayi Tang*",
          "authorURL": "https://gasteinh.github.io/",
          "affiliations": [
            {
              "name": "GSAI, Renmin Univ.",
              "url": ""
            }
          ]
        },
        {
          "author": "Yuhe Guo*",
          "authorURL": "https://yuziguo.github.io/",
          "affiliations": [
            {
              "name": "GSAI, Renmin Univ.",
              "url": ""
            }
          ]
        },
        {
          "author": "Yong Liu",
          "authorURL": "https://gsai.ruc.edu.cn/english/liuyong",
          "affiliations": [
            {
              "name": "GSAI, Renmin Univ.",
              "url": ""
            }
          ]
        },
        {
          "author": "Zhewei Wei",
          "authorURL": "https://weizhewei.com/",
          "affiliations": [
            {
              "name": "GSAI, Renmin Univ.",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/reviewing/">reviewing</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Restating the Proof of Linear Convergence for Linear GNNs</h1> <p>We lead the readers through the core proof of a pioneering paper that studies the training dynamics of linear GNNs. First, we reorganize the proof and provide a more concise and reader-friendly version, highlighting several key components. In doing so, we identify a hidden error and correct it, demonstrating that it has no impact on the main result. Additionally, we offer a dialectical discussion on the strengths and an overlooked aspect of the approach.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#minimal-background">Minimal Background</a></div> <ul> <li><a href="#linear-gnns-with-squared-loss">Linear GNNs with Squared Loss</a></li> <li><a href="#matrix-gradients">Matrix Gradients</a></li> <li><a href="#kronecker-product-with-vectorization">Kronecker Product with Vectorization</a></li> </ul> <div><a href="#journey-to-the-theorem">Journey to the Theorem</a></div> <ul> <li><a href="#step-1-gradients-and-optimization-steps">Step 1. Gradients and Optimization Steps.</a></li> <li><a href="#step-2-the-dynamics">Step 2. The Dynamics.</a></li> <li><a href="#step-3-solving-the-ode">Step 3. Solving the ODE.</a></li> </ul> <div><a href="#a-minor-mistake-in-the-original-paper">A Minor Mistake in the Original Paper</a></div> <div><a href="#critical-thinking">Critical Thinking</a></div> <ul> <li><a href="#the-overlooked-gap">The Overlooked Gap.</a></li> <li><a href="#dynamics-at-discrete-moments">Dynamics at Discrete Moments.</a></li> <li><a href="#a-balanced-view">A Balanced View.</a></li> </ul> <div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>(<em>* denotes equal contribution</em>)</p> <p>In this post, we’re diving into a 2021 paper that attempts to pin down the <em>training dynamics</em> of Graph Neural Networks (GNNs).</p> <p>This paper<d-cite key="Xu2021"></d-cite>, written by researchers Keyulu Xu, Mozhi Zhang, Stefanie Jegelka and Kenji Kawaguchi, comes with several <strong>strong claims</strong>. Among them, the first and most important one is: linear GNNs <d-footnote> <b>Hint</b>: Check the Minimal Background Section for formal definition of a linear GNN. </d-footnote> converge to the optimal loss at a <strong>linear rate</strong> <d-footnote> <b>Hint</b>: Linear convergence means that we need $\mathcal{O}(\log 1/\epsilon)$ steps of iteration to ensure that the absolute value of the difference between the current solution and the optimal solution is less than $\epsilon$. </d-footnote>. Intrigued? Let’s unravel it together.</p> <p>The key idea here is to think of the loss function of a linear GNN as a function over time - during optimization, as time changes, the weights change, and the loss changes accordingly. Here, we imagine each <strong>step</strong> in the optimization process as moving forward through a tiny <strong>time interval</strong>. For each <strong>discrete</strong> time point $t=1,2,…,T$, the gradients for learnable weights to loss decide how the weights would be changed, and thus, further decide <em>how quickly</em> the curve of loss surges up or down at these time points.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig1-V3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig1-V3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig1-V3-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig1-V3.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption"><b>Weights optimization and loss over time</b>. The left figure is for abstract visual representation, sourced from <a href="https://www.freepik.com/premium-ai-image/abstract-landscape-exploring-blue-pink-contour-lines_88044644.htm" rel="external nofollow noopener noopener noreferrer" target="_blank"> here</a>.</figcaption> </figure> <p>Now, let the time intervals get infinitely closer and closer. Then the discrete optimization process becomes <strong>continuous</strong>. At any time, correspondingly at any point in the weights’ landscape, how fast the loss changes is determined and bounded by a simple expression. <d-footnote> <b>Hint</b>: We'll see how to derive it! </d-footnote> The loss is like a light particle <em>swimming</em> through a <em>velocity field</em>; how fast it surges is controlled by the field <strong>anywhere</strong> as it swims This is why this methodology is called <strong>gradient dynamics</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig2-V2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig2-V2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig2-V2-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig2-V2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>By connecting the dots, now the <em>continuous</em> change in loss can be described by an Ordinary Differential Equation (ODE). With some gradient calculus and simplifications, the result comes down to this beautiful expression:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig3-V3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig3-V3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig3-V3-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig3-V3.png" width="100" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Surprising<d-footnote> <b>Hint</b>: Why it's suprising? A linear convergence rate to global optima for neural networks is <b>generally</b> difficult, due to the facts that the loss function is highly non-convex and there exists many local optima and saddle points. </d-footnote>, right? As we cautiously went through the proof, we encountered a mistake in a key step involving the “positive term”. <strong>It made us question everything</strong>. However, after a closer look, we realized that while there was indeed a small mistake, the rest of the derivation held true—and not just true, but elegantly so!</p> <p>In this blog post, we’ll make the proof flow <strong>crystal clear</strong>, and also correct that minor hiccup (it won’t take long, promise). Our goal is to make the reasoning behind linear convergence accessible, and bring you more confidence to read and think critically, even if you’re not a math whiz. <strong>All you need is a bit of algebra and the chain rule. Ready? Let’s get started!</strong></p> <h2 id="minimal-background">Minimal Background</h2> <blockquote> <p>In this section, we’ll give you the essential background to grasp the proof of linear convergence for linear GNNs. Don’t be intimidated by the strong conclusion—it’s actually quite accessible with just a bit of background.</p> </blockquote> <p>We’ll start with the basics of linear GNNs, then move on to some fundamental mathematical tools like matrix gradients and the Kronecker product, which are key to understanding gradient dynamics.</p> <h3 id="linear-gnns-with-squared-loss">Linear GNNs with Squared Loss</h3> <p>This paper focuses on <strong>Linear GNNs</strong>, where the non-linear activations are removed, and a simplified squared loss is considered. We’ll assume you’re already familiar with the basic form of GCNs, so let’s dive straight into the formulas below.</p> <blockquote> <p><strong>Note</strong>: For detailed explanations on GCN background and the notations, hover here! <d-footnote> In short, the simplest GNN involves just two types of operations: <b>feature propagation</b> over a <b>graph</b> and <b>non-linear transformation</b>, applied alternately. The number of alternations defines how many <b>layers</b> the model has. <br><br> In the referenced paper, within this $H$-layer linear GNN $f$ we put below, the initial feature matrix $X$ undergoes propagations through $S$. The intermediate features are transformed by weight matrices $B_1, \ldots, B_H$. Finally, another transformation by $W$ maps the representation of the $H$-th layer into the label space. <br><br> Note that the subscript ${ }_{\mathcal{:I}}$ represents indexing, extracting the rows corresponding to the training set nodes. In parallel, $Y$ corresponds to the portion of labels associated with the training nodes.</d-footnote></p> </blockquote> \[\begin{eqnarray*} f(X, W, B) &amp;=&amp; W X_{(H)} \\ &amp;=&amp; W B_{(H)} X_{(H-1)} S \\ &amp;=&amp; \cdots \\ &amp;=&amp; W B_{(H)} B_{(H-1)} \cdots B_{(1)} X S^H. \\ \\ L(W, B) &amp;=&amp; \| f(X,W,B)_{*\mathcal{I}} - Y \|_{\mathrm{F}}^2. \end{eqnarray*}\] <p>The above model is trained using <strong>gradient descent</strong>. To analyze the gradient dynamics, we need a bit of matrix gradient computation. </p> <h3 id="matrix-gradients">Matrix Gradients</h3> <p>Here we prepare backgrounds to compute $\nabla_{W}L$ and $\nabla_{B_{\ell}}L, \ell \in [1, H]$, and further, $\nabla_t{L}$. The gradients $\nabla_{W}L$ and $\nabla_{B_{\ell}}L$ determine the direction of change for the current parameters, which in turn influences $\nabla_t{L}$.</p> <ul> <li> <strong>Gradient to linear transformation</strong>. Suppose $Y$, $A$ and $B$ to be matrices,</li> </ul> \[Y = AB \to \nabla_A{Y} = B^{\top} \text{and } \nabla_B{Y} = A.\] <blockquote> <p><strong>Note</strong>: A cautious beginner might ask: why is there a transpose over $B$, but not over $A$? The answer is that, it is due to the <strong>layout convention</strong> of matrix calculus. The matrix form of multivariable gradients are just a <strong>collection</strong> of <strong>element-wise gradients</strong>, and they are organized following different conventions. We refer the reader to the convention sections in wiki page on the <a href="https://en.wikipedia.org/wiki/Matrix_calculus" rel="external nofollow noopener noopener noreferrer" target="_blank">Matrix Calculus</a> for about.</p> </blockquote> <ul> <li> <strong>Gradient to least square loss</strong>.</li> </ul> \[L = \|Y-\hat{Y}\|_{\mathrm{F}}^2 \to \nabla_{\hat{Y}}{L} = 2(Y-\hat{Y}).\] <ul> <li> <p><strong>“Chain rule”</strong>. In backpropagation, gradients are passed backward from the output layer down to earlier layers. For example, suppose loss function $L(\cdot)$ to be least square loss, then</p> \[\hat{Y} = XW \to \nabla_{X}{L} =\nabla_{\hat{Y}}{L} \cdot \nabla_{X}{\hat{Y}} = 2(\hat{Y} - Y)W^{\top}.\] <ul> <li>A <strong>subtle</strong> but important distinction is that, in matrix calculus, the ‘chain rule’ still applies in an element-wise manner. When these elements are arranged as a matrix and computed collectively, while operations may resemble the chain rule, gradients are not always obtained purely through matrix multiplication along a ‘chain’.</li> <li> <p>For example, let $L: \mathbb{R}^{m \times n} \to \mathbb{R}$ be a function over $\hat{Y}$,</p> \[\hat{Y} = W_1XW_2 \to \nabla_{X}{L} = W_1^{\top} \nabla_{\hat{Y}}{L} W_2^{\top} .\] </li> </ul> </li> </ul> <h3 id="kronecker-product-with-vectorization">Kronecker Product with Vectorization</h3> <p>When differentiating matrices, we often use the Kronecker product together with the $\mathrm{vec}[\cdot]$ operator.</p> <blockquote> <p><strong>Note</strong>: <strong>Hover here</strong> <d-footnote> <b>$\mathrm{vec}[\cdot]$ operator</b>. The $\mathrm{vec}[\cdot]$ operator is used to transform a matrix into a vector by stacking its columns on top of one another in a single column vector. For example, if $A$ is an $m \times n$ matrix, $\mathrm{vec}[A]$ rearranges all the <b>columns</b> of $A$ into a vector of size $mn \times 1$. <br><br> <b>Kronecker product</b>. The Kronecker product, denoted by $\otimes$, is an operation on two matrices $A$ and $B$, resulting in a block matrix. Specifically, for an $m \times n$ matrix $A$ and a $p \times q$ matrix $B$, the Kronecker product $A \otimes B$ is an $mp \times nq$ matrix formed by multiplying each element of $A$ by the entire matrix $B$. </d-footnote> for their basic definitions if you need! We also put an illustrative image here:</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig0_1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig0_1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig0_1-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig0_1.png" width="100" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>We’ll frequently encounter the following commonly used expression in the sections ahead.</p> \[\mathrm{vec}[ABC] = (C^\top \otimes A) \cdot \mathrm{vec}[B]\] <ul> <li>This expression allows us to transform expressions into the more familiar form of <strong>matrix-vector multiplication</strong>, which can help avoid the complexity of thinking in higher-dimensional spaces.</li> </ul> </li> </ul> <h2 id="proof-journey-to-the-theorem">Proof: Journey to the Theorem</h2> <p>Now that everything is prepared. Let’s dive into the proof! The proof and notations generally follows the original paper.</p> <p>To make things clearer, in the first place, let’s break this long expression into two parts:</p> \[f(X, W, B)_{*\mathcal{I}}= \underbrace{\tilde{W}_H}_{W B_{(H)} B_{(H-1)} \cdots B_{(1)}\quad\quad\quad} \underbrace{\tilde{G}_H}_{X (S^H)_{*\mathcal{I}}}.\] <p>This simplifies the loss:</p> \[L = \| \tilde{W}_H \tilde{G}_H - Y \|_{\mathrm{F}}^2.\] <h3 id="step-1-gradients-and-optimization-steps">Step 1. Gradients and Optimization Steps.</h3> <blockquote> <p><strong>Note</strong>: This part is composed of both the “backward” and “forward” passes. For the <strong>backward</strong> pass, we derive the gradients of the loss function with respect to the learnable weight matrices. For the <strong>forward</strong> pass, we derive how much they would shift after a step in the optimization process.</p> <p>$\tilde{W}_H$, combining all the weight matrices, acts as a <strong>“waystation”</strong> in both passes. It connects what comes before with what comes next.</p> </blockquote> <p>It is easy to get $\nabla_{\tilde{W}_H}L = 2(\hat{Y}-Y)\tilde{G}_H^{\top}$.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig4-V4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig4-V4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig4-V4-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig4-V4.png" width="100" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Now, let’s proceed by deriving gradients for all the learnable weight matrices. The backpropagation process is illustrated in the image below.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig5-V3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig5-V3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig5-V3-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig5-V3.png" width="100" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption"><b>Forward and backward pass for understanding gradients calculus</b>. The style of illustration refers to <a href="http://colah.github.io/posts/2015-08-Backprop/" rel="external nofollow noopener noopener noreferrer" target="_blank">this blog</a> for inspiration.</figcaption> </figure> <p>Let’s write out the explict expressions of gradients:</p> \[\begin{eqnarray} \label{gradients} \begin{aligned} \nabla_{W} L &amp;= \nabla_{\tilde{W}_H} L (B_{(H)} \cdots B_{(1)})^\top, \\ \nabla_{B_{\ell}} L &amp;= (W B_{(H)}\cdots B_{(\ell+1)})^\top \nabla_{\tilde{W}_H} L (B_{(\ell-1)} \cdots B_{(1)})^\top, \ell = 1, \ldots, H. \end{aligned} \end{eqnarray}\] <hr> <p>Now we consider the optimization process of this linear model, where $W$ and $B=(B_{(1)},\ldots,B_{(H)})$ are updated via gradient descent:</p> \[\begin{eqnarray*} \begin{aligned} &amp; W' = W - \alpha \nabla_{W} L, \\ &amp; B'_{(\ell)} = B_{(\ell)} - \alpha \nabla_{B_{(\ell)}} L, \ \ell = 1, \ldots, H. \end{aligned} \end{eqnarray*}\] <p>Once again, we gather at the “waystation node”: how much has $\tilde{W}_H$ shifted?</p> <p>From here, we’ll write the variables in a <em>vectorized</em> form to pave the way for the following sections. This approach will help us leverage matrix calculus techniques and make the derivations more concise and easier to follow.</p> \[\begin{eqnarray*} \begin{aligned} &amp; \textrm{vec}[\tilde{W}'_{H}] - \textrm{vec}[\tilde{W}_{H}] \\ = &amp; \textrm{vec}[W' B'_{(H)} \cdots B'_{(1)}] - \textrm{vec}[W B_{(H)} \cdots B_{(1)}] \\ = &amp; \textrm{vec}[(W - \textcolor{blue}{\alpha \nabla_{W} L}) (B_{(H)} - \textcolor{blue}{\alpha \nabla_{B_{(H)}} L}) \cdots (B_{(1)} - \textcolor{blue}{\alpha \nabla_{B_{(1)}} L})] - \textrm{vec}[W B_{(H)} \cdots B_{(1)}] \\ = &amp; - \alpha \underbrace{\textrm{vec}[\textcolor{blue}{\nabla_{W} L} B_{(H)}\cdots B_{(1)}] }_{\textcolor{blue}{\text{part a}}} - \alpha \sum_{\ell=1}^H \underbrace{\textrm{vec}[W B_{(H)}\cdots B_{(\ell+1)} \textcolor{blue}{\nabla_{B_{(\ell)}} L} B_{(\ell-1)} \cdots B_{(1)} ]}_{\textcolor{blue}{\text{part b}}} \\ &amp; + \mathcal{O}(\alpha^2). \end{aligned} \end{eqnarray*}\] <p>This might seem a bit complicated. But when we plug the gradient expressions in Eq.\ref{gradients} and connect the Kronecker product and vectorization<d-footnote> <b>Hint</b>: If you are not familiar with these two operations, check the Minimal Background Section!</d-footnote>, we find a beautiful symmetry:</p> \[\begin{eqnarray*} \begin{aligned} \textcolor{blue}{\text{part a }} &amp;\equiv [(B_{(H)}\cdots B_{(1)})^\top (B_{(H)}\cdots B_{(1)}) \otimes I_{m_y}] \textrm{vec} [\nabla_{\tilde{W}_H} L] \\ \textcolor{blue}{\text{part b }} &amp;\equiv \left( J^\top_{(\ell,H)} J_{(\ell,H)} \right) \textrm{vec}[\nabla_{\tilde{W}_H} L], \end{aligned} \end{eqnarray*}\] <p>with \(J_{(\ell,H)} := B_{(\ell-1)}\cdots B_{(1)} \otimes (W_{(H)} B_{(H)}\cdots B_{(\ell+1)})^\top, \ \ell = 1, \ldots, H.\)</p> <p>This symmetry will help us to throw away <strong>part b</strong> in the next part!</p> <hr> <h3 id="step-2-the-dynamics">Step 2. The Dynamics.</h3> <blockquote> <p><strong>Note</strong>: In this part, we’ll derive the dynamics of $\tilde{W}_H$, and then pass its dynamics to the loss $L$. Following the authors, we throw away the semi-positive term (part b*), reduce all eigenvalues to the minimal, and distinguish the parts of vectors inside and outside the representable space, to derive a bound of the loss dynamics with a <strong>favorable form</strong>.</p> <ul> <li>It should be clarified that all the learnable parameters, as well as $\tilde{W}_H$, the loss $L$ and the related eigenvalues and SVD decomposition, is <strong>changing over time</strong>. We <strong>neglect</strong> the time notation in this part for simplicity.</li> </ul> </blockquote> <p>Imagine a small step in the optimization as moving forward through a tiny time interval,<br> we have obtained the dynamics of $\tilde{W}_H$: \(\begin{equation*} \begin{aligned} \frac{\mathrm{d} }{\mathrm{d} t} \mathrm{vec}[\tilde{W}_{H}] :&amp;= \lim_{\alpha \to 0} \frac{\mathrm{vec}[\tilde{W}'_{(H)}] - \mathrm{vec}[\tilde{W}_{(H)}]}{\alpha} \\ &amp;= - \underbrace{[ (B_{(H)}\cdots B_{(1)})^\top (B_{(H)}\cdots B_{(1)}) \otimes I_{m_y}] \mathrm{vec} [\nabla_{\tilde{W}_H}L]}_{\textcolor{blue}{\text{part a'}}} \\ &amp; - \underbrace{\sum_{\ell=1}^H \left( J^\top_{(\ell,H)} J_{(\ell,H)} \right) \mathrm{vec}[\nabla_{\tilde{W}_H} L]}_{\textcolor{blue}{\text{part b'}}}. \\ \end{aligned} \end{equation*}\)</p> <p>Again, with just one step further from the “waystation” $\tilde{W}_H$, we get the dynamics of $L$:</p> \[\begin{equation*} \begin{aligned} \frac{\mathrm{d} }{\mathrm{d} t}L = &amp; \frac{\partial L}{\partial \mathrm{vec}[\tilde{W}_H]} \frac{\mathrm{d} }{\mathrm{d} t} \mathrm{vec}[\tilde{W}_H] \\ = &amp; \mathrm{vec} [\nabla_{\tilde{W}_H} L]^\top \frac{\mathrm{d} }{\mathrm{d} t} \mathrm{vec}[\tilde{W}_H] \\ = &amp; - \underbrace{\mathrm{vec} [\nabla_{\tilde{W}_H} L]^\top [ (B_{(H)}\cdots B_{(1)})^\top (B_{(H)}\cdots B_{(1)}) \otimes I_{m_y}] \mathrm{vec} [\nabla_{\tilde{W}_H} L]}_{\textcolor{blue}{\text{part a*}}} \\ &amp; - \underbrace{\sum_{\ell=1}^H \Vert J_{(\ell,H)} \mathrm{vec}[\nabla_{\tilde{W}_H} L] \Vert_2^2}_{\textcolor{blue}{\text{part b*}}} \\ \leq &amp; - \underbrace{\mathrm{vec} [\nabla_{\tilde{W}_H} L]^\top [ (B_{(H)}\cdots B_{(1)})^\top (B_{(H)}\cdots B_{(1)}) \otimes I_{m_y}] \mathrm{vec} [\nabla_{\tilde{W}_H} L]}_{\textcolor{blue}{\text{part a*}}}. \\ \end{aligned} \end{equation*}\] <blockquote> <p><strong>Note</strong>: <strong>Part b</strong>* is dropped!</p> </blockquote> <p>Denote that</p> \[\lambda_\mathrm{min} := \text{the minimal eigenvalue of } (B_{(H)}\cdots B_{(1)})^\top (B_{(H)}\cdots B_{(1)}),\] <p>and plug $\nabla_{\tilde{W}_H} L=2(\hat{Y}-Y)\tilde{G}_H^{\top}$ into the expression above, we have</p> \[\begin{eqnarray} \label{dynamics} \begin{aligned} \frac{\mathrm{d} }{\mathrm{d} t}L \leq &amp; - \lambda_{\rm min}\cdot \Vert \mathrm{vec} [\nabla_{\tilde{W}_H} L] \Vert^2_2 \\ = &amp; - 4\lambda_{\rm min}\cdot((\tilde{G}_H \otimes I_{m_y})\mathrm{vec} [\hat{Y}-Y])^\top ((\tilde{G}_H \otimes I_{m_y})\mathrm{vec} [\hat{Y}-Y]) \\ = &amp; - 4\lambda_{\rm min}\cdot \textcolor{blue}{\underbrace{\mathrm{vec}[\hat{Y}-Y]^\top \left[ \tilde{G}^\top_H \otimes I_{m_y} \right] \left[ \tilde{G}_H \otimes I_{m_y} \right] \mathrm{vec}[\hat{Y}-Y]}_{\textrm{Eq.(*)}}}. \end{aligned} \end{eqnarray}\] <p>Next, let’s bound Eq.(*).</p> <hr> <p>Before bounding Eq.(*), we introduce the notation $\mathbf{P}$, which projects vectors onto the <strong>column space</strong> of \(\tilde{G}_H^{\top} \otimes I_{m_y}\).</p> <p>Denote the SVD decomposition of \(\tilde{G}_H^{\top} \otimes I_{m_y}\) to be \(U \Sigma V^{\top}\), then \(\mathbf{P}=U U^{\top}\).</p> \[\begin{equation*} \begin{aligned} \textcolor{blue}{\textrm{Eq.(*)}} &amp;= \mathrm{vec}[\hat{Y}-Y]^{\top} U (\Sigma)^2 U^{\top} \mathrm{vec}[\hat{Y}-Y] &amp; \textcolor{gray}{\text{(SVD of $\tilde{G}_H^{\top} \otimes I_{m_y}$)}} \\ &amp; \geq \sigma_{\mathrm{min}}^2(\tilde{G}_H) \cdot \mathrm{vec}[\hat{Y}-Y]^{\top} U U^{\top} \mathrm{vec}[\hat{Y}-Y] &amp; \textcolor{gray}{\text{($\sigma_{\mathrm{min}}(\tilde{G}_H) := \sigma_{\mathrm{min}}(\tilde{G}_H\otimes I_{m_y})$)} } \\ &amp; \geq \sigma_{\mathrm{min}}^2(\tilde{G}_H) \cdot \Vert \mathbf{P} \mathrm{vec}[\hat{Y}-Y]\Vert_2^2 &amp; \textcolor{gray}{\text{(Projector to the column space)}} \end{aligned} \end{equation*}\] <p>Now, we split \(\mathrm{vec}[\hat{Y}-Y]\) into two parts:</p> \[\mathrm{vec}[\hat{Y}-Y] = \underbrace{\mathrm{vec}[Y^*-Y]}_{\text{orthogonal to [$\tilde{G}_H^{\top} \otimes I_{m_y}$]}} - \underbrace{\mathrm{vec}[Y^*-\hat{Y}]}_{\text{resides in [$\tilde{G}_H^{\top} \otimes I_{m_y}$]}}.\] <p>Thus we have</p> \[\begin{equation*} \begin{aligned} \textcolor{blue}{\text{Eq.(*)}} &amp;\geq \sigma_{\mathrm{min}}^2(\tilde{G}_H) \cdot \Vert \mathrm{vec}[Y^* - \hat{Y}]\Vert_2^2 \\&amp;= \sigma_{\mathrm{min}}^2(\tilde{G}_H) (\Vert \mathrm{vec}[\hat{Y}-Y]\Vert_2^2 - \Vert \mathrm{vec}[Y^* - Y]\Vert_2^2 ) \\&amp;= \sigma_{\mathrm{min}}^2(\tilde{G}_H) (L - L^*_H). \end{aligned} \end{equation*}\] <p>where $L^*_H$ stands for the optimal loss for this $H$-layer linear GNN.</p> <p>Combining the derivation to $L$, Eq.(\ref{dynamics}) comes into:</p> \[\begin{equation} \label{L_grad} \frac{\mathrm{d}}{\mathrm{d} t}L \leq - 4\lambda_{\rm min}\sigma^2_{\rm min}(\tilde{G}_H) (L - L^*_H). \end{equation}\] <hr> <h3 id="step-3-solving-the-ode">Step 3. Solving the ODE.</h3> <blockquote> <p><strong>Note</strong>: ODE is always used to describe and solve the dynamics of a system. Now, we are just one step away from the final result.</p> </blockquote> <p>Having Eq.(\ref{L_grad}), and use the fact that $\frac{\mathrm{d} }{\mathrm{d} t}L^*_H = 0$, we have</p> \[\begin{equation*} \frac{\mathrm{d}}{\mathrm{d} t}(L- L^*_H) \leq - 4\lambda_{\rm min}\sigma^2_{\rm min}(\tilde{G}_H) (L - L^*_H). \end{equation*}\] <p>Solving this ODE we have</p> \[\begin{equation*} \begin{aligned} L_T - L^*_H &amp; \leq (L_0 - L^*_H) e^{-4\sigma^2_{\rm min}(\tilde{G}_H) \int_{0}^T \lambda_{\rm min,t}} &amp; \\ &amp; \leq (L_0 - L^*_H) e^{-4 \lambda^{(H)}_T \sigma^2_{\rm min}(\tilde{G}_H) T} \\ &amp; = (L_0 - L^*_H) e^{-4 \lambda^{(H)}_T \sigma^2_{\rm min}(X (S^H)_{*\mathcal{I}}) T}, &amp; \end{aligned} \end{equation*}\] <p>where \(\lambda^{(H)}_T := {\rm inf}_{t \in [0,T]} {\lambda_{\rm min}((B_{(H),t}\cdots B_{(1),t})^\top (B_{(H),t}\cdots B_{(1),t})) }\). </p> <p>This finishes the proof.</p> <h2 id="a-minor-mistake-in-the-original-paper">A Minor Mistake in the Original Paper</h2> <p>The error exits in the following equation, which appear in Appendix A.1.3, the last inequality in Page 18 of the original paper. It writes:</p> \[\begin{equation*} \begin{aligned} &amp; \left( \Vert \mathrm{vec}[\hat{Y}-Y] \Vert_2 - \Vert (I_{m_y n} - \mathbf{P}_{\tilde{G}_H^\top \otimes I_{m_y}}) \mathrm{vec}[Y] \Vert_2 \right)^2 \\ \geq &amp; \Vert \mathrm{vec}[\hat{Y}-Y] \Vert^2_2 - \Vert (I_{m_y n} - \mathbf{P}_{\tilde{G}_H^\top \otimes I_{m_y}}) \mathrm{vec}[Y] \Vert^2_2, \end{aligned} \end{equation*}\] <p>where our notation \(\mathbf{P}\) matches \(\mathbf{P}_{\tilde{G}_H^\top \otimes I_{m_y}}\) here for neatness.</p> <p>By simple calculation, one can verify that the inequality only holds when</p> \[\begin{equation*} \underbrace{\Vert (I_{m_y n} - \mathbf{P}_{\tilde{G}_H^\top \otimes I_{m_y}}) \textrm{vec}[Y] \Vert_2}_{L_{H}^{*}} \geq \underbrace{\Vert \textrm{vec}[\hat{Y}-Y] \Vert_2}_{L}, \end{equation*}\] <p>which is contrary to the fact that $L^*_H \leq L$.</p> <h2 id="critical-thinking">Critical Thinking</h2> <p>We love to make the theorem in the introduced paper clear and engaging. Now, let’s dive into the chaos — think a bit deeper, and question what really matters.</p> <blockquote> <p><strong>Note</strong>： In this part, we remind readers that the continuous time perspective is an approximation. However, the inherent errors are often <strong>overlooked</strong>, leading to potentially <strong>overconfident conclusions</strong>. Nonetheless, on the other hand, treating discrete events as continuous has proven <strong>effective</strong>, not only in neural network analysis but also in various other scientific fields. We believe this approach is valuable, as long as we remain mindful of its limitations and recognize <strong>what has truly been achieved</strong>.</p> </blockquote> <h3 id="the-overlooked-gap">The Overlooked Gap.</h3> <p>The paper we’re talking about makes a bold claim. But does it always hold up in practice? Do real GNNs (when framed as a regression problem) truly guarantee convergence?</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig0_2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig0_2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig0_2-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig0_2.png" width="100" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Theorem in the paper.</figcaption> </figure> <p>Some gaps are pretty obvious, and the authors <d-cite key="Xu2021"></d-cite> points them out: typical GNNs aren’t linear, and researchers use more advanced optimizers than just gradient descent. However, there’s a subtler issue regarding the continuous perspective behind it.</p> <h3 id="dynamics-at-discrete-moments">Dynamics at Discrete Moments.</h3> <p>Take another look at these two diagrams I drew earlier:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig2_1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig2_1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig2_1-1400.webp"></source> <img src="/2025/assets/img/2025-04-28-linear-gnn-convergence-restated/fig2_1.png" width="100" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>When I drew the illustration, I was thinking about the dynamics we’re trying to capture. So at each time point, I make the loss surge downward, showing that $\nabla_{t} L$ is “under control” (meets Eq.(\ref{L_grad})) at these points, because at these moments, we are adjusting weight matrices using principles based on their current gradients. However, we can <strong>only</strong> control things at these specific moments. So besides these time points, I drew <strong>sharp upward surges</strong>, as highlighted by the orange circles.</p> <p>In the introduction, we hinted at the intuition behind gradient dynamics:</p> <blockquote> <p>“The loss is like a light particle <em>swimming</em> through a <em>velocity field</em>; how fast it surges is controlled by the field <strong>anywhere</strong> as it swims.”</p> </blockquote> <p>But in reality, <strong>“anywhere”</strong> is impossible, no matter how much we reduce the step size. The actual process is more like a particle speeding out at an initial velocity, <strong>maintaining</strong> that speed until landing at a new point, and then immediately flying to the next point with a new velocity. Thus, the trajectories solved by ODEs and the actual loss trajectories only <strong>share the same slopes at discrete points</strong>.</p> <h3 id="a-balanced-view">A Balanced View.</h3> <p>The paper’s claim might be a bit too confident. But pointing out these issues doesn’t mean we’re against using dynamic gradients. Our main point is that <strong>we need to know what’s uncertain</strong>.</p> <p>The limitation of this approach was pointed out by an anonymous reviewer of the early paper <d-cite key="Saxe2013"></d-cite>. Check out <a href="https://openreview.net/forum?id=_wzZwKpTDF_9C" rel="external nofollow noopener noopener noreferrer" target="_blank">OpenReview</a>, where the reviewer stated:</p> <blockquote> <p>“I appreciate the attempt to make the analysis more rigorous in terms of ‘learning speed’. Although perhaps a better way to work out a realistic lambda would be to bound the error one gets by taking the continuous time approximation of the original difference equations and choose lambda based on when this error can be argued to have a negligible effect … there is <strong>no guarantee</strong> that this will imply a reasonable correspondence between the discrete and continuous time versions, where mere stability isn’t enough.” –Reviewer 733d</p> </blockquote> <p>Even so, the method that study the discrete optimization process from a continuous viewpoint is widely adopted <d-cite key="Saxe2013"></d-cite><d-cite key="Huang2020"></d-cite><d-cite key="Ji2020"></d-cite><d-cite key="Kawaguchi2021"></d-cite><d-cite key="Arora2018"></d-cite><d-cite key="Arora2019"></d-cite>.</p> <p>What’s more, we notice that similar approaches, which treat inherently discrete entities as continuous by using an ODE or PDE to model the process, are also widely adopted in many other scientific areas. For example, when establishing mathematical models of <strong>epidemic diseases</strong>, researchers treat the disease spreading process among people as a continuous flow, which has led to many valuable insights. We refer the readers to the SI and SIS models introduced <a href="http://www.networksciencebook.com/chapter/10#epidemic" rel="external nofollow noopener noopener noreferrer" target="_blank">HERE</a> on Albert-László Barabási’s website.</p> <p>Therefore, we need to recognize <strong>both sides of the coin</strong>: such methods that convert discrete processes into continuous ones are widely applied and proven effective. While acknowledging their utility, we must also be aware of what we truly understand. It’s not just about “whether there’s a gap” — it’s about “whether we recognize it and are willing to accept it”.</p> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, we introduce an important theorem in the field of GNN optimization using vivid language and illustrations. Our flow builds upon the original paper’s proof strategy, with a clear yet detailed background provided. For the proof, we offer intuitive explanations and concise steps, making it easier for readers to follow. We also point out a small error in the original proof.</p> <p>We believe that the methodology of this paper can not only help in understanding the optimization of GNNs, but may also be applicable to <strong>other architectures</strong> that share some similarities, such as SSMs (State Space Models). And our post would lead more researchers to engage in the topics.</p> <p>One of the advantages of writing in blog format is that we can discuss related ideas more comprehensively. For instance, we acknowledge the strengths of the paper while also discussing, in a balanced manner, the overlooked aspects of the gradient dynamics method. By incorporating discussions from both inside and outside the neural network field, we help readers understand why this issue exists and why it has been overlooked to some extent.</p> <p>Thanks for reading! We hope you enjoyed this post and found it helpful.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2025/assets/bibliography/2025-04-28-linear-gnn-convergence-restated.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>